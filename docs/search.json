[
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-details",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-details",
    "title": "JSC 370: Data Science II",
    "section": "Course Details",
    "text": "Course Details\nLectures: Mondays 1–3pm, MS 3278\nLabs: Wednesdays 1–3pm, HS 108\n\nMeredith Franklin: meredith.franklin@utoronto.ca\nTAs: Johnny Meng and Mandy Yao"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-background",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-background",
    "title": "JSC 370: Data Science II",
    "section": "My Background",
    "text": "My Background\n\nIn late 2021 moved from Los Angeles where I was an Assistant/Associate Professor of Biostatistics at University of Southern California\nFrom Canada: McGill (BSc), Ottawa/Carleton Institute of Math (MSc), Harvard (PhD), UChicago (postdoc)\nHere I’m an Associate Professor with tenure in the Department of Statistical Science (51%) and the School of the Environment (49%)\nData Science Concentration Lead for the MScAC\nExecutive committee for the U of T Data Science Institute"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-teaching",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-teaching",
    "title": "JSC 370: Data Science II",
    "section": "My Teaching",
    "text": "My Teaching\n\nFounded a Master’s of Health Data Science program at USC that launched in 2020\nCo-taught the introduction data science course\nTaught graduate-level spatial statistics, inference, linear models\nAt U of T I teach STA465/STA2016/ENV1112 (Spatial Data Analysis) and I have also taught STA255 (Statistical Theory) and ENV1197 (Research Methods)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-research",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-research",
    "title": "JSC 370: Data Science II",
    "section": "My Research",
    "text": "My Research\n\nSpatial statistical methods for environmental data\nEnvironmental epidemiology\nData science techniques for remote sensing data/imagery\nFocus on pollution (air, noise) and climate (ghg, land cover change)\nMachine learning/Deep learning for spatiotemporal data"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-goals",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-goals",
    "title": "JSC 370: Data Science II",
    "section": "Course Goals",
    "text": "Course Goals\nThrough this course, you will hone techniques used in data science. You will learn:\n\nProgramming in Python, and tools Markdown/Quarto, Git\nExploratory data analysis — generating hypotheses and building intuition\nData visualization — interpretable summaries\nData collection — APIs, data scraping, wrangling, cleaning\nStatistical and machine learning algorithms\nBuilding a github.io website with interaction to communicate your work"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-platforms",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-platforms",
    "title": "JSC 370: Data Science II",
    "section": "Course Platforms",
    "text": "Course Platforms\n\nCourse website: weekly breakdown, lecture slides, labs, datasets\nhttps://jsc370.github.io/JSC370-2026/\nQuercus: announcements, Piazza discussion, grades, course logistics"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-data-science",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-data-science",
    "title": "JSC 370: Data Science II",
    "section": "What is data science?",
    "text": "What is data science?\n\nData science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.\n\n\n\n\n\n\n\nSource: https://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-do-data-scientists-actually-do",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-do-data-scientists-actually-do",
    "title": "JSC 370: Data Science II",
    "section": "What do data scientists actually do?",
    "text": "What do data scientists actually do?\n\nFrame a question and define success (what would convince you?)\nAcquire data (files, databases, APIs, scraping)\nClean + validate (units, missingness, duplicates, joins)\nExplore (EDA) → iterate → refine hypotheses\nModel + evaluate (prediction, inference, uncertainty)\nCommunicate results (reports, dashboards, reproducible code)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-scientists-have-great-responsibility-to-communicate-effectively",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-scientists-have-great-responsibility-to-communicate-effectively",
    "title": "JSC 370: Data Science II",
    "section": "Data Scientists have great responsibility to communicate effectively",
    "text": "Data Scientists have great responsibility to communicate effectively\n\nSource: https://xkcd.com/605/"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#why-communication-matters-in-data-science",
    "href": "slides/01-introduction/JSC370-slides-01.html#why-communication-matters-in-data-science",
    "title": "JSC 370: Data Science II",
    "section": "Why communication matters in Data Science",
    "text": "Why communication matters in Data Science\n\nOpaque models without uncertainty estimation or discussion of limitations\nBiased or unrepresentative data may lead to biased predictions\nConfounding \\(\\ne\\) causation (EDA can sometimes mislead)\nOverfitting and weak validation\nCommentary on Data Science: Towards Data Science"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#top-job-titles",
    "href": "slides/01-introduction/JSC370-slides-01.html#top-job-titles",
    "title": "JSC 370: Data Science II",
    "section": "Top Job Titles",
    "text": "Top Job Titles"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-scientists-in-demand",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-scientists-in-demand",
    "title": "JSC 370: Data Science II",
    "section": "Data Scientists in Demand",
    "text": "Data Scientists in Demand\n\nDemand for data science skills is high across many sectors.\nSkills show up again and again:\n\nPython + libraries (pandas, numpy, sklearn)\nSQL + relational thinking (joins, grouping)\nVisualization + storytelling (matplotlib/plotline/plotly)\nReproducibility (git, environments, reports)\nCommunication (clear write-ups, assumptions, limitations)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#global-labor-market-outlook",
    "href": "slides/01-introduction/JSC370-slides-01.html#global-labor-market-outlook",
    "title": "JSC 370: Data Science II",
    "section": "Global labor market outlook",
    "text": "Global labor market outlook\nWorld Economic Forum (Future of Jobs Report 2025) projects that by 2030:\n\n170 million roles created (across the labor market)\n92 million roles displaced\nNet +78 million jobs (~7% net growth)\n\nThese totals reflect all occupations (not just AI). Data/AI roles show up when we look at the fastest-growing job families. Source: World Economic Forum"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#which-jobs-are-growing-fastest",
    "href": "slides/01-introduction/JSC370-slides-01.html#which-jobs-are-growing-fastest",
    "title": "JSC 370: Data Science II",
    "section": "Which jobs are growing fastest?",
    "text": "Which jobs are growing fastest?\nFastest-growing roles (percentage terms) include:\n\nBig Data Specialists\nAI and Machine Learning Specialists\nFinTech Engineers\nSoftware and Application Developers"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#the-data-behind-the-plot",
    "href": "slides/01-introduction/JSC370-slides-01.html#the-data-behind-the-plot",
    "title": "JSC 370: Data Science II",
    "section": "The data behind the plot",
    "text": "The data behind the plot\n\n\n\nQuantity\nValue (millions)\n\n\n\n\nJobs created\n170\n\n\nJobs displaced\n92\n\n\nNet change\n78"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#visualization-of-these-data",
    "href": "slides/01-introduction/JSC370-slides-01.html#visualization-of-these-data",
    "title": "JSC 370: Data Science II",
    "section": "Visualization of these data",
    "text": "Visualization of these data\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show code\"\n#| fig-width: 6\n#| fig-height: 3\nimport matplotlib.pyplot as plt\nlabels = [\"Jobs created\", \"Jobs displaced\", \"Net change\"]\nvals = [170, 92, 78]  # WEF article summary\nplt.figure()\nplt.bar(labels, vals)\nplt.ylabel(\"Millions of jobs\")\nplt.title(\"Projected global job changes by 2030 (WEF)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-this-course",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-this-course",
    "title": "JSC 370: Data Science II",
    "section": "What is this course?",
    "text": "What is this course?\nThis course is an introduction to the world of data science following on from where JSC270 left off.\nWe will focus on transferable skills and modern workflows:\n\nPython + VS Code for computing\nQuarto for reproducible reports/slides\nGit/GitHub for version control and collaboration"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-you-should-expect",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-you-should-expect",
    "title": "JSC 370: Data Science II",
    "section": "What you should expect",
    "text": "What you should expect\n\nWeekly labs and bi-weekly homework using Python + Quarto\nSubmissions via GitHub Classroom (version control matters)\nFocus on reproducibility: clean repos, clear reports, runnable code\nCollaboration encouraged for discussion, but write-ups/code must be your own"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-science-resources-python",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-science-resources-python",
    "title": "JSC 370: Data Science II",
    "section": "Data Science Resources: Python",
    "text": "Data Science Resources: Python\n\nPython for Data Analysis (3e) — Wes McKinney\n\nFocus: pandas + NumPy + Jupyter for data wrangling, cleaning, and analysis\n\nGood for: practical workflows and “how to do it in pandas”\n\nPython Data Science Handbook — Jake VanderPlas\n\nFocus: core stack NumPy, pandas, Matplotlib, plus intro ML tooling\n\nBonus: full text is online and backed by notebooks GitHub notebooks repo**"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#python-and-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#python-and-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "Python and VS Code",
    "text": "Python and VS Code\n\nPython: general-purpose language widely used for data science\nVS Code: a lightweight, extensible editor with strong Python + notebook support"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "What is VS Code?",
    "text": "What is VS Code?\n\nAn editor for code, notebooks, and markdown\nWorks well with:\n\nPython environments (conda/venv)\nJupyter notebooks\nConnecting to remote environments (e.g. Digital Alliance Canada, AWS)\nGit/GitHub\nQuarto render/preview"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#vs-code-things-youll-use-every-week",
    "href": "slides/01-introduction/JSC370-slides-01.html#vs-code-things-youll-use-every-week",
    "title": "JSC 370: Data Science II",
    "section": "VS Code: Things you’ll use every week",
    "text": "VS Code: Things you’ll use every week\n\nSelect the correct Python environment\nIntegrated terminal (run quarto render, git, ssh)\nSource control panel (stage/commit/push)\nQuarto preview for slides/reports"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#vs-code-layout",
    "href": "slides/01-introduction/JSC370-slides-01.html#vs-code-layout",
    "title": "JSC 370: Data Science II",
    "section": "VS Code: Layout",
    "text": "VS Code: Layout"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-and-python-and-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-and-python-and-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "Quarto and Python and VS Code",
    "text": "Quarto and Python and VS Code\n\nQuarto: markdown-based publishing for reports, sites, and slides"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#how-does-quarto-work",
    "href": "slides/01-introduction/JSC370-slides-01.html#how-does-quarto-work",
    "title": "JSC 370: Data Science II",
    "section": "How does Quarto work?",
    "text": "How does Quarto work?\n\n\n\n\n\n\n\n\nYou write a single source file (.qmd) that mixes text, code, and output options (YAML + chunk options).\nQuarto executes code (e.g., Python via Jupyter) and captures tables, figures, and printed output.\nIt renders the document through Pandoc into a chosen format (HTML report, reveal.js slides, PDF, Word, etc.).\nThe final output is a self-contained deliverable (e.g., report.html) that can be shared or published."
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto",
    "href": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto",
    "title": "JSC 370: Data Science II",
    "section": "Generating reports with Quarto",
    "text": "Generating reports with Quarto\n\nA Quarto document is a plain-text file with extension .qmd.\nAt the top is a YAML header that controls metadata and output (title, author, date, format, options).\nThe format field determines what Quarto produces:\n\nhtml → a web report (.html)\npdf → a PDF report (LaTeX installed)\ndocx → a Word document\nrevealjs → slides"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto-1",
    "href": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto-1",
    "title": "JSC 370: Data Science II",
    "section": "Generating reports with Quarto",
    "text": "Generating reports with Quarto\nAn example yaml header for an html report\n---\ntitle: \"My Report\"\nformat: html\n---\n\nRendering turns MyReport.qmd into MyReport.html (and supporting files if needed).\nCore command: quarto render MyReport.qmd"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\nA code chunk might look like this:"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-1",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-1",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 9])\nplt.show()"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-2",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-2",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\nPreamble (options):\necho: show/hide code\neval: run or skip\nfig-width / fig-height: control plot size"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-and-options",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-and-options",
    "title": "JSC 370: Data Science II",
    "section": "Quarto Code chunks and options",
    "text": "Quarto Code chunks and options\nCode:\nimport: libraries\n.plot, .show: make figures and display them"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#github",
    "href": "slides/01-introduction/JSC370-slides-01.html#github",
    "title": "JSC 370: Data Science II",
    "section": "GitHub",
    "text": "GitHub\n\nVersion control is essential in industry and academia\nBuilding a GitHub portfolio supports job hunting\nYou will build a github.io website as part of this course, with interactivity and app-type features"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#git-vs-github",
    "href": "slides/01-introduction/JSC370-slides-01.html#git-vs-github",
    "title": "JSC 370: Data Science II",
    "section": "Git vs GitHub",
    "text": "Git vs GitHub\n\nGit: version control tool on your computer\nGitHub: hosting + collaboration (remote repos, PRs, issues)\nWorkflow: edit → commit (git) → push (to GitHub)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#this-week-checklist",
    "href": "slides/01-introduction/JSC370-slides-01.html#this-week-checklist",
    "title": "JSC 370: Data Science II",
    "section": "This week: checklist",
    "text": "This week: checklist\n\nInstall Python + VS Code + Quarto\nClone your GitHub Classroom repo\nRender a .qmd locally\nCommit + push your changes\nLab will be on your own this week, starting in person next week, January 14!"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#next-week",
    "href": "slides/01-introduction/JSC370-slides-01.html#next-week",
    "title": "JSC 370: Data Science II",
    "section": "Next Week",
    "text": "Next Week\n\nLecture: Monday January 12, 1–3pm (Version control)\nLab: Wednesday January 14, 1–3pm (VS Code, Python, Quarto, GitHub)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability",
    "href": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability",
    "title": "JSC 370: Data Science II",
    "section": "Repeatability vs Reproducibility vs Replicability",
    "text": "Repeatability vs Reproducibility vs Replicability\n\nThese terms are often used interchangeably, but they are different.\nRepeatability: Generating the exact same results when using the same data by the same person.\nReproducibility: Generating the exact same results when using the same data by a different person or group. If we can’t reproduce a study, how can we replicate it?\nReplicability: Repeating a study by independently performing another study on new data."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability-1",
    "title": "JSC 370: Data Science II",
    "section": "Repeatability vs Reproducibility vs Replicability",
    "text": "Repeatability vs Reproducibility vs Replicability"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility\nA different analyst/researcher re-performs the analysis with the\n\nsame code and\nsame data and\nobtains the same result\n\n\n⚠️ If your results are not repeatable then they will not be reproducible!"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-2",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-3",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility\nBarriers to doing reproducible work:\n\nPoor documentation\nManual steps\nNon-transferable tools\nIncorrect training\nTime"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Workflow",
    "text": "Reproducible Workflow"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nIn academia, incentives often prioritize publication. But many results are difficult to reproduce, so there’s a push to publish code, data, and the tools needed to re-run analyses."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nIn computational sciences and data analysis, what is reproducibility?\nDefinition: The data and code used to make a finding are available and presented so that an independent researcher can (relatively) straightforwardly recreate the result."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-2",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nThis still seldom happens. Two examples from Tim Vines DataSeer.ai:\n\nData availability declines rapidly with article age (reported ~17% lower odds per year in one analysis).\nReanalyses using the program STRUCTURE found a substantial fraction of published results could not be reproduced (reported ~30% in one study)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-3",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nScientific articles often include detailed methods, but they are typically insufficient to reproduce a computational analysis.\nRoger Peng and Stephanie Hicks wrote: “Reproducibility is typically thwarted by a lack of availability of the original data and computer code.”\nScientists owe it to themselves and their community to keep an explicit record of all steps in a computational analysis."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-dos",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-dos",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Do’s",
    "text": "Reproducible Research Do’s\n\nStart with a good question: make it focused and something you care about.\nTeach your computer to do the work from beginning to end (automation &gt; manual steps).\nUse version control.\nTrack your software environment (toolchain + package versions).\nSet a random seed for any random generation/sampling (e.g., train/test splits).\nThink about the entire pipeline (raw data -&gt; cleaning -&gt; analysis -&gt; output)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Don’ts",
    "text": "Reproducible Research Don’ts\nDo not do things by hand. This includes:\n\nEditing spreadsheets to “clean” them (e.g., removing outliers, ad hoc QA/QC)\nManually editing tables or figures\nDownloading data by clicking around in a web browser\nSplitting data and moving it around manually\n\nIf something truly must be done by hand, document it explicitly."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Don’ts",
    "text": "Reproducible Research Don’ts\n\nAvoid point-and-click or highly interactive tools when possible.\n\nThey often leave no trace of the steps.\nIf you must use them, write down the exact sequence of actions.\n\nSave the data and code that generated the output, rather than the output alone."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility Challenges",
    "text": "Reproducibility Challenges\n\nData size\n\nBuild tools into your code to manage large datasets (chunking, efficient formats, parallelism).\nStore data in smaller chunks and write code that pulls and combines files automatically.\nWrite metadata and use tools that support data organization."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility Challenges",
    "text": "Reproducibility Challenges\n\nData complexity\n\nUse smaller “toy” subsets to regularly check reproducibility.\nBe explicit about training/validation/test sets.\nUse diagnostic visualizations.\n\nWorkflow complexity\n\nUse README files (and keep them updated)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control",
    "title": "JSC 370: Data Science II",
    "section": "What is version control?",
    "text": "What is version control?"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control-1",
    "title": "JSC 370: Data Science II",
    "section": "What is version control?",
    "text": "What is version control?\nVersion control is the management of changes to documents and code. Changes are identified by a revision (e.g., “revision 1”, then “revision 2”, …). Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and sometimes merged.\n— Wikipedia (Version control)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care",
    "title": "JSC 370: Data Science II",
    "section": "Why do we care?",
    "text": "Why do we care?\nHave you ever…\n\nMade a change to code, realized it was a mistake, and wanted to revert?\nLost work (or only had an old backup)?\nNeeded to maintain multiple versions (e.g., “final_final_v3”)?\nWanted to compare two versions of your code to see exactly what changed?\nNeeded to prove that a particular change broke (or fixed) something?\nWanted to review the history of a file to understand why it looks like it does?"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care-contd",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care-contd",
    "title": "JSC 370: Data Science II",
    "section": "Why do we care? (cont’d)",
    "text": "Why do we care? (cont’d)\nIn these cases (and many others), a version control system should make your life easier.\n\nWanted to submit a change to someone else’s code (without emailing files around)?\nWanted to share code and collaborate without overwriting each other?\nWanted to see who did what, when, and where (accountability and provenance)?\nWanted to experiment with a feature without disrupting working code?\n\n— Version Control (Atlassian)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#a-common-workflow-pattern",
    "href": "slides/02-version-control/JSC370-slides-02.html#a-common-workflow-pattern",
    "title": "JSC 370: Data Science II",
    "section": "A common workflow pattern",
    "text": "A common workflow pattern"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#git",
    "href": "slides/02-version-control/JSC370-slides-02.html#git",
    "title": "JSC 370: Data Science II",
    "section": "Git",
    "text": "Git\nGit was created by Linus Torvalds and originally described as “the stupid content tracker.”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-git",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-git",
    "title": "JSC 370: Data Science II",
    "section": "Why Git?",
    "text": "Why Git?\n\nDistributed architecture: you have the full history locally (you can work offline).\nEfficient branching and merging: easy to switch between branches, supporting experimentation and collaboration.\nIn this course (and likely beyond), we’ll use Git + GitHub.\nGit is the dominant version control system in practice (Stack Overflow reports very high adoption).\n\nSee: Stack Overflow discussion of survey results\n\nA great reference: Pro Git (free online)\nMore on the name: Git naming (Wikipedia)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#how-can-i-use-git",
    "href": "slides/02-version-control/JSC370-slides-02.html#how-can-i-use-git",
    "title": "JSC 370: Data Science II",
    "section": "How can I use Git?",
    "text": "How can I use Git?\nA few common ways to include Git in your workflow:\n\nCommand line (most universal)\nVS Code Source Control panel (very common for Python work)\nGitHub Desktop (beginner-friendly)\nGitKraken (feature-rich GUI)\nGitHub web interface (quick edits + reviews)\n\nMore alternatives: Git GUI clients"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-git-tracks",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-git-tracks",
    "title": "JSC 370: Data Science II",
    "section": "What Git tracks",
    "text": "What Git tracks"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#key-terms-youll-hear-a-lot",
    "href": "slides/02-version-control/JSC370-slides-02.html#key-terms-youll-hear-a-lot",
    "title": "JSC 370: Data Science II",
    "section": "Key terms you’ll hear a lot",
    "text": "Key terms you’ll hear a lot\n\nRepository (repo): the project + its version history\nCommit: a saved snapshot with a message (and an author + timestamp)\nBranch: an independent line of development (safe experimentation)\nMerge / Pull request: integrating changes back together\nRemote: a copy of the repo hosted elsewhere (e.g., GitHub)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#git-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#git-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Git workflow",
    "text": "Git workflow"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#setting-up-the-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#setting-up-the-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Setting up the workflow",
    "text": "Setting up the workflow\n\nGo to GitHub and sign in.\nCreate a repository (name it, choose public/private as appropriate, add a README).\nClone it (copy it onto your local machine).\nMake sure Git knows who you are (git config) and that authentication works (HTTPS token or SSH key).\n\nNote: We assume Git is installed: https://git-scm.com"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#cloning-visual",
    "href": "slides/02-version-control/JSC370-slides-02.html#cloning-visual",
    "title": "JSC 370: Data Science II",
    "section": "Cloning (visual)",
    "text": "Cloning (visual)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo",
    "href": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo",
    "title": "JSC 370: Data Science II",
    "section": "Workflow for an existing repo",
    "text": "Workflow for an existing repo\n\nStart by syncing (if you collaborate):\n\ngit pull\n\nMake changes in your editor.\nInspect what changed:\n\ngit status\n\ngit diff\n\nStage changes (choose what will go into the next commit):\n\ngit add &lt;file&gt; (or git add .)\n\nCommit with a clear message:\n\ngit commit -m \"Explain *why* you changed it\"\n\nPush your commits to GitHub: git push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo-cont",
    "href": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo-cont",
    "title": "JSC 370: Data Science II",
    "section": "Workflow for an existing repo (con’t)",
    "text": "Workflow for an existing repo (con’t)\nUndo helpers (common):\n\nUnstage a file: git restore --staged &lt;file&gt;\nDiscard local edits to a file: git restore &lt;file&gt;\n(Older syntax you may see online: git checkout -- &lt;file&gt;)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-0-introduce-yourself",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-0-introduce-yourself",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 0: Introduce yourself",
    "text": "Hands-on 0: Introduce yourself\nIn terminal set up your Git identity (this writes to your global Git config):\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@email.com\"\nCheck what Git thinks your settings are:\ngit config --list\n(Press q to exit the pager.)\nFor more see the config customizations on the git site"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo\nGoal: create a GitHub repo and make your first commit.\n\nCreate a new repository on GitHub (e.g., JSC370). Include a README.md.\nOn your computer, choose where you want the project folder to live. Change to that directory.\nClone the repository (copy the HTTPS/SSH URL from GitHub). Then in terminal git clone https://github.com//.git\nEdit README.md (VS Code is fine).\nStage + commit + push:\n\ngit add README.md\ngit commit -m “Edit README”\ngit push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-1",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo\nSome useful checks are to see what’s pending:\n- git status\n- See commit history:\ngit log --oneline --decorate --graph --all"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-2",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-3",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-local-first",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-local-first",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Local first",
    "text": "Hands-on 1: Local first\nThis creates a local repo (no GitHub yet), just to practice the cycle:\nmkdir -p ~/JSC370-demo\ncd ~/JSC370-demo\n\ngit init\necho \"Hello Git\" &gt; README.md\n\ngit status\ngit add README.md\ngit commit -m \"Add README\"\n\ngit log --oneline"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-connect-your-local-repo-to-github",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-connect-your-local-repo-to-github",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Connect your local repo to GitHub",
    "text": "Hands-on 1: Connect your local repo to GitHub\nOn GitHub, create a new empty repo (do not add a README if you already have one locally). Then in your local project folder, add the GitHub repo as a remote named origin:\ngit remote add origin https://github.com/&lt;you&gt;/&lt;repo&gt;.git\nConfirm the remote was added:\ngit remote -v\nPush your local commits to GitHub\ngit push -u origin main\n(if your default branch is master instead of main git push -u origin master or if main doesn’t exist yet locally you can create or rename it git branch -M main then run the push command above.)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#removing-a-mistakenly-stagedtracked-file",
    "href": "slides/02-version-control/JSC370-slides-02.html#removing-a-mistakenly-stagedtracked-file",
    "title": "JSC 370: Data Science II",
    "section": "Removing a mistakenly staged/tracked file",
    "text": "Removing a mistakenly staged/tracked file\nIf you accidentally added a file you don’t want to track (example: class-notes.docx):\ngit rm --cached class-notes.docx\nThis removes it from Git tracking but not from your computer.\nThen prevent it from being tracked again using .gitignore"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#example-.gitignore",
    "href": "slides/02-version-control/JSC370-slides-02.html#example-.gitignore",
    "title": "JSC 370: Data Science II",
    "section": "Example .gitignore",
    "text": "Example .gitignore\nExample adapted from Pro Git\n# ignore all .a files\n*.a\n\n# but do track lib.a, even though you're ignoring .a files above\n!lib.a\n\n# only ignore the TODO file in the current directory, not subdir/TODO\n/TODO\n\n# ignore all files in any directory named build\nbuild/\n\n# ignore doc/notes.txt, but not doc/server/arch.txt\ndoc/*.txt\n\n# ignore all .pdf files in the doc/ directory and any of its subdirectories\ndoc/**/*.pdf"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branches-forks-pull-requests-merge-conflicts",
    "href": "slides/02-version-control/JSC370-slides-02.html#branches-forks-pull-requests-merge-conflicts",
    "title": "JSC 370: Data Science II",
    "section": "Branches, Forks, Pull Requests, Merge Conflicts",
    "text": "Branches, Forks, Pull Requests, Merge Conflicts\nA typical flow is: branch (or fork + branch) → pull request → merge → resolve conflicts (if needed)\nThese concepts make collaboration (mostly) painless:\n\nBranches: work in parallel without breaking main\nForks: work on a copy of a repo when you don’t have write access\nPull Requests: propose + review changes before merging\nMerge conflicts: what happens when Git can’t auto-combine edits"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-vs-fork",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-vs-fork",
    "title": "JSC 370: Data Science II",
    "section": "Branch vs Fork",
    "text": "Branch vs Fork\nBranch = a new line of work inside the same repository\nFork = your own copy of the entire repository under your account\nRule of thumb:\n\nWorking in a shared class/team repo → branch\nContributing to a repo you can’t write to → fork"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch",
    "title": "JSC 370: Data Science II",
    "section": "Branch",
    "text": "Branch\n\nRepo: course-repo\nYou create: student/meredith-lab2\nYou push to the same repo\nPR: student/meredith-lab2 → main\n\nBest for: teams/classes with shared access"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#fork",
    "href": "slides/02-version-control/JSC370-slides-02.html#fork",
    "title": "JSC 370: Data Science II",
    "section": "Fork",
    "text": "Fork\n\nUpstream repo: org/course-repo\nYour fork: yourname/course-repo\nYou work in your fork (often on a branch)\nPR: yourname:branch → org:main\n\nBest for: open-source external projects with no write access"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branches-what-problem-do-they-solve",
    "href": "slides/02-version-control/JSC370-slides-02.html#branches-what-problem-do-they-solve",
    "title": "JSC 370: Data Science II",
    "section": "Branches: what problem do they solve?",
    "text": "Branches: what problem do they solve?\nWithout branches:\n\nEveryone edits main\nWork collides\nIt’s hard to experiment safely\n\nWith branches:\n\nmain stays stable\nEach feature/bugfix happens on its own branch\nChanges are merged back only when ready\n\nBranches are easy in Git: creating/switching is fast."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-naming-conventions",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-naming-conventions",
    "title": "JSC 370: Data Science II",
    "section": "Branch naming conventions",
    "text": "Branch naming conventions\nIndustry-style examples: feature, bugfix, hotfix, release, documentation\n\nfeature/lab1\nbugfix/path-images\ndocs/update-syllabus\n\nTip: use short, descriptive names. Avoid temp and final2. Here are some additional conventions"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-naming-convention-for-the-course",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-naming-convention-for-the-course",
    "title": "JSC 370: Data Science II",
    "section": "Branch naming convention for the course",
    "text": "Branch naming convention for the course\nCourse pattern student/&lt;name&gt;-lab1\nExample: student/meredith-lab2\nWhy this works:\n\ncommunicates who owns the branch\ncommunicates what it’s for (e.g. lab number)\navoids confusion with repo folders (the / is just naming)\n\n\n\n\n\n\n\nNote\n\n\nBranch names are labels, not file paths. Tools may group student/* branches together, but Git treats the whole string as the branch name."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#common-branch-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#common-branch-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Common branch workflow",
    "text": "Common branch workflow\n\nStart from an up-to-date main\nCreate a new branch\nMake changes and commit on the branch\nPush the branch to GitHub"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Branch workflow",
    "text": "Branch workflow\ngit switch main\ngit pull\n\ngit switch -c student/meredith-lab2\n# edit files...\n\ngit add .\ngit commit -m \"Add lab 2 report\"\ngit push -u origin student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-do-switch-and--c-mean",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-do-switch-and--c-mean",
    "title": "JSC 370: Data Science II",
    "section": "What do switch and -c mean?",
    "text": "What do switch and -c mean?\ngit switch is the Git command to move between branches\n(older ways often use git checkout for this).\n\ngit switch main\n“Move my working directory to the main branch.”\ngit switch -c student/meredith-lab2\n-c means create a new branch and switch to it immediately.\n\nSo this is equivalent to two steps:\ngit branch student/meredith-lab2\ngit switch student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-start-from-main-and-pull",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-start-from-main-and-pull",
    "title": "JSC 370: Data Science II",
    "section": "Why do we start from main and pull?",
    "text": "Why do we start from main and pull?\ngit switch main\ngit pull\n\nEnsures your branch starts from the latest main\nReduces merge conflicts later\nMakes your Pull Request easier to review"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#how-these-concepts-connect",
    "href": "slides/02-version-control/JSC370-slides-02.html#how-these-concepts-connect",
    "title": "JSC 370: Data Science II",
    "section": "How these concepts connect",
    "text": "How these concepts connect\n\nBranch: where you do your work safely\nCommit: save a snapshot with a message\nPush: publish your branch to GitHub\nPull Request: ask to merge your branch into main\nMerge: integrate the branch work into main\nConflict: Git needs you to decide how to combine edits"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#pull-requests-what-are-they",
    "href": "slides/02-version-control/JSC370-slides-02.html#pull-requests-what-are-they",
    "title": "JSC 370: Data Science II",
    "section": "Pull Requests: what are they?",
    "text": "Pull Requests: what are they?\nA Pull Request (PR) is:\n\na proposal to merge one branch into another (often → main)\na review space (comments, approvals, requested changes)\na record of what changed and why (discussion + diff + commits)\n\nPRs are the standard way to collaborate on GitHub."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-use-pull-requests",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-use-pull-requests",
    "title": "JSC 370: Data Science II",
    "section": "Why use Pull Requests?",
    "text": "Why use Pull Requests?\nPRs help you:\n\ncatch bugs early (someone else reads your diff)\nenforce project standards (formatting, tests, style)\ndocument decisions (“why did we do this?”)\nreduce “surprise merges” into main"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-makes-a-good-pull-request",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-makes-a-good-pull-request",
    "title": "JSC 370: Data Science II",
    "section": "What makes a good Pull Request?",
    "text": "What makes a good Pull Request?\n\nSmall enough to review (avoid mega-PRs)\nClear title + description\nExplains intent: what changed and why\nScreenshots/output examples when relevant\nLinks to an issue (if you use issues)\nIncludes only relevant files (no accidental large data, secrets, etc.)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#example-pull-request-terminal-commands",
    "href": "slides/02-version-control/JSC370-slides-02.html#example-pull-request-terminal-commands",
    "title": "JSC 370: Data Science II",
    "section": "Example Pull Request terminal commands",
    "text": "Example Pull Request terminal commands\n# Start from main and get the latest changes\ngit switch main\ngit pull\n\n# Create a new branch for your lab work\ngit switch -c student/meredith-lab2\n\n#  Do your work (edit files in VS Code or any editor)\n# (example files you might create/edit)\n# - train.py\n# - requirements.txt\n# - README.md\n\n# Check what changed\ngit status\ngit diff\n\n# Stage and commit\ngit add train.py requirements.txt README.md\ngit commit -m \"Lab 2: add reproducible model training script\"\n\n# Push the branch to GitHub\ngit push -u origin student/meredith-lab2\n\n# Open a Pull Request on GitHub:\n#    student/meredith-lab2  --&gt;  main\n# After review changes are requested:\n# Make edits, then repeat add/commit/push\ngit add .\ngit commit -m \"Address PR feedback\"\ngit push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#merging-what-does-it-mean",
    "href": "slides/02-version-control/JSC370-slides-02.html#merging-what-does-it-mean",
    "title": "JSC 370: Data Science II",
    "section": "Merging: what does it mean?",
    "text": "Merging: what does it mean?\nMerging integrates two lines of work by combining their histories:\n\nFast-forward merge: main simply moves forward (no divergence)\n3-way merge: Git creates a new merge commit that joins two lines of work\n\nEither way, the goal is the same: integrate branch work into main."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#merge-conflicts-what-are-they",
    "href": "slides/02-version-control/JSC370-slides-02.html#merge-conflicts-what-are-they",
    "title": "JSC 370: Data Science II",
    "section": "Merge conflicts: what are they?",
    "text": "Merge conflicts: what are they?\nA merge conflict happens when:\n\ntwo branches edited the same lines in the same file, and\nGit can’t determine how to combine them safely\n\nImportant:\n\nConflicts are normal in collaboration\nThey’re not “errors” so much as “decisions Git asks humans to make”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#when-do-conflicts-happen-most",
    "href": "slides/02-version-control/JSC370-slides-02.html#when-do-conflicts-happen-most",
    "title": "JSC 370: Data Science II",
    "section": "When do conflicts happen most?",
    "text": "When do conflicts happen most?\n\nLong-lived branches (you drift far from main)\nMany people editing the same file\nMoving/renaming files while someone else edits them"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#preventing-conflicts-best-practices",
    "href": "slides/02-version-control/JSC370-slides-02.html#preventing-conflicts-best-practices",
    "title": "JSC 370: Data Science II",
    "section": "Preventing conflicts (best practices)",
    "text": "Preventing conflicts (best practices)\n\nPull often (or merge main into your branch regularly)\nKeep PRs small and merge them sooner\nAvoid huge “format everything” commits mixed with logic changes\nCommunicate: “I’m editing slides/week2.qmd today”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-a-conflict-looks-like",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-a-conflict-looks-like",
    "title": "JSC 370: Data Science II",
    "section": "What a conflict looks like",
    "text": "What a conflict looks like\nGit inserts markers like this into a file:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the version from main\n=======\nThis is the version from your branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#resources",
    "href": "slides/02-version-control/JSC370-slides-02.html#resources",
    "title": "JSC 370: Data Science II",
    "section": "Resources",
    "text": "Resources\n\nGit everyday commands: man giteveryday in terminal\nGitHub’s cheat sheets: https://github.github.com/training-kit/￼\nPro Git (free online): https://git-scm.com/book￼\nGit exercises: https://gitexercises.fracz.com/￼\nGitHub Guides (YouTube): https://www.youtube.com/user/GitHubGuides￼"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#other-tools-to-explore",
    "href": "slides/02-version-control/JSC370-slides-02.html#other-tools-to-explore",
    "title": "JSC 370: Data Science II",
    "section": "Other tools to explore",
    "text": "Other tools to explore\n\nJira￼: issue/task tracking + workflow management\nGit + Jira integrations exist via many clients"
  },
  {
    "objectID": "labs/lab2/lab02-github.html",
    "href": "labs/lab2/lab02-github.html",
    "title": "Lab 02 - GitHub",
    "section": "",
    "text": "In this lab, you are expected to learn/put in practice the following skills:\n\nCreating your own repo for JSC370\nBranching on GitHub\nGit workflow clone/commit/push\nUsing pull requests (PR)"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-1-clone-the-repo-and-create-a-branch",
    "href": "labs/lab2/lab02-github.html#step-1-clone-the-repo-and-create-a-branch",
    "title": "Lab 02 - GitHub",
    "section": "Step 1: Clone the repo and create a branch",
    "text": "Step 1: Clone the repo and create a branch\nBranching is a core feature of Git that allows you to create an independent line of development. When you create a branch, you’re making a copy of the code at that point in time where you can make changes without affecting the main branch. This is how teams collaborate: each person works on their own branch, then proposes their changes via a pull request (i.e. “I have this great update to the project! Would you like to add it by pulling my branch into main?”).\nFirst, clone the repository to your local machine using the git clone command:\ncd &lt;your-directory&gt;\ngit clone &lt;repository-url&gt;\nOnce you have the repository on your local machine, create a new branch for your changes. Use your name or a descriptive name for the branch2:\ncd &lt;repository-name&gt;\ngit switch -c &lt;your-name&gt;\nThe -c flag creates a new branch and switches to it. You can verify you’re on your new branch by running:\ngit branch\nThis will show all branches with an asterisk (*) next to your current branch.\nWhen you’re ready to push your local branch to the remote repository for the first time, use:\ngit push --set-upstream origin your-name\nThis sets up tracking between your local branch and the remote, so future pushes can simply use git push. Now you’re ready to make changes without affecting the main branch!"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-2-modifying-the-corresponding-line",
    "href": "labs/lab2/lab02-github.html#step-2-modifying-the-corresponding-line",
    "title": "Lab 02 - GitHub",
    "section": "Step 2: Modifying the corresponding line",
    "text": "Step 2: Modifying the corresponding line\nIf you got the correct copy, you should find a very simple repository with only two files: .gitignore and README.md. The first file is just a reference file for git to know what things it should be “looking at” (checkout the lecture slides), so we will ignore it at this time (pun intended). The second file is the one that we will be playing with. The README file, which happens to be a Markdown file, contains, or at least will contain, your and your team members biographies. Here is what you need to do:\n\nFind the line with your name.\nIn that single line (i.e. not spanning multiple lines), write something about yourself, e.g. “I am from XYZ, I love doing ABC, …”.\n(optional) if you feel like it, add at the end of the line a picture of yourself (or avatar) using either html or markdown. This will require you to include the figure in the repo (if you are not linking a web fig).\nCommit the changes and push the changes to your branch using git commit and git push, e.g.\n\ngit add .\ngit commit -m \"[A short but meaningful message]\"\ngit push\nYou are now one step closer to make your first “pull request”. We will see how that happens in the next part."
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-3-do-the-pull-request",
    "href": "labs/lab2/lab02-github.html#step-3-do-the-pull-request",
    "title": "Lab 02 - GitHub",
    "section": "Step 3: Do the pull request",
    "text": "Step 3: Do the pull request\nThis is the final step. Overall, pull requests (PR) are as complex as the proposed changes are. The PR that you are about to make should go smoothly, yet, any time that you make a new PR, the changes should be able to be merged in the original repository without conflicts. Conflicts may only appear if the proposed changes are out-dated with respect to the main repository, meaning that the main repository was modified after you created your branch and your proposed changes cannot be merged without generating conflicts3. For now, let’s just look at the simple case.\nTo create the PR, you just need to go to the repository on GitHub and click on the “Compare & pull request” button for your branch:\n\n\n\nSource: Screenshot\n\n\nThis will create a PR in the reference repository. GitHub will automatically analyze the PR and check whether merging the PR to the main branch will result in a conflict or not. If all is OK, then the owner/admin of the repository can merge the PR. Otherwise, if there’s a conflict, you can go back to your local repo, make the needed changes, commit the changes, and push the changes to your branch. In this stage, the PR will automatically update to reflect the new changes you made.\nFor more information checkout Creating a pull request on GitHub.\n\nSubmit a pull request to https://github.com/JSC370/JSC370-lab2-2026-whoami\nAdd details of the pull request below (the commit id of your change (you can find this using git log)\n\ncommit:\nAuthor:\nDate:"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#footnotes",
    "href": "labs/lab2/lab02-github.html#footnotes",
    "title": "Lab 02 - GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTeam-members could be working on the same file but editing different lines of code. If this is the case, after pull/push, git will integrate the changes without conflicts.↩︎\nFor more on branch naming conventions, see Naming conventions for Git Branches.↩︎\nMore info about how to deal with conflicts in this very neat post on stackoverflow.com How to resolve merge conflicts in Git. GitHub also has a way to solve conflicts in PRs, but this is only available to the admins of target repo. More info here.↩︎"
  },
  {
    "objectID": "labs/JSC370-lab-01.html",
    "href": "labs/JSC370-lab-01.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#overview",
    "href": "labs/JSC370-lab-01.html#overview",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#what-you-will-submit-to-markus",
    "href": "labs/JSC370-lab-01.html#what-you-will-submit-to-markus",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "What you will submit to MarkUs",
    "text": "What you will submit to MarkUs\n\nThis completed .qmd file. Download the lab .qmd on GitHub\nThe rendered HTML output (JSC370-lab-01.html)"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#before-you-start",
    "href": "labs/JSC370-lab-01.html#before-you-start",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "0. Before you start",
    "text": "0. Before you start\n\nIf you do not already have it, install VS Code. Then open this file in .\nIn VS Code open the terminal (Terminal -&gt; New Terminal). Recall you can toggle between terminals in VS Code. You should see a list of terminals on the bottom right panel."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#quarto",
    "href": "labs/JSC370-lab-01.html#quarto",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "1. Quarto",
    "text": "1. Quarto\nIn the terminal, run:\nquarto --version\nIf it is not installed, go to the Quarto download page and install for your OS: https://quarto.org/docs/download/.\nAfter installing, quit and reopen VS Code (so PATH updates), then run quarto –version again.\nPASTE YOUR VERSION HERE\nNow install the Quarto extension in VS Code:\n\nIn VS Code, click the Extensions icon (left sidebar), or press:\n\nmacOS: Cmd+Shift+X\nWindows/Linux: Ctrl+Shift+X\n\nSearch: Quarto\nInstall: Quarto (publisher: quarto-dev)\nYou may need to reload VS Code.\n\nPreview this lab in VS Code\nOpen this .qmd file, then preview it with Command Palette (Cmd/Ctrl+Shift+P) and select Quarto: Preview.\nTo stop preview: press Ctrl+C in the terminal running the preview."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#confirm-python-is-installed",
    "href": "labs/JSC370-lab-01.html#confirm-python-is-installed",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "2. Confirm Python is installed",
    "text": "2. Confirm Python is installed\nYou can try this in terminal:\n#| echo: true\npython3 --version\n(If python3 doesn’t work, try python –version in terminal.) Paste the output, but the chunk above should do it for you if installed correctly."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#install-core-packages-in-terminal",
    "href": "labs/JSC370-lab-01.html#install-core-packages-in-terminal",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "3. Install core packages in terminal",
    "text": "3. Install core packages in terminal\nRun the following in terminal. It is best to install packages in the terminal (not from inside Quarto).\npython3 -m pip install -U pip\npython3 -m pip install numpy pandas matplotlib jupyter ipykernel plotnine plotly\nAlso install the Jupyter extension in VS Code, similarly to how we installed the Quarto extension above (search for Jupyter).\nConfirm Quarto can see Jupyter\nquarto check jupyter\nPASTE OUTPUT HERE"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#verify-your-python-environment",
    "href": "labs/JSC370-lab-01.html#verify-your-python-environment",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "4. Verify your Python environment",
    "text": "4. Verify your Python environment\nRun the chunk below. If this fails, double-check that VS Code is using the Python environment where you installed packages."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#your-turn-make-a-plot",
    "href": "labs/JSC370-lab-01.html#your-turn-make-a-plot",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "5. Your turn: make a plot",
    "text": "5. Your turn: make a plot\nUse the plotnine library (ggplot-like) to make a simple scatterplot. Update the code below:\n\nTitle must include your name (e.g., \"Penguin bill measurements — Your Name\").\nChange the point transparency by setting alpha to a new value."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#render-.qmd-to-.html",
    "href": "labs/JSC370-lab-01.html#render-.qmd-to-.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "6. Render ‘.qmd’ to ‘.html’",
    "text": "6. Render ‘.qmd’ to ‘.html’\nIn the terminal, make sure you are in the folder containing this file, then run:\nquarto render JSC370-lab-01.qmd\nThis should create an HTML file in the same folder, e.g. JSC370-lab-01.html.\nConfirm it exists:\nls -l JSC370-lab-01.html\nPrint out the output and make sure it is in this lab. Also open the HTML in your browser to confirm it looks correct."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "JSC370: Data Science II",
    "section": "Resources",
    "text": "Resources\n\nHelpers and Templates\n\nQuarto (download/install)\nQuarto in VS Code\nQuarto VS Code extension\nJupyter extension for VS Code\nCookiecutter Data Science\nJupytext (pair notebooks with .py/.md)\nQuarto: Markdown Basics\nQuarto: Gallery (examples)\nMarkdown Guide\nMarkdown tutorial\n\n\n\nGuides\n\nPEP 8 – Style Guide for Python Code\nGoogle Python Style Guide\npandas: 10 minutes to pandas\nNumPy user guide\nMatplotlib tutorials\nseaborn documentation\nVega-Altair documentation\nscikit-learn: Getting Started\npre-commit\npytest: Getting Started\n\n\n\nTools\n\nVisual Studio Code\nPython\nCore libraries:\n\npandas\nNumPy\nscikit-learn\n\nData access & wrangling:\n\nRequests\nBeautiful Soup\nDuckDB Python API\nSQLAlchemy\n\nPublishing / reproducibility:\n\nQuarto\nPandoc\nGit\nGitHub\nGitHub Classroom\nGitHub Pages"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "JSC370: Data Science II",
    "section": "Data",
    "text": "Data\nMany of these websites provide APIs and/or bulk downloads.\n\nCanadian Data\n\nGovernment of Canada Open Data (Open Government Portal)\nStatistics Canada (Census + surveys)\nStatistics Canada Web Data Service (API)\nCanada GIS Data\nUniversity of Toronto Library (Geospatial data guides)\nCity of Toronto Open Data\nToronto Police Service Open Data\nOntario Data Catalogue\nPublic Health Ontario Open Data\nBritish Columbia Data Catalogue\n\n\n\nEnvironmental and Climate Data\n\nUS EPA Air Quality Data\nUS EPA AQS Data API (Air Quality System)\nNOAA National Centers for Environmental Information (NCEI)\nNOAA NCEI Access Data Service (API)\nNorth American Regional Climate Change Assessment Program (NARCCAP)\nNatural Resources Canada (Geospatial data/tools)\nCoastal wave / buoy data (CDIP, UCSD)\nGreat Lakes Bathymetry (NOAA)\nUS Energy Information Administration (EIA)\n\n\n\nInternational and Global Data\n\nUN Geospatial Hub\nUNdata API manual\nWorld Bank Open Data\nWorld Bank Developer / API info\nFAO FAOSTAT (global food/agriculture stats)\nFAOSTAT API entry point\nNASA Earthdata (data catalog)\n\n\n\nUS Data\n\nData.gov (catalog)\nData.gov Geospatial datasets\nUS Census (NHGIS)\nNYU Spatial Data Repository\nGoogle Earth Engine datasets\nGoogle Dataset Search\nFiveThirtyEight data (GitHub)\nLos Angeles Open Data\n\n\n\nHealth and Biological Data\n\nSEER (NIH/NCI cancer surveillance)\nWorld Health Organization (GHO OData API)\nCDC Open Data\nCalifornia Health & Human Services Open Data\nCanada COVID tracking (community project)\nUniProt (proteins)\nGene Ontology\n\n\n\nNews, Media, and Text Data\n\nNew York Times APIs (specs repo)\nThe Guardian Open Platform\nGDELT Project (global news/events)\nCommon Crawl (web archive)\nWikipedia / Wikimedia APIs (MediaWiki REST API)\nThe Movie Database\n\n\n\nSocial Networks and Platforms\n\nX (Twitter) Developer Platform\nGitHub REST API\nInstagram Platform\nLinkedIn API documentation (Microsoft Learn)\nSpotify Web API\nZillow Group Data & APIs\n\n\n\nAcademic Publications and Research Data\n\nFigshare\nZenodo\nHarvard Dataverse\nElsevier Developer APIs\nOpenAlex (scholarly metadata + API)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#goals",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#goals",
    "title": "JSC 370: Data Science II",
    "section": "Goals",
    "text": "Goals\n\nLoad in large datasets stored locally and remotely (GitHub), check memory issues\nUnderstand what EDA is (and is not)\nBuild a repeatable EDA workflow\nMake plots that reveal structure, not just decoration\nAvoid common visualization pitfalls"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#data-science-pipeline",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#data-science-pipeline",
    "title": "JSC 370: Data Science II",
    "section": "Data Science Pipeline",
    "text": "Data Science Pipeline\n\nWe will work on the blue words today"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#what-is-exploratory-data-analysis",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#what-is-exploratory-data-analysis",
    "title": "JSC 370: Data Science II",
    "section": "What is Exploratory Data Analysis?",
    "text": "What is Exploratory Data Analysis?\n\nEDA = iterative exploration and building modeling intuition\nFocus on data distributions, missingness, relationships, anomalies\nCreate: summary stats, exploratory plots\nOutput: questions, hypotheses, and next steps"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-data-analysis",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-data-analysis",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the process of summarizing data\nIt should be the first step in your analysis pipeline, and it serves to:\n\n✅ check data (import issues, outliers, missing values, data errors)\n🧹 clean data (fix implausible values, remove missing if necessary, rename, etc.)\n\\(\\Sigma\\) summary statistics of key variables (univariate and bivariate)\n📊 basic plots and graphs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#eda-checklist",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#eda-checklist",
    "title": "JSC 370: Data Science II",
    "section": "EDA Checklist",
    "text": "EDA Checklist\nEDA Checklist:\n\n\nFormulate a question\n\n\nRead in the data\n\n\nCheck the dimensions and headers and footers of the data\n\n\nCheck the variable types in the data\n\n\nTake a closer look at some/all of the variables\n\n\nValidate with an external source\n\n\nConduct some summary statistics to answer the initial question\n\n\nMake exploratory graphs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#case-study",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#case-study",
    "title": "JSC 370: Data Science II",
    "section": "Case Study",
    "text": "Case Study\nWe are going to use a dataset created from the National Center for Environmental Information NOAA/NCEI. The data are hourly measurements from weather stations across the continental U.S."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#formulate-a-question",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#formulate-a-question",
    "title": "JSC 370: Data Science II",
    "section": "Formulate a Question",
    "text": "Formulate a Question\nIt is a good idea to first have a question such as:\n\nwhat weather stations reported the hottest and coldest daily temperatures?\nwhat day of the month was on average the hottest?\nis there covariation between temperature and humidity in my dataset?"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data",
    "title": "JSC 370: Data Science II",
    "section": "Read in the data",
    "text": "Read in the data\nThere are several ways to read in data (some depend on the type of data you have):\n\npandas.read_csv() for delimited files\npandas.read_parquet() for parquet\npandas.read_feather() for feather\npandas.read_excel() for .xls and .xlsx\npyarrow.dataset / dask for larger-than-memory workflows"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data-1",
    "title": "JSC 370: Data Science II",
    "section": "Read in the data",
    "text": "Read in the data\nWe’ll use Python packages pandas, numpy, matplotlib, seaborn, plotnine\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom plotnine import ggplot, aes, geom_point, labs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-local-file",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-local-file",
    "title": "JSC 370: Data Science II",
    "section": "Example: read in local file",
    "text": "Example: read in local file\nIf your data are a gzipped delimited file (e.g., met_all.gz), pandas can read it directly.\n\n\nmet = pd.read_csv(\"met_all.gz\", compression=\"gzip\")\nmet.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n0\n690150\n93121\n2019\n8\n1\n0\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n37.2\n5\n10.6\n5\n1009.9\n5\n19.881266\n\n\n1\n690150\n93121\n2019\n8\n1\n1\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n35.6\n5\n10.6\n5\n1010.3\n5\n21.760980\n\n\n2\n690150\n93121\n2019\n8\n1\n2\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n34.4\n5\n7.2\n5\n1010.6\n5\n18.482122\n\n\n3\n690150\n93121\n2019\n8\n1\n3\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n33.3\n5\n5.0\n5\n1011.6\n5\n16.888622\n\n\n\n\n4 rows × 30 columns"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-github-file",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-github-file",
    "title": "JSC 370: Data Science II",
    "section": "Example: read in github file",
    "text": "Example: read in github file\nIf your data are a gzipped delimited file but stored on github, use\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nmet = pd.read_csv(url, compression=\"gzip\")    \nmet.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n0\n690150\n93121\n2019\n8\n1\n0\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n37.2\n5\n10.6\n5\n1009.9\n5\n19.881266\n\n\n1\n690150\n93121\n2019\n8\n1\n1\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n35.6\n5\n10.6\n5\n1010.3\n5\n21.760980\n\n\n2\n690150\n93121\n2019\n8\n1\n2\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n34.4\n5\n7.2\n5\n1010.6\n5\n18.482122\n\n\n3\n690150\n93121\n2019\n8\n1\n3\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n33.3\n5\n5.0\n5\n1011.6\n5\n16.888622\n\n\n\n\n4 rows × 30 columns"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-types-of-variables-in-our-dataset",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-types-of-variables-in-our-dataset",
    "title": "JSC 370: Data Science II",
    "section": "Check the types of variables in our dataset",
    "text": "Check the types of variables in our dataset\nWe can get the types of data (integer, float, character) from met.info. This also tells us the memory usage.\n\nmet.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2377343 entries, 0 to 2377342\nData columns (total 30 columns):\n #   Column             Dtype  \n---  ------             -----  \n 0   USAFID             int64  \n 1   WBAN               int64  \n 2   year               int64  \n 3   month              int64  \n 4   day                int64  \n 5   hour               int64  \n 6   min                int64  \n 7   lat                float64\n 8   lon                float64\n 9   elev               int64  \n 10  wind.dir           float64\n 11  wind.dir.qc        object \n 12  wind.type.code     object \n 13  wind.sp            float64\n 14  wind.sp.qc         object \n 15  ceiling.ht         float64\n 16  ceiling.ht.qc      int64  \n 17  ceiling.ht.method  object \n 18  sky.cond           object \n 19  vis.dist           float64\n 20  vis.dist.qc        object \n 21  vis.var            object \n 22  vis.var.qc         object \n 23  temp               float64\n 24  temp.qc            object \n 25  dew.point          float64\n 26  dew.point.qc       object \n 27  atm.press          float64\n 28  atm.press.qc       int64  \n 29  rh                 float64\ndtypes: float64(10), int64(10), object(10)\nmemory usage: 544.1+ MB"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#what-counts-as-very-large-data-on-a-laptop",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#what-counts-as-very-large-data-on-a-laptop",
    "title": "JSC 370: Data Science II",
    "section": "What counts as “very large” data on a laptop?",
    "text": "What counts as “very large” data on a laptop?\nOn laptops, “very large” usually means it stresses RAM and slows basic operations.\nA dataset can be “very large” even with only a few million rows if it has:\n\nmany columns\nlots of strings (object dtype)\nexpensive operations (joins, groupbys, sorting)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#practical-rule-of-thumb",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#practical-rule-of-thumb",
    "title": "JSC 370: Data Science II",
    "section": "Practical rule-of-thumb",
    "text": "Practical rule-of-thumb\nThink in terms of memory, not just rows × columns.\n\nComfortable: under ~1 GB in RAM\nLarge: ~1–5 GB (needs care: dtypes, filtering early)\nVery large: &gt; ~5 GB or doesn’t fit in RAM → you need a different approach/tool"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas",
    "title": "JSC 370: Data Science II",
    "section": "Measure size in pandas",
    "text": "Measure size in pandas\n\nCalculate the total memory used by the DataFrame (convert bytes -&gt; GB)\n\n\nmet.memory_usage(deep=True).sum() / 1024**3\n\nnp.float64(1.401238577440381)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas-1",
    "title": "JSC 370: Data Science II",
    "section": "Measure size in pandas",
    "text": "Measure size in pandas\n\nFind the biggest memory columns (this time in MB)\n\n\n(met.memory_usage(deep=True).sort_values(ascending=False) / 1024**2).head(10)\n\ndew.point.qc         113.360548\ntemp.qc              113.360548\nvis.var              113.360548\nwind.type.code       113.360548\nsky.cond             113.360548\nceiling.ht.method    113.360548\nvis.dist.qc          103.298048\nvis.var.qc           102.860548\nwind.sp.qc            99.797945\nwind.dir.qc           85.994595\ndtype: float64"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#why-strings-make-data-big",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#why-strings-make-data-big",
    "title": "JSC 370: Data Science II",
    "section": "Why strings make data “big”",
    "text": "Why strings make data “big”\n\nIn pandas, object columns (strings) are often the memory hog.\ndtypes tells is what kind of variables we have in a dataset.\n\n\nmet.dtypes\n\nUSAFID                 int64\nWBAN                   int64\nyear                   int64\nmonth                  int64\nday                    int64\nhour                   int64\nmin                    int64\nlat                  float64\nlon                  float64\nelev                   int64\nwind.dir             float64\nwind.dir.qc           object\nwind.type.code        object\nwind.sp              float64\nwind.sp.qc            object\nceiling.ht           float64\nceiling.ht.qc          int64\nceiling.ht.method     object\nsky.cond              object\nvis.dist             float64\nvis.dist.qc           object\nvis.var               object\nvis.var.qc            object\ntemp                 float64\ntemp.qc               object\ndew.point            float64\ndew.point.qc          object\natm.press            float64\natm.press.qc           int64\nrh                   float64\ndtype: object"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously",
    "title": "JSC 370: Data Science II",
    "section": "Big data can be read in more cautiously",
    "text": "Big data can be read in more cautiously\n\nIf the file is really big (ours is borderline), we could read only the columns we need. This is where having a question in mind is useful. For ours, we probably just want station location (lat, lon), year, month, day, hour, temperature and humidity.\n\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nusecols = [\"USAFID\",\"lat\",\"lon\", \"year\", \"month\",\"day\",\"hour\",\"temp\", \"rh\"]\nmet_small = pd.read_csv(\n    url,\n    compression=\"gzip\",\n    usecols=usecols\n)\nmet_small.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nyear\nmonth\nday\nhour\nlat\nlon\ntemp\nrh\n\n\n\n\n0\n690150\n2019\n8\n1\n0\n34.3\n-116.166\n37.2\n19.881266\n\n\n1\n690150\n2019\n8\n1\n1\n34.3\n-116.166\n35.6\n21.760980\n\n\n2\n690150\n2019\n8\n1\n2\n34.3\n-116.166\n34.4\n18.482122\n\n\n3\n690150\n2019\n8\n1\n3\n34.3\n-116.166\n33.3\n16.888622"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously-1",
    "title": "JSC 370: Data Science II",
    "section": "Big data can be read in more cautiously",
    "text": "Big data can be read in more cautiously\nChunking in pandas means reading in a file in pieces so you don’t load the whole dataset into RAM at once.\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nchunks = pd.read_csv(url, compression=\"gzip\", chunksize=200_000)\nn = 0\ntemp_sum = 0.0\nfor ch in chunks:\n    temp_sum += ch[\"temp\"].sum(skipna=True)\n    n += ch[\"temp\"].notna().sum()\ntemp_sum / n\n\nnp.float64(23.590542469664527)\n\n\nThis computes the overall mean temperature without ever storing the full dataset:\n\ntemp_sum accumulates the sum of temps across chunks.\nn accumulates how many non-missing temps you saw.\ntemp_sum / n is the mean.\n\nWe’re basically doing: \\(\\bar{x} = \\frac{\\sum x_i}{\\#\\{x_i \\text{ not missing}\\}}\\)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data",
    "title": "JSC 370: Data Science II",
    "section": "Check the data",
    "text": "Check the data\nOnce we have loaded the data we should check the dimensions of the dataset. In pandas, the we use met.shape.\n\nprint(\"shape:\", met.shape)\nprint(\"n_rows:\", met.shape[0])\nprint(\"n_cols:\", met.shape[1])\n\nshape: (2377343, 30)\nn_rows: 2377343\nn_cols: 30"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-summary",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-summary",
    "title": "JSC 370: Data Science II",
    "section": "Check the data: Summary",
    "text": "Check the data: Summary\n\nWe see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables.\nWe know that we are supposed to have hourly measurements of weather data for the month of August 2019 for the entire US. We should check that we have all of these components. Let us check:\n\n\nthe year, month, hours\nthe range of locations (latitude and longitude)\ntemperature\nrelative humidity"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely",
    "title": "JSC 370: Data Science II",
    "section": "Check the data more closely",
    "text": "Check the data more closely\nWe can get a quick summary from met.describe.\n\nmet.describe()\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\nwind.dir\nwind.sp\nceiling.ht\nceiling.ht.qc\nvis.dist\ntemp\ndew.point\natm.press\natm.press.qc\nrh\n\n\n\n\ncount\n2.377343e+06\n2.377343e+06\n2377343.0\n2377343.0\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n1.592053e+06\n2.297650e+06\n2.256068e+06\n2.377343e+06\n2.296387e+06\n2.317254e+06\n2.311055e+06\n711069.000000\n2.377343e+06\n2.310917e+06\n\n\nmean\n7.230995e+05\n2.953885e+04\n2019.0\n8.0\n1.599589e+01\n1.134106e+01\n3.956097e+01\n3.794132e+01\n-9.214509e+01\n4.158152e+02\n1.850082e+02\n2.459166e+00\n1.616648e+04\n5.027185e+00\n1.492117e+04\n2.359054e+01\n1.702084e+01\n1014.164390\n7.728208e+00\n7.164138e+01\n\n\nstd\n2.156678e+03\n3.336921e+04\n0.0\n0.0\n8.917096e+00\n6.860069e+00\n1.724761e+01\n5.171552e+00\n1.195324e+01\n5.737842e+02\n9.230697e+01\n2.147395e+00\n9.282465e+03\n1.249445e+00\n3.879577e+03\n6.059275e+00\n6.206226e+00\n4.059265\n2.018170e+00\n2.275361e+01\n\n\nmin\n6.901500e+05\n1.160000e+02\n2019.0\n8.0\n1.000000e+00\n0.000000e+00\n0.000000e+00\n2.455000e+01\n-1.242900e+02\n-1.300000e+01\n3.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n0.000000e+00\n-4.000000e+01\n-3.720000e+01\n960.500000\n1.000000e+00\n8.334298e-01\n\n\n25%\n7.209280e+05\n3.706000e+03\n2019.0\n8.0\n8.000000e+00\n5.000000e+00\n2.300000e+01\n3.396700e+01\n-9.801800e+01\n1.010000e+02\n1.200000e+02\n0.000000e+00\n3.048000e+03\n5.000000e+00\n1.609300e+04\n1.960000e+01\n1.380000e+01\n1011.800000\n5.000000e+00\n5.579015e+01\n\n\n50%\n7.227280e+05\n1.386000e+04\n2019.0\n8.0\n1.600000e+01\n1.100000e+01\n5.000000e+01\n3.835000e+01\n-9.170800e+01\n2.520000e+02\n1.800000e+02\n2.100000e+00\n2.200000e+04\n5.000000e+00\n1.609300e+04\n2.350000e+01\n1.810000e+01\n1014.100000\n9.000000e+00\n7.655374e+01\n\n\n75%\n7.250900e+05\n5.476700e+04\n2019.0\n8.0\n2.400000e+01\n1.700000e+01\n5.500000e+01\n4.194000e+01\n-8.298600e+01\n4.000000e+02\n2.600000e+02\n3.600000e+00\n2.200000e+04\n5.000000e+00\n1.609300e+04\n2.780000e+01\n2.170000e+01\n1016.400000\n9.000000e+00\n9.062916e+01\n\n\nmax\n7.268130e+05\n9.499800e+04\n2019.0\n8.0\n3.100000e+01\n2.300000e+01\n5.900000e+01\n4.894100e+01\n-6.831300e+01\n9.999000e+03\n3.600000e+02\n3.600000e+01\n2.200000e+04\n9.000000e+00\n1.600000e+05\n5.600000e+01\n3.600000e+01\n1059.900000\n9.000000e+00\n1.000000e+02\n\n\n\n\n\n\n\n\nNote that this doesn’t tell us if there are any missing data!"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely-1",
    "title": "JSC 370: Data Science II",
    "section": "Check the data more closely",
    "text": "Check the data more closely\nIt is a little harder to quickly summarize missing data in pandas, but here’s a way to count missing observations and calculate the percent missing.\n\nmissing = met.isna().sum().to_frame(\"n_missing\")\nmissing[\"pct_missing\"] = (missing[\"n_missing\"] / len(met)).round(4)\nmissing.sort_values(\"pct_missing\", ascending=False).head(20)\n\n\n\n\n\n\n\n\nn_missing\npct_missing\n\n\n\n\natm.press\n1666274\n0.7009\n\n\nwind.dir\n785290\n0.3303\n\n\nceiling.ht\n121275\n0.0510\n\n\nvis.dist\n80956\n0.0341\n\n\nwind.sp\n79693\n0.0335\n\n\ndew.point\n66288\n0.0279\n\n\nrh\n66426\n0.0279\n\n\ntemp\n60089\n0.0253\n\n\nlat\n0\n0.0000\n\n\nday\n0\n0.0000\n\n\natm.press.qc\n0\n0.0000\n\n\nyear\n0\n0.0000\n\n\ndew.point.qc\n0\n0.0000\n\n\nmonth\n0\n0.0000\n\n\ntemp.qc\n0\n0.0000\n\n\nvis.var.qc\n0\n0.0000\n\n\nvis.var\n0\n0.0000\n\n\nvis.dist.qc\n0\n0.0000\n\n\nsky.cond\n0\n0.0000\n\n\nlon\n0\n0.0000"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely",
    "title": "JSC 370: Data Science II",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIf we return to our initial question—what weather stations reported the hottest and coldest temperatures, we should take a closer look at our key variable, temperature (temp).\n\nmet[\"temp\"].describe()\n\ncount    2.317254e+06\nmean     2.359054e+01\nstd      6.059275e+00\nmin     -4.000000e+01\n25%      1.960000e+01\n50%      2.350000e+01\n75%      2.780000e+01\nmax      5.600000e+01\nName: temp, dtype: float64\n\n\n\nIt looks like the data are in Celsius."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely-1",
    "title": "JSC 370: Data Science II",
    "section": "Check variables more closely",
    "text": "Check variables more closely\n\nIt’s odd to have -40C in August.\nSubset rows where temp == -40.0 and select a few columns with loc, which is an indexer for selecting rows and columns.\n\n\nmet_ss = met.loc[met[\"temp\"] == -40.0, [\"USAFID\",\"hour\", \"lat\", \"lon\",  \"elev\", \"temp\", \"wind.sp\"]]\nmet_ss.shape\nmet_ss.head()\n\n\n\n\n\n\n\n\nUSAFID\nhour\nlat\nlon\nelev\ntemp\nwind.sp\n\n\n\n\n496047\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496048\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496049\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496050\n720717\n1\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496051\n720717\n1\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n\n\n\n\n\n\nIt appears that all of the data with -40C are from the same site and have missing. Points to a data issue."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#validate-against-an-external-source",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#validate-against-an-external-source",
    "title": "JSC 370: Data Science II",
    "section": "Validate against an external source",
    "text": "Validate against an external source\nWe should check outside sources to make sure that our data make sense. The observation with -40°C is suspicious, so we should look up the location of the weather station.\nGo to Google maps and enter the coordinates for the site with -40C (29.12, -89.55)\n\n\n\n\n\n\n\n\nIt doesn’t make much sense to have a -40C reading in the Gulf of Mexico off the coast of Louisiana."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#filter-implausible-values",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#filter-implausible-values",
    "title": "JSC 370: Data Science II",
    "section": "Filter implausible values",
    "text": "Filter implausible values\nFilter with loc and sort the data with sort_values in one line:\n\nmet = met.loc[met[\"temp\"] &gt; -40].sort_values(\"temp\")\nmet.head(20)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n1203054\n722817\n3068\n2019\n8\n1\n1\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203127\n722817\n3068\n2019\n8\n3\n11\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203128\n722817\n3068\n2019\n8\n3\n12\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203221\n722817\n3068\n2019\n8\n6\n21\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203224\n722817\n3068\n2019\n8\n6\n22\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203052\n722817\n3068\n2019\n8\n1\n0\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203225\n722817\n3068\n2019\n8\n6\n23\n3\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203220\n722817\n3068\n2019\n8\n6\n21\n39\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203222\n722817\n3068\n2019\n8\n6\n22\n32\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203223\n722817\n3068\n2019\n8\n6\n22\n41\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203053\n722817\n3068\n2019\n8\n1\n1\n6\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1105458\n722518\n12974\n2019\n8\n12\n3\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1011.6\n1\nNaN\n\n\n1105456\n722518\n12974\n2019\n8\n12\n1\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1010.3\n1\nNaN\n\n\n1105454\n722518\n12974\n2019\n8\n11\n23\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1010.3\n1\nNaN\n\n\n2370757\n726764\n94163\n2019\n8\n27\n11\n50\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\nC\n-5.0\nC\nNaN\n9\n86.265368\n\n\n2370758\n726764\n94163\n2019\n8\n27\n12\n10\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\n5\n-4.0\n5\nNaN\n9\n92.910834\n\n\n2370759\n726764\n94163\n2019\n8\n27\n12\n30\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\n5\n-4.0\n5\nNaN\n9\n92.910834\n\n\n2370760\n726764\n94163\n2019\n8\n27\n12\n50\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\nC\n-4.0\nC\nNaN\n9\n92.910834\n\n\n252488\n720411\n137\n2019\n8\n18\n12\n35\n36.422\n-105.290\n2554\n...\n5\nN\n5\n-2.4\n5\n-3.7\n5\nNaN\n9\n90.914749\n\n\n2370687\n726764\n94163\n2019\n8\n26\n12\n30\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-2.0\n5\n-3.0\n5\nNaN\n9\n92.966900\n\n\n\n\n20 rows × 30 columns\n\n\n\n\nWe should do a quick check of (38.767, -104.300) and (27.901, -98.052) to see if the -17C rows are also implausible and should be removed."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#filter-and-select",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#filter-and-select",
    "title": "JSC 370: Data Science II",
    "section": "Filter and select",
    "text": "Filter and select\nBack to question: what weather stations reported the hottest and coldest daily temperatures?\nCompute the min and max hourly temperatures and the associated stations. Filter with loc and find min and max (it ignores missing) with idxmin() and inxmax()\n\nmin_row = met.loc[met[\"temp\"].idxmin(), [\"temp\", \"USAFID\", \"day\"]]\nmax_row = met.loc[met[\"temp\"].idxmax(), [\"temp\", \"USAFID\", \"day\"]]\nmin_row, max_row\n\n(temp       -17.2\n USAFID    722817\n day            1\n Name: 1203054, dtype: object,\n temp        56.0\n USAFID    720267\n day           26\n Name: 42402, dtype: object)\n\n\n\nOk but we need daily temperatures to answer our questions!"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data",
    "title": "JSC 370: Data Science II",
    "section": "Grouping and summarizing data",
    "text": "Grouping and summarizing data\nWe need to transform our data to answer our initial question at the daily scale. Let’s find daily mean temperature by station and day. Use groupby and agg to calculate the mean and create a new dataset met_daily.\n\nmet_daily = (\n    met.groupby([\"USAFID\", \"day\"], as_index=False)\n       .agg(\n           temp=(\"temp\", \"mean\"),\n           lat=(\"lat\", \"mean\"),\n           lon=(\"lon\", \"mean\"),\n           rh=(\"rh\", \"mean\"),\n       )\n       .sort_values(\"temp\")\n)\n\nmet_daily.head(), met_daily.tail() \n\n(       USAFID  day       temp     lat    lon         rh\n 20774  722817    3 -17.200000  38.767 -104.3        NaN\n 20773  722817    1 -17.133333  38.767 -104.3        NaN\n 20775  722817    6 -17.066667  38.767 -104.3        NaN\n 43605  726130   11   4.278261  44.270  -71.3  99.540440\n 43625  726130   31   4.304348  44.270  -71.3  99.710254,\n        USAFID  day       temp        lat         lon         rh\n 27043  723805    5  40.975000  34.768000 -114.618000  11.369535\n 2345   720339   14  41.000000  32.146000 -111.171000   9.137857\n 27042  723805    4  41.183333  34.768000 -114.618000  14.211822\n 20623  722787    5  41.357143  33.527000 -112.295000  19.829822\n 30     690150   31  41.716667  34.299667 -116.165667   8.676717)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data-1",
    "title": "JSC 370: Data Science II",
    "section": "Grouping and summarizing data",
    "text": "Grouping and summarizing data\nWhat day of the month was the hottest?\n\nmin_daily = met_daily.loc[met_daily[\"temp\"].idxmin(), [\"temp\", \"USAFID\", \"day\"]]\nmax_daily = met_daily.loc[met_daily[\"temp\"].idxmax(), [\"temp\", \"USAFID\", \"day\"]]\nmin_daily, max_daily\n\n(temp         -17.2\n USAFID    722817.0\n day            3.0\n Name: 20774, dtype: float64,\n temp          41.716667\n USAFID    690150.000000\n day           31.000000\n Name: 30, dtype: float64)\n\n\n\nThe hottest day was August 31st, and the average daily temperature was 41.7C"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#looking-at-two-variables-at-a-time",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#looking-at-two-variables-at-a-time",
    "title": "JSC 370: Data Science II",
    "section": "Looking at two variables at a time",
    "text": "Looking at two variables at a time\nTo answer the last question about covariation between temperature and humidity, we can first calculate correlation\n\npearson = met[\"temp\"].corr(met[\"rh\"], method=\"pearson\")\nspearman = met[\"temp\"].corr(met[\"rh\"], method=\"spearman\")\n\npearson, spearman, spearman - pearson\n\n(np.float64(-0.5464705142984516),\n np.float64(-0.5461260667477443),\n np.float64(0.00034444755070728306))\n\n\nIs it the same for the daily data?\n\npearson = met_daily[\"temp\"].corr(met_daily[\"rh\"], method=\"pearson\")\nspearman = met_daily[\"temp\"].corr(met_daily[\"rh\"], method=\"spearman\")\n\npearson, spearman, spearman - pearson\n\n(np.float64(-0.25729536769933825),\n np.float64(-0.20801597675598085),\n np.float64(0.0492793909433574))\n\n\nThe correlation is quite different between hourly and daily data. Why?"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nWith exploratory graphs we aim to:\n•   debug any issues remaining in the data\n•   understand properties of the data\n•   look for patterns in the data\n•   inform modeling strategies\nExploratory graphs do not need to be perfect, we’ll look at presentation-ready plots with python’s ggplot equivalent."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nWhat type of data are these types of exploratory graphs good for?\n•   histograms \n•   boxplots\n•   scatterplots \n•   barplots \n•   lineplots\n•   violin plots \n•   maps"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory histogram",
    "text": "Exploratory histogram\nFocusing on temperature, let’s look at the distribution of hourly temperature (after removing -40°C) using a histogram.\n\nplt.hist(met[\"temp\"].dropna(), bins=50)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Count\")\nplt.title(\"Hourly Temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory histogram",
    "text": "Exploratory histogram\nLet’s look at the daily data\n\nplt.hist(met_daily[\"temp\"].dropna(), bins=50)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Count\")\nplt.title(\"Daily Mean Temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory boxplot",
    "text": "Exploratory boxplot\nA boxplot gives us an idea of the quantiles of the distribution and any outliers.\n\nplt.boxplot(met[\"temp\"].dropna(), vert=True)\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Hourly temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory boxplot",
    "text": "Exploratory boxplot\nOften boxplots are better for grouped continuous data, such as temperature by hour (over all days in the dataset).\n\nhours = range(24)\ndata_by_hour = [met.loc[met[\"hour\"] == h, \"temp\"].to_numpy() for h in hours]\n\nplt.boxplot(data_by_hour, labels=list(hours), showfliers=False)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Temperature by Hour\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-violin-plot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-violin-plot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory violin plot",
    "text": "Exploratory violin plot\nOften violin plots are helpful for grouped continuous data (they show the shape of the distribution), such as temperature by hour (over all days in the dataset).\n\nplt.violinplot(data_by_hour, showmeans=False, showmedians=True, showextrema=False)\nplt.xticks(ticks=np.arange(1, 25), labels=list(hours))\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Temperature by Hour\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory scatterplot",
    "text": "Exploratory scatterplot\nLet’s check covariation with a scatterplot of humidity vs temperature.\n\nplt.scatter(met[\"temp\"], met[\"rh\"])\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Relative humidity (%)\")\nplt.title(\"Temperature vs humidity (sample)\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory scatterplot",
    "text": "Exploratory scatterplot\nSince there are so many points let’s take a random sample of 10,000 rows using sample and add alpha transparency to see the scatter better. random_state is like setting a seed for reproducibility.\n\nmet_s = met.sample(min(10_000, len(met)), random_state=1)\n\nplt.scatter(met_s[\"temp\"], met_s[\"rh\"], alpha=0.2)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Relative humidity (%)\")\nplt.title(\"Temperature vs humidity (sample)\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map\nMay need to python3 -m pip install geopandas contextily to make these maps\n\nimport geopandas as gpd\nimport contextily as cx\n\n# extract latitudes and longitudes\ngdf = gpd.GeoDataFrame(\n    met_daily.dropna(subset=[\"lat\", \"lon\"]).copy(),\n    geometry=gpd.points_from_xy(met_daily[\"lon\"], met_daily[\"lat\"]),\n    crs=\"EPSG:4326\"   \n)\n\ngdf_web = gdf.to_crs(epsg=3857)\n\nax = gdf_web.plot(markersize=10, alpha=0.6)\ncx.add_basemap(ax, source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\n\nSlightly more advanced to make an interactive map with folium.\nYou can use leaflet as another (my preferred) option, but it seems to have some problems in quarto.\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-1",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\nThis is how folium works:\n\nselect unique lat and lon\nset up the map with folium.Map\nadd the location points on the map with folium.CircleMarker, add opacity and radius for point size here"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-2",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-2",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\n\nimport folium\n\npts = met_daily[[\"lat\", \"lon\"]].dropna().drop_duplicates()\n\nm = folium.Map(\n    location=[pts[\"lat\"].mean(), pts[\"lon\"].mean()],\n    zoom_start=4,\n    tiles=\"CartoDB positron\"\n)\n\nfor _, r in pts.iterrows():\n    folium.CircleMarker(\n        location=[r[\"lat\"], r[\"lon\"]],\n        radius=2,\n        fill=True,\n        fill_opacity=0.8,\n        opacity=0.8,\n    ).add_to(m)\n\nm"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-2",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-2",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map\nLet’s now look at where the hottest and coldest places were based on the daily temperatures. We add a layer for cold and a layer for hot.\n\ncold = met_daily.loc[met_daily[\"temp\"].idxmin()]\nhot  = met_daily.loc[met_daily[\"temp\"].idxmax()]\ncold, hot\n\ncenter = [met_daily[\"lat\"].mean(), met_daily[\"lon\"].mean()]\nm = folium.Map(location=center, zoom_start=4, tiles=\"CartoDB positron\")\n\n# Coldest (blue)\nfolium.CircleMarker(\n    location=[cold[\"lat\"], cold[\"lon\"]],\n    radius=10,\n    popup=f\"Coldest daily mean: {cold['temp']:.2f}°C&lt;br&gt;USAFID={cold['USAFID']}&lt;br&gt;day={cold['day']}\",\n    color=\"blue\",\n    fill=True,\n    fill_color=\"blue\",\n    fill_opacity=0.9,\n).add_to(m)\n\n# Hottest (red)\nfolium.CircleMarker(\n    location=[hot[\"lat\"], hot[\"lon\"]],\n    radius=10,\n    popup=f\"Hottest daily mean: {hot['temp']:.2f}°C&lt;br&gt;USAFID={hot['USAFID']}&lt;br&gt;day={hot['day']}\",\n    color=\"red\",\n    fill=True,\n    fill_color=\"red\",\n    fill_opacity=0.9,\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#plotnine-ggplot-style-plots-in-python",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#plotnine-ggplot-style-plots-in-python",
    "title": "JSC 370: Data Science II",
    "section": "plotnine (ggplot-style plots in Python)",
    "text": "plotnine (ggplot-style plots in Python)\nIf you like ggplot2, plotnine uses the same “add layers with +” workflow: - map variables with aes() - add geoms (points, smoothers) - add labels/themes\n\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom plotnine import (\n    ggplot, aes, geom_point, geom_smooth, labs, theme_bw, annotate, coord_cartesian\n)\n\nmet_s = met[[\"temp\", \"rh\"]].dropna().sample(min(10_000, len(met)), random_state=1)\n\n# Fit line: rh = m*temp + b\nx = met_s[\"temp\"].to_numpy()\ny = met_s[\"rh\"].to_numpy()\nm, b = np.polyfit(x, y, 1)\n\ny_hat = m * x + b\nr2 = r2_score(y, y_hat)\n\nlabel = f\"rh = {m:.2f}·temp + {b:.2f}\\nR² = {r2:.3f}\"\n\n# location for the label\nx_pos = np.quantile(x, 0.9)\ny_pos = np.quantile(y, 0.999)\n\n(\n  ggplot(met_s, aes(x=\"temp\", y=\"rh\"))\n  + geom_point(alpha=0.2, size=1.0)\n  + geom_smooth(method=\"lm\", se=False)\n  + annotate(\"text\", x=x_pos, y=y_pos, label=label, ha=\"left\")\n  + coord_cartesian(ylim=(0, 100))\n  + labs(\n      title=\"Temperature vs humidity (sample) with linear fit\",\n      x=\"Temperature (°C)\",\n      y=\"Relative humidity (%)\"\n    )\n  + theme_bw()\n)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#plotnine",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#plotnine",
    "title": "JSC 370: Data Science II",
    "section": "plotnine",
    "text": "plotnine\n\n(\n  ggplot(met_s, aes(x=\"temp\", y=\"rh\"))\n  + geom_point(alpha=0.2, size=1.0)\n  + geom_smooth(method=\"loess\", se=False)\n  + coord_cartesian(ylim=(0, 100))\n  + labs(\n      title=\"Temperature vs humidity (sample) with linear fit\",\n      x=\"Temperature (°C)\",\n      y=\"Relative humidity (%)\"\n    )\n  + theme_bw()\n)"
  },
  {
    "objectID": "labs/lab3/lab03-github.html",
    "href": "labs/lab3/lab03-github.html",
    "title": "Lab 03 - EDA and Viz 1",
    "section": "",
    "text": "Download lab 3 here\nRead in from github and get familiar with a meteorology dataset\nPlan out how to tackle the objective question\nStep through the EDA “checklist” presented in the class slides\nMake exploratory graphs and maps"
  },
  {
    "objectID": "labs/lab3/lab03-github.html#answer-research-questions",
    "href": "labs/lab3/lab03-github.html#answer-research-questions",
    "title": "Lab 03 - EDA and Viz 1",
    "section": "6. Answer research questions",
    "text": "6. Answer research questions\nRemember to keep the initial questions in mind. We want to pick out the weather station with minimum elevation and examine its temperature.\nSome ideas for steps: 1. subset the data for the weather station with minimum elevation 2. look at histograms of temperature 3. make a time series plot (need to create date variable) 4. make a map to see where it is\n\n6a. Subset minimum elevation site\n\n# Subset data for the weather station with minimum elevation\n\n\n\n6b. Histogram of temperature at minimum elevation site\n\nplt.hist(...)\n\n\n\n6c. Create date variable and time series plot\n\nLook at the time series of temperature at this location. For this we will need to create a date-time variable for the x-axis.\nSummarize any trends that you see in these time series plots.\n\n\n# Create a date variable for min_elev_station\nmin_elev_station = min_elev_station.copy()\nmin_elev_station['date'] = pd.to_datetime(\n    min_elev_station[...]\n)\n\n# Sort by date\nmin_elev_station = min_elev_station.sort_values('date')\n\n# Create line plot of temperature by date\n\nplt.plot(...)\nplt.xlabel()\nplt.ylabel()\nplt.show()\n\nSummarize time series plot:\n\n\n6d. Where is the lowest weather station?\n\nstation_location = min_elev_station[...].drop_duplicates()\n\nlat = station_location['lat'].iloc[0]\nlon = station_location['lon'].iloc[0]\nelev = station_location['elev'].iloc[0]\n\n# make elev into GeoDataFrame for mapping\ngdf = gpd.GeoDataFrame(\n    {'elevation': [elev]},\n    geometry=gpd.points_from_xy([lon], [lat]),\n    crs='EPSG:4326'\n)\n\n# Convert to Web Mercator for basemap\ngdf = gdf.to_crs('EPSG:3857')\n\n# map\nfig, ax = plt.subplots(figsize=(10, 8))\ngdf.plot(ax=ax, color='red', markersize=300, marker='*', edgecolor='black', linewidth=2, zorder=5)\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title(f'Lowest Elevation Station (Elevation: {elev} m)')\nplt.show()\n\nSummarize where the lowest elevation station is located."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html",
    "href": "slides/03-eda-viz/JSC370-slides-03.html",
    "title": "JSC 370: Data Science II",
    "section": "",
    "text": "Load in large datasets stored locally and remotely (GitHub), check memory issues\nUnderstand what EDA is (and is not)\nBuild a repeatable EDA workflow\nMake plots that reveal structure, not just decoration\nAvoid common visualization pitfalls"
  },
  {
    "objectID": "labs/lab03-github.html",
    "href": "labs/lab03-github.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/lab03-github.html#overview",
    "href": "labs/lab03-github.html#overview",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/lab03-github.html#what-you-will-submit-to-markus",
    "href": "labs/lab03-github.html#what-you-will-submit-to-markus",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "What you will submit to MarkUs",
    "text": "What you will submit to MarkUs\n\nThis completed .qmd file. Download the lab .qmd on GitHub\nThe rendered HTML output (JSC370-lab-01.html)"
  },
  {
    "objectID": "labs/lab03-github.html#before-you-start",
    "href": "labs/lab03-github.html#before-you-start",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "0. Before you start",
    "text": "0. Before you start\n\nIf you do not already have it, install VS Code. Then open this file in .\nIn VS Code open the terminal (Terminal -&gt; New Terminal). Recall you can toggle between terminals in VS Code. You should see a list of terminals on the bottom right panel."
  },
  {
    "objectID": "labs/lab03-github.html#quarto",
    "href": "labs/lab03-github.html#quarto",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "1. Quarto",
    "text": "1. Quarto\nIn the terminal, run:\nquarto --version\nIf it is not installed, go to the Quarto download page and install for your OS: https://quarto.org/docs/download/.\nAfter installing, quit and reopen VS Code (so PATH updates), then run quarto –version again.\nPASTE YOUR VERSION HERE\nNow install the Quarto extension in VS Code:\n\nIn VS Code, click the Extensions icon (left sidebar), or press:\n\nmacOS: Cmd+Shift+X\nWindows/Linux: Ctrl+Shift+X\n\nSearch: Quarto\nInstall: Quarto (publisher: quarto-dev)\nYou may need to reload VS Code.\n\nPreview this lab in VS Code\nOpen this .qmd file, then preview it with Command Palette (Cmd/Ctrl+Shift+P) and select Quarto: Preview.\nTo stop preview: press Ctrl+C in the terminal running the preview."
  },
  {
    "objectID": "labs/lab03-github.html#confirm-python-is-installed",
    "href": "labs/lab03-github.html#confirm-python-is-installed",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "2. Confirm Python is installed",
    "text": "2. Confirm Python is installed\nYou can try this in terminal:\n#| echo: true\npython3 --version\n(If python3 doesn’t work, try python –version in terminal.) Paste the output, but the chunk above should do it for you if installed correctly."
  },
  {
    "objectID": "labs/lab03-github.html#install-core-packages-in-terminal",
    "href": "labs/lab03-github.html#install-core-packages-in-terminal",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "3. Install core packages in terminal",
    "text": "3. Install core packages in terminal\nRun the following in terminal. It is best to install packages in the terminal (not from inside Quarto).\npython3 -m pip install -U pip\npython3 -m pip install numpy pandas matplotlib jupyter ipykernel plotnine plotly\nAlso install the Jupyter extension in VS Code, similarly to how we installed the Quarto extension above (search for Jupyter).\nConfirm Quarto can see Jupyter\nquarto check jupyter\nPASTE OUTPUT HERE"
  },
  {
    "objectID": "labs/lab03-github.html#verify-your-python-environment",
    "href": "labs/lab03-github.html#verify-your-python-environment",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "4. Verify your Python environment",
    "text": "4. Verify your Python environment\nRun the chunk below. If this fails, double-check that VS Code is using the Python environment where you installed packages.\nimport sys\nimport pandas as pd\n\nprint(\"Python executable:\", sys.executable)\nprint(\"pandas version:\", pd.__version__)"
  },
  {
    "objectID": "labs/lab03-github.html#your-turn-make-a-plot",
    "href": "labs/lab03-github.html#your-turn-make-a-plot",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "5. Your turn: make a plot",
    "text": "5. Your turn: make a plot\nUse the plotnine library (ggplot-like) to make a simple scatterplot. Update the code below:\n\nTitle must include your name (e.g., \"Penguin bill measurements — Your Name\").\nChange the point transparency by setting alpha to a new value.\n\nfrom plotnine import ggplot, aes, geom_point, labs, theme_minimal\nfrom plotnine.data import penguins\n\n(\n    ggplot(penguins, aes(\"bill_length_mm\", \"bill_depth_mm\", color=\"species\"))\n    + geom_point(alpha=0.4)  \n    + labs(title=\"Penguin bill measurements — YOUR NAME\")  \n    + theme_minimal()\n)"
  },
  {
    "objectID": "labs/lab03-github.html#render-.qmd-to-.html",
    "href": "labs/lab03-github.html#render-.qmd-to-.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "6. Render ‘.qmd’ to ‘.html’",
    "text": "6. Render ‘.qmd’ to ‘.html’\nIn the terminal, make sure you are in the folder containing this file, then run:\nquarto render JSC370-lab-01.qmd\nThis should create an HTML file in the same folder, e.g. JSC370-lab-01.html.\nConfirm it exists:\nls -l JSC370-lab-01.html\nPrint out the output and make sure it is in this lab. Also open the HTML in your browser to confirm it looks correct."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#this-week",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#this-week",
    "title": "JSC 370: Data Science II",
    "section": "This week",
    "text": "This week\n\nThis lecture presents a deeper introduction to plotnine, a Python package that implements the grammar of graphics (similar to R’s ggplot2), providing better graphics options than matplotlib’s default plots."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#this-week-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#this-week-1",
    "title": "JSC 370: Data Science II",
    "section": "This week",
    "text": "This week\nWe will also dig into non-parametric regression with splines and generalized additive models (GAMs).\nA generalized additive model (GAM) predicts \\(y\\) by adding up learned smooth functions of the predictors:\n\\[\ng(\\mathbb{E}[Y \\mid X])=\\beta_0 + s_1(x_1) + s_2(x_2) + \\cdots + s_p(x_p)\n\\]\n\nNonlinear: each \\(s_j(\\cdot)\\) is a flexible curve (often a spline)\nRegularized: we control “wiggliness” with a penalty (bias–variance tradeoff)\nFits the ML pattern: choose flexibility by minimizing training loss + complexity penalty\nInterpretable: you can plot each learned effect \\(s_j(x_j)\\) (partial effect curves)\nGeneralized: works for continuous, binary, counts via a link \\(g(\\cdot)\\)\nMental model: like linear regression, but each coefficient is replaced by a smooth curve."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#background",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#background",
    "title": "JSC 370: Data Science II",
    "section": "Background",
    "text": "Background\nplotnine is based on the grammar of graphics, the same underlying philosophy as R’s ggplot2. In the Python ecosystem, we use pandas for data manipulation (similar to R’s dplyr), and plotnine for visualization.\nThe tidyverse philosophy of readable, chainable operations translates well to Python with method chaining in pandas."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#layers-pandas-and-method-chaining",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#layers-pandas-and-method-chaining",
    "title": "JSC 370: Data Science II",
    "section": "Layers, pandas and method chaining",
    "text": "Layers, pandas and method chaining\n\nWe should take a step back and discuss Python’s data manipulation ecosystem.\nplotnine behaves very similarly to pandas. They share a philosophy of readable, chainable operations.\nIn pandas, operations are typically chained using method calls on DataFrames.\nEach method takes a DataFrame and returns a new DataFrame.\nMethod chaining in Python uses the . operator (similar to R’s pipe).\nYou can also use parentheses to break long chains across multiple lines."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#method-chaining-in-python",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#method-chaining-in-python",
    "title": "JSC 370: Data Science II",
    "section": "Method chaining in Python",
    "text": "Method chaining in Python\n\nAn example using method chaining: subset the flights data to LAX, and take the mean arrival delay times by year, month, day.\nWe need to start with the data, query or boolean indexing to filter to the desired destination (LAX), groupby to prepare the groups that we want to aggregate over, then agg to take the mean.\n\n\n(flights\n .query('dest == \"LAX\"')\n .groupby(['year', 'month', 'day'])\n .agg(arr_delay=('arr_delay', lambda x: x.mean(skipna=True)))\n .reset_index())\n\n\n\n\n\n\n\n\nyear\nmonth\nday\narr_delay\n\n\n\n\n0\n2013\n1\n1\n7.743590\n\n\n1\n2013\n1\n2\n-7.414634\n\n\n2\n2013\n1\n3\n-28.725000\n\n\n3\n2013\n1\n4\n-25.125000\n\n\n4\n2013\n1\n5\n-3.142857\n\n\n...\n...\n...\n...\n...\n\n\n360\n2013\n12\n27\n-29.957447\n\n\n361\n2013\n12\n28\n-15.650000\n\n\n362\n2013\n12\n29\n8.355556\n\n\n363\n2013\n12\n30\n-6.000000\n\n\n364\n2013\n12\n31\n6.230769\n\n\n\n\n365 rows × 4 columns"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data",
    "title": "JSC 370: Data Science II",
    "section": "Flights data",
    "text": "Flights data\nTo illustrate many of today’s visualization examples we will look at the flights data from the nycflights13 package. They are all flights that departed from NYC airports (JFK, LGA, EWR) in 2013.\n\nfrom nycflights13 import flights\nprint(\"Columns:\", flights.columns.tolist())\nprint(\"Shape:\", flights.shape)\n\nColumns: ['year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay', 'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight', 'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'time_hour']\nShape: (336776, 19)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-1",
    "title": "JSC 370: Data Science II",
    "section": "Flights data",
    "text": "Flights data"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#method-chaining-in-python-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#method-chaining-in-python-1",
    "title": "JSC 370: Data Science II",
    "section": "Method chaining in Python",
    "text": "Method chaining in Python\n\nAn example using method chaining: subset the flights data to LAX, and take the mean arrival delay times by year, month, day.\nWe need to start with the data, query or boolean indexing to filter to the desired destination (LAX), groupby to prepare the groups that we want to aggregate over, then agg to take the mean.\n\n\n(flights\n .query('dest == \"LAX\"')\n .groupby(['year', 'month', 'day'])\n .agg(arr_delay=('arr_delay', lambda x: x.mean(skipna=True)))\n .reset_index())\n\n\n\n\n\n\n\n\nyear\nmonth\nday\narr_delay\n\n\n\n\n0\n2013\n1\n1\n7.743590\n\n\n1\n2013\n1\n2\n-7.414634\n\n\n2\n2013\n1\n3\n-28.725000\n\n\n3\n2013\n1\n4\n-25.125000\n\n\n4\n2013\n1\n5\n-3.142857\n\n\n...\n...\n...\n...\n...\n\n\n360\n2013\n12\n27\n-29.957447\n\n\n361\n2013\n12\n28\n-15.650000\n\n\n362\n2013\n12\n29\n8.355556\n\n\n363\n2013\n12\n30\n-6.000000\n\n\n364\n2013\n12\n31\n6.230769\n\n\n\n\n365 rows × 4 columns"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#alternative-syntax-with-pipe",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#alternative-syntax-with-pipe",
    "title": "JSC 370: Data Science II",
    "section": "Alternative syntax with pipe()",
    "text": "Alternative syntax with pipe()\nLet’s do the same thing using pandas’ pipe() method for a more functional style:\n\n(flights\n .pipe(lambda df: df[df['dest'] == 'LAX'])\n .groupby(['year', 'month', 'day'])\n .agg(arr_delay=('arr_delay', 'mean'))\n .reset_index())\n\n\n\n\n\n\n\n\nyear\nmonth\nday\narr_delay\n\n\n\n\n0\n2013\n1\n1\n7.743590\n\n\n1\n2013\n1\n2\n-7.414634\n\n\n2\n2013\n1\n3\n-28.725000\n\n\n3\n2013\n1\n4\n-25.125000\n\n\n4\n2013\n1\n5\n-3.142857\n\n\n...\n...\n...\n...\n...\n\n\n360\n2013\n12\n27\n-29.957447\n\n\n361\n2013\n12\n28\n-15.650000\n\n\n362\n2013\n12\n29\n8.355556\n\n\n363\n2013\n12\n30\n-6.000000\n\n\n364\n2013\n12\n31\n6.230769\n\n\n\n\n365 rows × 4 columns"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#a-few-coding-style-tips",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#a-few-coding-style-tips",
    "title": "JSC 370: Data Science II",
    "section": "A few coding style tips",
    "text": "A few coding style tips\n\nVariable names should use only lowercase letters, numbers, and _\nUse _ to separate words within a name (snake_case)\nMethod chains should have each method on its own line, indented\nWrap long chains in parentheses for cleaner formatting\nEach chained method should be indented for readability\n\nExample:\n\nshort_flights = (flights\n    .query('air_time &lt; 60'))"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail",
    "title": "JSC 370: Data Science II",
    "section": "Flights data in more detail",
    "text": "Flights data in more detail\nThe nycflights13 package also provides hourly airport weather data for the three NYC airports (the origin). Let’s join the flights data with the weather data so we can look at more interesting relationships in our visualizations.\n\nweather = pd.read_csv('flights_weather.csv')\nprint(\"Columns:\", weather.columns.tolist())\nprint(\"Shape:\", weather.shape)\n\nColumns: ['origin', 'year', 'month', 'day', 'hour', 'temp', 'dewp', 'humid', 'wind_dir', 'wind_speed', 'wind_gust', 'precip', 'pressure', 'visib', 'time_hour']\nShape: (26115, 15)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail-1",
    "title": "JSC 370: Data Science II",
    "section": "Flights data in more detail",
    "text": "Flights data in more detail\nLooks like we can join these datasets on year, month, day, hour and origin (which is the origin airport). We can examine the relationship between flight delays and weather at the origin airports (JFK, LGA, EWR).\nA merge with how='left' will keep all of the observations in the left DataFrame (flights) and merge with the observations in the right DataFrame (weather at origin).\n\nflights_weather = pd.merge(\n    flights,\n    weather,\n    how='left',\n    on=['year', 'month', 'day', 'hour', 'origin']\n)\n\nflights_weather.head()\nprint(\"Shape:\", flights_weather.shape)\n\nShape: (336776, 29)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flights-data-in-more-detail-2",
    "title": "JSC 370: Data Science II",
    "section": "Flights data in more detail",
    "text": "Flights data in more detail\n\nflights_weather.head()\nprint(\"Shape:\", flights_weather.shape)\n\nShape: (336776, 29)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#daily-flights-data",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#daily-flights-data",
    "title": "JSC 370: Data Science II",
    "section": "Daily flights data",
    "text": "Daily flights data\nLet’s make a more manageable sized dataset and summarize the data by month, day, and origin airport by taking the means of the weather variables.\n\nagg_cols = ['dep_delay', 'arr_delay', 'temp', 'dewp', 'humid',\n            'wind_dir', 'wind_speed', 'wind_gust', 'precip', 'pressure', 'visib']\n\nflights_weather_day = (flights_weather\n    .groupby(['year', 'month', 'day', 'origin'])\n    .agg({col: 'mean' for col in agg_cols})\n    .reset_index())\n\nflights_weather_day.head()\nprint(\"Shape:\", flights_weather_day.shape)\n\nShape: (1095, 15)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#visualizations",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#visualizations",
    "title": "JSC 370: Data Science II",
    "section": "Visualizations",
    "text": "Visualizations\nplotnine is designed on the principle of adding layers."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#layers-in-plotnine",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#layers-in-plotnine",
    "title": "JSC 370: Data Science II",
    "section": "Layers in plotnine",
    "text": "Layers in plotnine\n\nWith plotnine a plot is initiated with the function ggplot()\nThe first argument of ggplot() is the dataset to use in the graph\nWe add aesthetics using aes()\nThe aes() mapping takes the x and y axes of the plot\nLayers are added to ggplot() with +\nLayers include geom_ functions such as point, lines, etc.\n\n\n(ggplot(data, aes(x='x_var', y='y_var')) +\n  geom_point())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#basic-scatterplot",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#basic-scatterplot",
    "title": "JSC 370: Data Science II",
    "section": "Basic scatterplot",
    "text": "Basic scatterplot\nThe first argument of ggplot() is the dataset to use, then the aesthetics. With the + you add one or more layers.\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay')) +\n  geom_point() +\n  theme_minimal())\n\n\n\n\n\n\n\n\nAs expected, we see that if a flight has a late departure, it has a late arrival."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#another-basic-scatterplot",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#another-basic-scatterplot",
    "title": "JSC 370: Data Science II",
    "section": "Another basic scatterplot",
    "text": "Another basic scatterplot\nWe can see if there is a relationship between departure delays and pressure (low pressure means clouds and precipitation, high pressure means better weather).\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#adding-to-a-basic-scatterplot",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#adding-to-a-basic-scatterplot",
    "title": "JSC 370: Data Science II",
    "section": "Adding to a basic scatterplot",
    "text": "Adding to a basic scatterplot\n\ngeom_point() adds a layer of points to your plot, creating a scatterplot.\nplotnine comes with many geom functions that each add a different type of layer to a plot.\nEach geom function in plotnine takes a mapping argument.\nThis defines how variables in your dataset are mapped to visual properties.\nThe mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes.\nOne common problem when creating plotnine graphics is to put the + in the wrong place: it has to come at the end of the line, not the start."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics",
    "title": "JSC 370: Data Science II",
    "section": "Coloring by a variable - using aesthetics",
    "text": "Coloring by a variable - using aesthetics\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the origin variable to reveal groupings.\nplotnine chooses colors, and adds a legend, automatically.\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay', color='origin')) +\n  geom_point() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-1",
    "title": "JSC 370: Data Science II",
    "section": "Coloring by a variable - using aesthetics",
    "text": "Coloring by a variable - using aesthetics\nNote when there are a lot of classes or groups, the coloring is not distinguished well.\nThis is generally bad practice.\n\n(ggplot(flights_weather, aes(x='arr_delay', y='dep_delay', color='dest')) +\n  geom_point() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-2",
    "title": "JSC 370: Data Science II",
    "section": "Coloring by a variable - using aesthetics",
    "text": "Coloring by a variable - using aesthetics\nToo many points? Take a random sample and plot again.\n\n(ggplot(flights_weather.sample(1000), aes(x='arr_delay', y='dep_delay', color='dest')) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend_position='right'))"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-3",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-by-a-variable---using-aesthetics-3",
    "title": "JSC 370: Data Science II",
    "section": "Coloring by a variable - using aesthetics",
    "text": "Coloring by a variable - using aesthetics\nToo many points? Take a random sample and plot again."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#determining-point-type-using-a-variable",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#determining-point-type-using-a-variable",
    "title": "JSC 370: Data Science II",
    "section": "Determining point type using a variable",
    "text": "Determining point type using a variable\nBy default plotnine uses up to 6 shapes. If there are more, some of your data is not plotted!! (At least it warns you.)\n\n(ggplot(flights_weather.sample(1000), aes(x='arr_delay', y='dep_delay', shape='dest')) +\n  geom_point())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#determining-point-type-using-a-variable-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#determining-point-type-using-a-variable-1",
    "title": "JSC 370: Data Science II",
    "section": "Determining point type using a variable",
    "text": "Determining point type using a variable\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay', shape='origin')) +\n  geom_point(alpha=0.6) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#controlling-point-transparency-using-alpha",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#controlling-point-transparency-using-alpha",
    "title": "JSC 370: Data Science II",
    "section": "Controlling point transparency using “alpha”",
    "text": "Controlling point transparency using “alpha”\nThe alpha control can go in the layer that you are plotting, for example in the geom_point(alpha = 0.1) layer we can directly control the transparency of the points (0 is fully transparent, 1 is fully opaque).\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(alpha=0.4) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#manual-control-of-aesthetics",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#manual-control-of-aesthetics",
    "title": "JSC 370: Data Science II",
    "section": "Manual control of aesthetics",
    "text": "Manual control of aesthetics\nIf we control aesthetics manually in the layer such as geom_point() (i.e. outside aes()), they become fixed constants for that geom, not variables mapped from the data.\nOne more important detail in plotnine: if you write aes(color='blue'), that’s a mapping (to the literal string “blue”), not a fixed aesthetic, so you’ll often get an odd legend. Fixed values should be outside aes(), but coloring by a variable (such as a group) should be inside aes().\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay')) +\n  geom_point(color='blue', alpha=0.5) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#summary-of-aesthetics",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#summary-of-aesthetics",
    "title": "JSC 370: Data Science II",
    "section": "Summary of aesthetics",
    "text": "Summary of aesthetics\nThe various aesthetics of aes:\n\n\n\nCode\nDescription\n\n\n\n\nx\nposition on x-axis\n\n\ny\nposition on y-axis\n\n\nshape\nshape\n\n\ncolor\ncolor of element borders\n\n\nfill\ncolor inside elements\n\n\nsize\nsize\n\n\nalpha\ntransparency\n\n\nlinetype\ntype of line"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#facets-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#facets-1",
    "title": "JSC 370: Data Science II",
    "section": "Facets 1",
    "text": "Facets 1\nFacets are particularly useful for categorical variables:\n\n(ggplot(flights_weather_day, aes(x='wind_speed', y='dep_delay')) +\n  geom_point(alpha=0.5) +\n  facet_wrap('~origin', nrow=1) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#facets-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#facets-2",
    "title": "JSC 370: Data Science II",
    "section": "Facets 2",
    "text": "Facets 2\nOr you can facet on two variables:\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay')) +\n  geom_point(alpha=0.3, size=0.5) +\n  facet_grid('month~origin') +\n  theme_minimal() +\n  theme(figure_size=(12, 16)))\n\n\n\n\n\n\n\n\nNote this plot is probably too large and busy for practical use, but it gives you the idea what is possible with faceting."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-1",
    "title": "JSC 370: Data Science II",
    "section": "Geometric Objects 1",
    "text": "Geometric Objects 1\nGeometric objects are used to control the type of plot you draw. Let’s plot a smoothed line fitted to the scatterplot data between pressure and departure delay.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(alpha=0.3) +\n  geom_smooth() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-1-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-1-1",
    "title": "JSC 370: Data Science II",
    "section": "Geometric Objects 1",
    "text": "Geometric Objects 1"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#geometric-objects-2",
    "title": "JSC 370: Data Science II",
    "section": "Geometric Objects 2",
    "text": "Geometric Objects 2\nNote that not every aesthetic works with every geom function.\nFor example, linetype is an aesthetic mapping that controls the pattern used to draw lines (solid, dashed, dotted, etc.).\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay', linetype='origin')) +\n  geom_smooth(se=False) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#geoms---reference",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#geoms---reference",
    "title": "JSC 370: Data Science II",
    "section": "Geoms - Reference",
    "text": "Geoms - Reference\nplotnine provides many geoms, mirroring ggplot2’s functionality. See https://plotnine.readthedocs.io/ for documentation.\nHere is a nice plotnine guide\nAnd cheatsheet (front) and cheatsheet (back)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-1",
    "title": "JSC 370: Data Science II",
    "section": "Multiple Geoms 1",
    "text": "Multiple Geoms 1\nTo display multiple geoms in the same plot, add multiple geom functions to ggplot(), for example geom_point and geom_smooth"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-2",
    "title": "JSC 370: Data Science II",
    "section": "Multiple Geoms 2",
    "text": "Multiple Geoms 2\nIf you place mappings in a geom function, plotnine will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(aes(color='origin'), alpha=0.5) +\n  geom_smooth() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-3",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-3",
    "title": "JSC 370: Data Science II",
    "section": "Multiple Geoms 3",
    "text": "Multiple Geoms 3\nYou can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the dataset, the flights departing from JFK. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only.\n\njfk_data = flights_weather_day.query('origin == \"JFK\"')\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(aes(color='origin'), alpha=0.5) +\n  geom_smooth(data=jfk_data, se=False, color='black') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#statistical-transformations---e.g.-bar-charts",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#statistical-transformations---e.g.-bar-charts",
    "title": "JSC 370: Data Science II",
    "section": "Statistical Transformations - e.g. Bar charts",
    "text": "Statistical Transformations - e.g. Bar charts\nLet’s make a bar chart of the number of flights at each origin airport in 2013. The algorithm uses a built-in statistical transformation, called a “stat”, to calculate the counts.\n\n(ggplot(flights_weather, aes(x='origin')) +\n  geom_bar() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#bar-charts-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#bar-charts-2",
    "title": "JSC 370: Data Science II",
    "section": "Bar charts 2",
    "text": "Bar charts 2\nYou can override the statistic a geom uses to construct its plot. e.g., if we want to plot proportions, rather than counts:\n\n(ggplot(flights_weather, aes(x='origin', y='..prop..', group=1)) +\n  geom_bar() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts",
    "title": "JSC 370: Data Science II",
    "section": "Coloring barcharts",
    "text": "Coloring barcharts\nYou can color a bar chart using either the color aesthetic (only does the outline), or, more usefully, fill:\n\n(ggplot(flights_weather, aes(x='origin', fill='origin')) +\n  geom_bar() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-1",
    "title": "JSC 370: Data Science II",
    "section": "Coloring barcharts",
    "text": "Coloring barcharts\nMore interestingly, you can fill by another variable. Let’s first subset our data to look at the destination airports with a lot of flights (more than 10,000 flights)\n\nflights_weather_ss = (flights_weather\n    .groupby('dest')\n    .filter(lambda x: len(x) &gt; 10000))\n\n(ggplot(flights_weather_ss, aes(x='origin', fill='dest')) +\n  geom_bar() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-2",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-2",
    "title": "JSC 370: Data Science II",
    "section": "Coloring barcharts",
    "text": "Coloring barcharts\nThe position='dodge' places overlapping objects directly beside one another. This makes it easier to compare individual values.\n\n(ggplot(flights_weather_ss, aes(x='origin', fill='dest')) +\n  geom_bar(position='dodge') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-3",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coloring-barcharts-3",
    "title": "JSC 370: Data Science II",
    "section": "Coloring barcharts",
    "text": "Coloring barcharts\nThe position='dodge' places overlapping objects directly beside one another. This makes it easier to compare individual values.\n\n(ggplot(flights_weather_ss, aes(x='origin', fill='dest')) +\n  geom_bar(position='dodge') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#statistical-transformations---another-example",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#statistical-transformations---another-example",
    "title": "JSC 370: Data Science II",
    "section": "Statistical transformations - another example",
    "text": "Statistical transformations - another example\nYou might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value.\nHere we plot the median delay with IQR (25th-75th quantiles) as bars.\n\n(ggplot(flights_weather_ss, aes(x='dest', y='dep_delay')) +\n  stat_summary(fun_y=np.median,\n               fun_ymin=lambda x: np.percentile(x.dropna(), 25),\n               fun_ymax=lambda x: np.percentile(x.dropna(), 75)) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-systems",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-systems",
    "title": "JSC 370: Data Science II",
    "section": "Coordinate systems",
    "text": "Coordinate systems\nCoordinate systems are one of the more complicated corners of plotnine. To start with something simple, here’s how to flip axes:\n\n# Unflipped\n(ggplot(flights_weather_day, aes(x='origin', y='dep_delay')) +\n  geom_boxplot() +\n  theme_minimal())\n\n\n\n\n\n\n\n\n\n# Flipped\n(ggplot(flights_weather_day, aes(x='origin', y='dep_delay')) +\n  geom_boxplot() +\n  coord_flip() +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#adding-labels",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#adding-labels",
    "title": "JSC 370: Data Science II",
    "section": "Adding labels",
    "text": "Adding labels\nYou can make nicer axes and add titles with the labs layer\nAlso showing a minimal theme that removes the background grey theme_bw()\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(alpha=0.3) +\n  geom_smooth() +\n  labs(\n    x='Atmospheric Pressure, millibars',\n    y='Departure Delay, minutes',\n    title='Association between weather and flight departure delays'\n  ) +\n  theme_bw())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#color-ramps",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#color-ramps",
    "title": "JSC 370: Data Science II",
    "section": "Color ramps",
    "text": "Color ramps\nIf you add a continuous variable in your color aesthetic, it will create a color ramp. Here we apply color with the variable temp.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(aes(color='temp'), alpha=0.6) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#color-palettes",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#color-palettes",
    "title": "JSC 370: Data Science II",
    "section": "Color palettes",
    "text": "Color palettes\nYou can define your own color ramp or use one of the palettes. Here we manually change the color palette with scale_color_gradient.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(aes(color='temp'), alpha=0.6) +\n  scale_color_gradient(low='blue', high='red') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#color-palettes-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#color-palettes-1",
    "title": "JSC 370: Data Science II",
    "section": "Color palettes",
    "text": "Color palettes\nYou can get as detailed as you would like for the color ramp.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(aes(color='temp'), alpha=0.6) +\n  scale_color_gradientn(colors=['#999999', '#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#summary-r-to-python-translation",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#summary-r-to-python-translation",
    "title": "JSC 370: Data Science II",
    "section": "Summary: R to Python Translation",
    "text": "Summary: R to Python Translation\nFor those of you who have used tidyverse and ggplot in R and want to compare to Python’s pandas/plotnine:\n\n\n\nR (tidyverse/ggplot2)\nPython (pandas/plotnine)\n\n\n\n\nlibrary(ggplot2)\nfrom plotnine import *\n\n\n%&gt;% or |&gt;\nMethod chaining with .\n\n\nfilter()\n.query() or boolean indexing\n\n\ngroup_by()\n.groupby()\n\n\nsummarize()\n.agg()\n\n\nleft_join()\npd.merge(how='left')\n\n\naes(x = var)\naes(x='var')"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#what-is-geom_smooth-actually-doing",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#what-is-geom_smooth-actually-doing",
    "title": "JSC 370: Data Science II",
    "section": "What is geom_smooth() actually doing?",
    "text": "What is geom_smooth() actually doing?\nWhen we use geom_smooth() in plotnine, we’re fitting a smooth regression line to our data.\n\nBy default, geom_smooth() uses LOWESS (Locally Weighted Scatterplot Smoothing) for small datasets\nFor larger datasets, it uses a GAM (Generalized Additive Model)\nThe smooth line adapts to the local structure of the data\n\nThis raises a question: How do we model non-linear relationships?"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#the-problem-with-linear-models",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#the-problem-with-linear-models",
    "title": "JSC 370: Data Science II",
    "section": "The Problem with Linear Models",
    "text": "The Problem with Linear Models\nAs we know, a linear model tries to fit the best straight line through the data.\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\nThis works well when the relationship is actually linear\nBut many real-world relationships are non-linear\nForcing a straight line through curved data gives poor predictions"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#motivating-example-co₂-data",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#motivating-example-co₂-data",
    "title": "JSC 370: Data Science II",
    "section": "Motivating Example: CO₂ Data",
    "text": "Motivating Example: CO₂ Data\nLet’s look at atmospheric CO\\(_2\\) measurements from Mauna Loa, Hawaii (collected since 1958).\n\n# Load CO2 data\nimport pandas as pd\n\nurl = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\"\n\nco2 = pd.read_csv(url, comment=\"#\")  # ignores the metadata header\nco2 = co2.rename(columns={\"average\": \"co2\"})\nco2[\"month_year\"] = pd.to_datetime(co2[[\"year\", \"month\"]].assign(day=1))\n\nco2.head()\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal date\nco2\ndeseasonalized\nndays\nsdev\nunc\nmonth_year\n\n\n\n\n0\n1958\n3\n1958.2027\n315.71\n314.44\n-1\n-9.99\n-0.99\n1958-03-01\n\n\n1\n1958\n4\n1958.2877\n317.45\n315.16\n-1\n-9.99\n-0.99\n1958-04-01\n\n\n2\n1958\n5\n1958.3699\n317.51\n314.69\n-1\n-9.99\n-0.99\n1958-05-01\n\n\n3\n1958\n6\n1958.4548\n317.27\n315.15\n-1\n-9.99\n-0.99\n1958-06-01\n\n\n4\n1958\n7\n1958.5370\n315.87\n315.20\n-1\n-9.99\n-0.99\n1958-07-01"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#linear-vs-smooth-fit",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#linear-vs-smooth-fit",
    "title": "JSC 370: Data Science II",
    "section": "Linear vs Smooth Fit",
    "text": "Linear vs Smooth Fit\n\n(ggplot(co2, aes(x='decimal date', y='co2')) +\n  geom_point(alpha=0.3, size=0.5) +\n  geom_smooth(method='lm', color='red', se=False) +\n  geom_smooth(method='lowess', color='blue', se=False) +\n  labs(x='Year', y='CO2 (ppm)',\n       title='Atmospheric CO2 at Mauna Loa') +\n  theme_minimal())\n\n\n\n\n\n\n\n\nThe linear model misses the curvature and seasonal cycles!"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#zooming-in-2025-data",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#zooming-in-2025-data",
    "title": "JSC 370: Data Science II",
    "section": "Zooming in: 2025 Data",
    "text": "Zooming in: 2025 Data\n\nco2_2025 = co2[co2['year'] == 2025].copy()\n\n\n(ggplot(co2_2025, aes(x='decimal date', y='co2')) +\n  geom_point(size=2) +\n  geom_smooth(method='lm', color='red', se=False) +\n  geom_smooth(method='lowess', color='blue', se=False) +\n  labs(x='Year', y='CO2 (ppm)',\n       title='CO2 in 2025: Linear vs Smooth') +\n  theme_minimal())\n\n\n\n\n\n\n\n\nThe smooth line captures the seasonal pattern that the linear model completely misses."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-regression-a-first-attempt",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-regression-a-first-attempt",
    "title": "JSC 370: Data Science II",
    "section": "Polynomial Regression: A First Attempt",
    "text": "Polynomial Regression: A First Attempt\nOne approach to modeling non-linearity is polynomial regression:\n\\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\epsilon\\]\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Fit polynomial regression (degree 3)\nX = co2_2025[['month']].values\ny = co2_2025['co2'].values\n\npoly = PolynomialFeatures(degree=3)\nX_poly = poly.fit_transform(X)\nmodel = LinearRegression().fit(X_poly, y)\n\nco2_2025['poly_pred'] = model.predict(X_poly)\nprint(f\"Coefficients: {model.intercept_:.2f} + {model.coef_[1]:.2f}x + {model.coef_[2]:.2f}x² + {model.coef_[3]:.2f}x³\")\n\nCoefficients: 421.40 + 5.00x + -0.90x² + 0.04x³"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-fit-visualization",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-fit-visualization",
    "title": "JSC 370: Data Science II",
    "section": "Polynomial Fit Visualization",
    "text": "Polynomial Fit Visualization\n\n(ggplot(co2_2025, aes(x='month')) +\n  geom_point(aes(y='co2'), size=2) +\n  geom_line(aes(y='poly_pred'), color='red', size=1) +\n  labs(x='Month', y='CO2 (ppm)',\n       title='Polynomial Regression (degree 3)') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#problems-with-polynomial-regression",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#problems-with-polynomial-regression",
    "title": "JSC 370: Data Science II",
    "section": "Problems with Polynomial Regression",
    "text": "Problems with Polynomial Regression\n\nGlobal behavior: Changing one coefficient affects the entire curve\nInstability at boundaries: Polynomials can behave wildly at the edges (Runge’s phenomenon)\nPoor extrapolation: Predictions outside the data range are unreliable\nChoice of degree: How do we choose the right polynomial degree?\n\nWe need something more flexible and local. This motivates using splines."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#why-splines",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#why-splines",
    "title": "JSC 370: Data Science II",
    "section": "Why Splines?",
    "text": "Why Splines?\n\nPolynomial regression: models non-linearity but has global effects\nSplines: model structure in the mean function using piecewise polynomials\n\nGeneral setup:\n\\[y = f(x) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2 I)\\]\n\nGoal: estimate a smooth function \\(f(x)\\) that captures the trend\nBenefits:\n\nFlexible, nonparametric modeling of trends\nStill yields prediction errors (statistical smoothing)\nNaturally extended to generalized additive models (GAMs)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-intuition",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-intuition",
    "title": "JSC 370: Data Science II",
    "section": "Basis Functions: Intuition",
    "text": "Basis Functions: Intuition\nA basis is a set of simple functions \\(\\{b_j(x)\\}\\) that can be combined (with coefficients \\(\\beta_j\\)) to approximate more complex functions \\(f(x)\\):\n\\[f(x) = \\sum_{j=1}^k \\beta_j b_j(x)\\]\n\nExample: polynomial basis \\(b_1(x) = 1, \\quad b_2(x) = x, \\quad b_3(x) = x^2, \\ldots\\)\nAnalogy: like combining Lego blocks to build different shapes — basis functions are the blocks, coefficients \\(\\beta_j\\) are how much of each block we use\nThe regression coefficients \\(\\beta_j\\) control the contribution of each basis function"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-polynomial-example",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-polynomial-example",
    "title": "JSC 370: Data Science II",
    "section": "Basis Functions: Polynomial Example",
    "text": "Basis Functions: Polynomial Example\nRepresent a complicated function \\(f(x)\\) as a linear combination of simpler basis functions:\n\\[f(x) = \\sum_{j=1}^k \\beta_j b_j(x)\\]\nExample: polynomial basis in 1-D:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + \\epsilon_i\\]\nBasis functions:\n\\[b_1(x) = 1, \\quad b_2(x) = x, \\quad b_3(x) = x^2, \\quad b_4(x) = x^3, \\quad b_5(x) = x^4\\]"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-matrix-form",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#basis-functions-matrix-form",
    "title": "JSC 370: Data Science II",
    "section": "Basis Functions: Matrix Form",
    "text": "Basis Functions: Matrix Form\nCollect basis functions into a basis matrix \\(\\mathbf{B}\\):\n\\[\\mathbf{f} = \\mathbf{B}\\boldsymbol{\\beta}\\]\nExample (polynomial basis):\n\\[\\begin{bmatrix} f(x_1) \\\\ f(x_2) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 & x_1^4 \\\\ 1 & x_2 & x_2^2 & x_2^3 & x_2^4 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 & x_n^3 & x_n^4 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-visualization",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-visualization",
    "title": "JSC 370: Data Science II",
    "section": "Polynomial Basis Visualization",
    "text": "Polynomial Basis Visualization\n\nThe basis functions are each multiplied by \\(\\beta_j\\) and then summed to give the final curve \\(f(x)\\)."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-co₂-example",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-co₂-example",
    "title": "JSC 370: Data Science II",
    "section": "Polynomial Basis: CO₂ Example",
    "text": "Polynomial Basis: CO₂ Example\n\n\n\n\n\nMonthly CO₂ data fit with a polynomial basis, each colored curve is a basis function scaled by its coefficient."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-recap",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#polynomial-basis-recap",
    "title": "JSC 370: Data Science II",
    "section": "Polynomial Basis Recap",
    "text": "Polynomial Basis Recap\n\nA function \\(f(x)\\) can be written as a weighted sum of polynomial basis functions:\n\n\\[f(x) = \\sum_{j=0}^p \\beta_j x^j\\]\n\nThe coefficients \\(\\beta_j\\) control the shape of the curve\nPolynomials provide flexibility, but they can behave poorly:\n\nHigh-order polynomials become unstable (“wiggly”)\nPoor fit near boundaries (Runge’s phenomenon)\n\nThis motivates using splines: piecewise polynomials joined smoothly at knots"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#splines-concepts-types",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#splines-concepts-types",
    "title": "JSC 370: Data Science II",
    "section": "Splines: Concepts & Types",
    "text": "Splines: Concepts & Types\nA spline is a function made up of piecewise polynomials:\n\nDefined between knots (points where polynomial pieces join)\nJoined smoothly (continuous first and second derivatives)\n\nAdvantages over high-order polynomials:\n\nFlexible but stable (avoids oscillations at boundaries)\nLocal control: changing one knot only affects the nearby region\n\nGeneral form:\n\\[f(x) = \\sum_{j=1}^K \\beta_j b_j(x), \\quad \\text{where } b_j(x) \\text{ are spline basis functions}\\]"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#types-of-splines",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#types-of-splines",
    "title": "JSC 370: Data Science II",
    "section": "Types of Splines",
    "text": "Types of Splines\n\nCubic splines: piecewise cubic polynomials joined smoothly\nNatural splines: cubic splines with linear constraints at the boundaries (reduces edge wiggles)\nB-splines: basis representation of splines using local polynomial pieces\nSmoothing splines: knots at every observation, with a penalty \\(\\lambda\\) to control wiggliness\nCyclic splines: cubic splines with connected ends (for periodic data like seasons)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#cubic-splines-example",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#cubic-splines-example",
    "title": "JSC 370: Data Science II",
    "section": "Cubic Splines Example",
    "text": "Cubic Splines Example\n\nChoosing the number and placement of knots controls smoothness:\n\nFew knots \\(\\rightarrow\\) smoother curve\nMany knots \\(\\rightarrow\\) wiggly curve"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#visualizing-b-spline-basis-functions",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#visualizing-b-spline-basis-functions",
    "title": "JSC 370: Data Science II",
    "section": "Visualizing B-spline Basis Functions",
    "text": "Visualizing B-spline Basis Functions\n\nfrom scipy.interpolate import BSpline\n\n# Create B-spline basis functions\nx = np.linspace(0, 1, 100)\nknots = [0, 0, 0, 0, 0.25, 0.5, 0.75, 1, 1, 1, 1]\ndegree = 3\n\nbasis_df = pd.DataFrame({'x': x})\nfor i in range(len(knots) - degree - 1):\n    coeffs = np.zeros(len(knots) - degree - 1)\n    coeffs[i] = 1\n    spline = BSpline(knots, coeffs, degree)\n    basis_df[f'B{i}'] = spline(x)\n\nbasis_long = basis_df.melt(id_vars='x', var_name='basis', value_name='y')\n\n(ggplot(basis_long, aes(x='x', y='y', color='basis')) +\n  geom_line(size=1) +\n  labs(x='x', y='Basis function value',\n       title='B-spline Basis Functions (degree 3)') +\n  theme_minimal())\n\n\n\n\n\n\n\n\nEach B-spline basis function is non-zero only over a limited range — this gives local control."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-concept",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-concept",
    "title": "JSC 370: Data Science II",
    "section": "Smoothing Splines: Concept",
    "text": "Smoothing Splines: Concept\nGoal: Fit a smooth curve \\(f(x)\\) through noisy data \\((x_i, y_i)\\).\nWe want to balance:\n\nGoodness of fit: match the data closely\nSmoothness: avoid overly wiggly functions\n\nDefine an objective function:\n\\[\\text{RSS}(f) + \\lambda J(f)\\]\nwhere:\n\\[\\text{RSS}(f) = \\sum_i (y_i - f(x_i))^2, \\quad J(f) = \\int (f''(x))^2 \\, dx\\]\n\n\\(\\lambda \\geq 0\\) is the smoothness parameter\n\nSmall \\(\\lambda\\): fit the data very closely (wiggly curve)\nLarge \\(\\lambda\\): force \\(f\\) to be smoother (nearly linear)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-optimization",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-optimization",
    "title": "JSC 370: Data Science II",
    "section": "Smoothing Splines: Optimization",
    "text": "Smoothing Splines: Optimization\nWe minimize the penalized residual sum of squares:\n\\[\\min_f \\left\\{ \\sum_i (y_i - f(x_i))^2 + \\lambda \\int (f''(x))^2 \\, dx \\right\\}\\]\nThis is a trade-off between:\n\n\\(\\sum_i (y_i - f(x_i))^2\\) = data fidelity (how well the curve fits the observed points)\n\\(\\int (f''(x))^2 \\, dx\\) = roughness penalty (penalizes curvature/wiggliness)\n\nSolution: a natural cubic spline with knots at each unique \\(x_i\\).\nThe “best” curve is obtained by penalizing the wiggliness of the spline."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-basis-representation",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-basis-representation",
    "title": "JSC 370: Data Science II",
    "section": "Smoothing Splines: Basis Representation",
    "text": "Smoothing Splines: Basis Representation\nRepresent \\(f(x)\\) using spline basis functions:\n\\[f(x) = \\sum_{j=1}^K \\beta_j b_j(x)\\]\nLet \\(\\mathbf{B}\\) be the basis matrix and \\(\\mathbf{S}\\) be the penalty matrix:\n\\[B_{ij} = b_j(x_i), \\quad S_{jk} = \\int b_j''(x) \\, b_k''(x) \\, dx\\]\nThe optimization problem becomes:\n\\[\\min_\\beta \\; (\\mathbf{y} - \\mathbf{B}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{B}\\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^\\top \\mathbf{S} \\boldsymbol{\\beta}\\]\nSolution (penalized least squares):\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{B}^\\top \\mathbf{B} + \\lambda \\mathbf{S})^{-1} \\mathbf{B}^\\top \\mathbf{y}\\]"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-co₂-example",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#smoothing-splines-co₂-example",
    "title": "JSC 370: Data Science II",
    "section": "Smoothing Splines: CO₂ Example",
    "text": "Smoothing Splines: CO₂ Example\n\n\n\n\n\nA smoothing spline fit to the CO₂ data captures both the long-term trend and seasonal cycles."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#effect-of-the-smoothing-parameter-lambda",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#effect-of-the-smoothing-parameter-lambda",
    "title": "JSC 370: Data Science II",
    "section": "Effect of the Smoothing Parameter \\(\\lambda\\)",
    "text": "Effect of the Smoothing Parameter \\(\\lambda\\)\n\n\nLeft (\\(\\lambda\\) very small = 1e-4): spline is very wiggly, almost interpolates the noise\nMiddle (\\(\\lambda = 1\\)): good balance between smoothness and fit\nRight (\\(\\lambda\\) very large = 1000): curve is almost straight (linear trend)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#knots-vs.-penalty-parameter-lambda",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#knots-vs.-penalty-parameter-lambda",
    "title": "JSC 370: Data Science II",
    "section": "Knots vs. Penalty Parameter \\(\\lambda\\)",
    "text": "Knots vs. Penalty Parameter \\(\\lambda\\)\nIn smoothing splines:\n\nA knot is placed at each observation \\(x_i\\)\nThis gives the spline the potential to fit the data exactly\n\nThe penalty parameter \\(\\lambda\\) controls how much we use that flexibility:\n\n\\(\\lambda \\to 0\\): little penalty → spline interpolates the data (very wiggly)\n\\(\\lambda \\to \\infty\\): heavy penalty → spline approaches a straight line\n\nInterpretation:\n\nKnots: define the “vocabulary” of possible wiggles\n\\(\\lambda\\): decides how much of that vocabulary is actually used"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#knots-vs.-lambda-visualization",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#knots-vs.-lambda-visualization",
    "title": "JSC 370: Data Science II",
    "section": "Knots vs. \\(\\lambda\\): Visualization",
    "text": "Knots vs. \\(\\lambda\\): Visualization\n\n\nSmall \\(\\lambda\\): all knots are active (blue), spline wiggles to fit data closely\nModerate \\(\\lambda\\): only some knots are active, others are suppressed (grey), leading to a smoother curve\nLarge \\(\\lambda\\): only a few key knots remain active, spline is very smooth"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#types-of-1-d-splines-summary",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#types-of-1-d-splines-summary",
    "title": "JSC 370: Data Science II",
    "section": "Types of 1-D Splines: Summary",
    "text": "Types of 1-D Splines: Summary\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nCubic splines\nPiecewise cubic polynomials with smooth joins at knots\n\n\nNatural cubic splines\nCubic splines with boundary constraints (linear tails beyond boundary knots)\n\n\nPeriodic/Cyclic splines\nEnforce wrap-around continuity (function and derivatives match at endpoints)\n\n\nB-splines\nA numerically stable basis representation for polynomial splines (local support)\n\n\nRegression splines\nFinite set of knots; fit spline coefficients by least squares\n\n\nSmoothing/penalized splines\nKnots at every point, add a roughness penalty \\(\\lambda\\) to control fit-smoothness\n\n\nCardinal splines\nKnot placement is always a certain distance apart (common in grid settings)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#from-splines-to-gams",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#from-splines-to-gams",
    "title": "JSC 370: Data Science II",
    "section": "From Splines to GAMs",
    "text": "From Splines to GAMs\nA Generalized Additive Model (GAM) extends splines to multiple predictors. It is a regression model where predictors enter through a spline. GAMs can have a combination of splines and linear terms.\n\\[y = \\beta_0 + s_1(x_1) + s_2(x_2) + \\ldots + s_p(x_p) + \\epsilon\\]\nWhere each \\(s_j(x_j)\\) is a smooth function (typically a penalized cubic spline) of predictor \\(x_j\\).\nKey advantages:\n\nEach predictor can have its own non-linear relationship with the response\nThe effects are additive — total effect is the sum of individual smooth effects\nAutomatic smoothness selection — each \\(s_j\\) has its own penalty \\(\\lambda_j\\)\nInterpretable — you can plot each \\(s_j(x_j)\\) to see how predictor \\(j\\) affects \\(y\\)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#gams-in-python",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#gams-in-python",
    "title": "JSC 370: Data Science II",
    "section": "GAMs in Python",
    "text": "GAMs in Python\nIn Python, we can fit GAMs using statsmodels or pygam.\nHere we use pygam function LinearGAM\nRead more here\n\n# Prepare data\nX = co2_2025[['decimal date']].values\ny = co2_2025['co2'].values\n\n# Fit GAM with smoothing spline (pygam)\ngam = LinearGAM(s(0, n_splines=10)).fit(X, y)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#gam-summary",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#gam-summary",
    "title": "JSC 370: Data Science II",
    "section": "GAM Summary",
    "text": "GAM Summary\n\ngam.summary()\n  \nprint(gam.terms)\nprint(gam.coef_[:11])  # fitted coefficients\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                      4.0838\nLink Function:                     IdentityLink Log Likelihood:                                   -13.6117\nNumber of Samples:                           12 AIC:                                               37.3911\n                                                AICc:                                               47.847\n                                                GCV:                                                1.9142\n                                                Scale:                                              0.8918\n                                                Pseudo R-Squared:                                    0.847\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                10           4.1          1.45e-02     *           \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\ns(0) + intercept\n[ 36.58068341  38.26196477  39.94449927  41.23700237  40.90631244\n  38.83656747  36.99855708  37.08258697  38.46904898  40.03810926\n 388.35533059]"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#gam-predictions",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#gam-predictions",
    "title": "JSC 370: Data Science II",
    "section": "GAM Predictions",
    "text": "GAM Predictions\n\nco2_2025['gam_pred'] = gam.predict(X)\n\n(ggplot(co2_2025, aes(x='decimal date')) +\n  geom_point(aes(y='co2'), size=2) +\n  geom_line(aes(y='gam_pred'), color='blue', size=1) +\n  labs(x='Year', y='CO2 (ppm)',\n       title='GAM Fit to 2025 CO2 Data') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#fitting-gam-to-full-dataset",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#fitting-gam-to-full-dataset",
    "title": "JSC 370: Data Science II",
    "section": "Fitting GAM to Full Dataset",
    "text": "Fitting GAM to Full Dataset\nLet’s fit a GAM to capture both the long-term trend and seasonal cycles:\n\n# Prepare full dataset\nX_full = co2[['decimal date', 'month']].values\ny_full = co2['co2'].values\n\n# Fit GAM with two smooth terms\n# s(0) for long-term trend, s(1) for seasonal (cyclic)\n\ngam_full = LinearGAM(s(0, n_splines=20) + s(1, n_splines=12)).fit(X_full, y_full)\nprint(gam_full.summary())\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                      21.573\nLink Function:                     IdentityLink Log Likelihood:                                  -502.9597\nNumber of Samples:                          814 AIC:                                             1051.0655\n                                                AICc:                                            1052.4119\n                                                GCV:                                                0.2172\n                                                Scale:                                              0.4548\n                                                Pseudo R-Squared:                                   0.9998\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           14.0         1.11e-16     ***         \ns(1)                              [0.6]                12           7.6          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\nNone"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#full-gam-visualization",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#full-gam-visualization",
    "title": "JSC 370: Data Science II",
    "section": "Full GAM Visualization",
    "text": "Full GAM Visualization\n\nco2['gam_pred'] = gam_full.predict(X_full)\n\n(ggplot(co2, aes(x='decimal date')) +\n  geom_point(aes(y='co2'), alpha=0.3, size=0.5) +\n  geom_line(aes(y='gam_pred'), color='blue', size=0.8) +\n  labs(x='Year', y='CO2 (ppm)',\n       title='GAM Fit: Long-term Trend + Seasonal Cycles') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#partial-effects",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#partial-effects",
    "title": "JSC 370: Data Science II",
    "section": "Partial Effects",
    "text": "Partial Effects\nGAMs allow us to visualize each smooth term separately:\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot partial effect of decimal.date (long-term trend)\nXX = gam_full.generate_X_grid(term=0, n=100)\naxes[0].plot(XX[:, 0], gam_full.partial_dependence(term=0, X=XX))\naxes[0].set_xlabel('Year')\naxes[0].set_ylabel('Partial Effect')\naxes[0].set_title('Long-term Trend')\n\n# Plot partial effect of month (seasonal)\nXX = gam_full.generate_X_grid(term=1, n=12)\naxes[1].plot(XX[:, 1], gam_full.partial_dependence(term=1, X=XX))\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Partial Effect')\naxes[1].set_title('Seasonal Effect')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#connecting-geom_smooth-to-gams",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#connecting-geom_smooth-to-gams",
    "title": "JSC 370: Data Science II",
    "section": "Connecting geom_smooth() to GAMs",
    "text": "Connecting geom_smooth() to GAMs\nWhen you use geom_smooth() in plotnine:\n\nIt’s using similar smoothing techniques under the hood\nThe method parameter controls the smoothing approach:\n\n'lm': Linear regression\n'lowess': Local regression (default for small data)\n'gam': Generalized Additive Model (R’s ggplot2)\n\nThe smooth adapts to your data automatically\n\n\n# These are conceptually similar:\ngeom_smooth()  # Automatic smoothing\ngeom_smooth(method='lowess')  # Local regression\n# In R: geom_smooth(method='gam')  # GAM smoothing"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#key-gam-concepts",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#key-gam-concepts",
    "title": "JSC 370: Data Science II",
    "section": "Key GAM Concepts",
    "text": "Key GAM Concepts\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nSpline\nPiecewise polynomial with smooth connections\n\n\nKnots\nPoints where polynomial pieces connect\n\n\nBasis functions \\(b_j(x)\\)\nBuilding blocks combined to form the smooth\n\n\nPenalization (\\(\\lambda\\))\nControls wiggliness, prevents overfitting\n\n\nEDF\nEffective degrees of freedom (complexity measure)\n\n\n\nNotation convention:\n\n\\(f(x) = \\sum_j \\beta_j b_j(x)\\): a spline (smooth function built from basis functions)\n\\(s_j(x_j)\\): smooth term for predictor \\(x_j\\) in a GAM"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#when-to-use-gams",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#when-to-use-gams",
    "title": "JSC 370: Data Science II",
    "section": "When to Use GAMs",
    "text": "When to Use GAMs\nGAMs are useful when:\n\nYou expect non-linear relationships but don’t know the exact form\nYou want interpretable models (can visualize each smooth)\nYou have enough data to estimate smooth functions reliably\nYou want to capture seasonal patterns or trends\n\nCautions:\n\nGAMs can overfit with too many knots\nHigher EDF = more complex model = potential overfitting\nAlways validate on held-out data!"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#summary-smooth-regression",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#summary-smooth-regression",
    "title": "JSC 370: Data Science II",
    "section": "Summary: Smooth Regression",
    "text": "Summary: Smooth Regression\n\nLinear models assume straight-line relationships: \\(y = \\beta_0 + \\beta_1 x\\)\nPolynomial regression allows curves but has global effects and boundary issues\nSplines \\(f(x) = \\sum_j \\beta_j b_j(x)\\) are piecewise polynomials joined at knots\nSmoothing parameter \\(\\lambda\\) controls the bias-variance tradeoff\nGAMs model each predictor flexibly: \\(y = \\beta_0 + s_1(x_1) + \\ldots + s_p(x_p)\\)\ngeom_smooth() uses these techniques under the hood\nIn Python: use pygam or statsmodels for GAM fitting (pygam shown today)"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#python-libraries-for-gams",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#python-libraries-for-gams",
    "title": "JSC 370: Data Science II",
    "section": "Python Libraries for GAMs",
    "text": "Python Libraries for GAMs\n\n\n\nLibrary\nDescription\n\n\n\n\npygam\nPure Python GAM implementation, easy to use\n\n\nstatsmodels\nGLMGam class for GAMs\n\n\nscikit-learn\nSpline transformers for feature engineering\n\n\nscipy\nLow-level spline interpolation"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#references",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#references",
    "title": "JSC 370: Data Science II",
    "section": "References",
    "text": "References\n\nWood, S.N. (2017). Generalized Additive Models: An Introduction with R\nHastie, T. & Tibshirani, R. (1990). Generalized Additive Models\npygam documentation: https://pygam.readthedocs.io/\nplotnine documentation: https://plotnine.readthedocs.io/"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#gams-as-a-machine-learning-model",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#gams-as-a-machine-learning-model",
    "title": "JSC 370: Data Science II",
    "section": "GAMs as a “machine learning” model",
    "text": "GAMs as a “machine learning” model\nA generalized additive model (GAM) predicts (y) by adding up learned smooth functions of the predictors:\n\\[\ng(\\mathbb{E}[Y \\mid X])=\\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p)\n\\]\n\nNonlinear: each (f_j()) is a flexible curve (often a spline basis)\nRegularized: we control “wiggliness” with a penalty (bias–variance tradeoff)\nFits the ML pattern: choose flexibility by minimizing training loss + complexity penalty\nInterpretable: you can plot each learned effect (f_j(x_j)) (partial effect curves)\nGeneralized: works for continuous, binary, counts via a link (g())\n\nMental model: like linear regression, but each coefficient is replaced by a smooth curve."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#mapping-flight-routes",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#mapping-flight-routes",
    "title": "JSC 370: Data Science II",
    "section": "Mapping Flight Routes",
    "text": "Mapping Flight Routes\nThe nycflights13 package also includes airport coordinates. We can merge these in with flights to map where are the popular NYC destinations.\n\nfrom nycflights13 import airports\n\n# get routes by origin (EWR, JFK, LGA) and destination, count the number of flights per\n\nflight_routes = (flights\n    .groupby(['origin', 'dest'])\n    .size()\n    .reset_index(name='n_flights'))\n\n# extract geospatial and merging info about the airports\nairports[['faa', 'name', 'lat', 'lon']].head()\n\n\n\n\n\n\n\n\nfaa\nname\nlat\nlon\n\n\n\n\n0\n04G\nLansdowne Airport\n41.130472\n-80.619583\n\n\n1\n06A\nMoton Field Municipal Airport\n32.460572\n-85.680028\n\n\n2\n06C\nSchaumburg Regional\n41.989341\n-88.101243\n\n\n3\n06N\nRandall Airport\n41.431912\n-74.391561\n\n\n4\n09J\nJekyll Island Airport\n31.074472\n-81.427778"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-data",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-data",
    "title": "JSC 370: Data Science II",
    "section": "Flight Routes Data",
    "text": "Flight Routes Data\nMerge flight routes with geospatial information (latitude and longitude) by origin (it is the faa column in airports) and rename to have dest_lat, dest_lon along with origin_lat and origin_lon\n\nflight_routes_geo = (flight_routes\n    .merge(airports[['faa', 'lat', 'lon']],\n           left_on='dest', right_on='faa', how='inner')\n    .rename(columns={'lat': 'dest_lat', 'lon': 'dest_lon'})\n    .drop(columns=['faa'])\n    .merge(airports[['faa', 'lat', 'lon']],\n           left_on='origin', right_on='faa', how='inner')\n    .rename(columns={'lat': 'origin_lat', 'lon': 'origin_lon'})\n    .drop(columns=['faa']))\n\nprint(f\"Routes mapped: {len(flight_routes_geo)}\")\nflight_routes_geo.head()\n\nRoutes mapped: 217\n\n\n\n\n\n\n\n\n\norigin\ndest\nn_flights\ndest_lat\ndest_lon\norigin_lat\norigin_lon\n\n\n\n\n0\nEWR\nALB\n439\n42.748267\n-73.801692\n40.6925\n-74.168667\n\n\n1\nEWR\nANC\n8\n61.174361\n-149.996361\n40.6925\n-74.168667\n\n\n2\nEWR\nATL\n5022\n33.636719\n-84.428067\n40.6925\n-74.168667\n\n\n3\nEWR\nAUS\n968\n30.194528\n-97.669889\n40.6925\n-74.168667\n\n\n4\nEWR\nAVL\n265\n35.436194\n-82.541806\n40.6925\n-74.168667"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-map-with-basemap",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-map-with-basemap",
    "title": "JSC 370: Data Science II",
    "section": "Flight Routes Map (with Basemap)",
    "text": "Flight Routes Map (with Basemap)\n\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Set map extent first (in lat/lon)\nax.set_xlim(-130, -65)\nax.set_ylim(22, 52)\n\n# Add basemap - using CRS for lat/lon coordinates\nctx.add_basemap(ax, crs='EPSG:4326', source=ctx.providers.CartoDB.Positron)\n\n# Plot flight routes as lines (alpha based on number of flights)\nmax_flights = flight_routes_geo['n_flights'].max()\nfor _, row in flight_routes_geo.iterrows():\n    alpha = min(0.1 + 0.5 * (row['n_flights'] / max_flights), 0.6)\n    ax.plot([row['origin_lon'], row['dest_lon']],\n            [row['origin_lat'], row['dest_lat']],\n            color='steelblue', alpha=alpha, linewidth=0.5)\n\n# Plot destination airports\nax.scatter(flight_routes_geo['dest_lon'], flight_routes_geo['dest_lat'],\n           s=flight_routes_geo['n_flights']/100, c='coral', alpha=0.7,\n           edgecolors='darkred', linewidths=0.5, label='Destinations', zorder=4)\n\n# Plot NYC origin airports (get coords from airports data)\nnyc_airports = airports[airports['faa'].isin(['JFK', 'LGA', 'EWR'])]\nfor _, row in nyc_airports.iterrows():\n    ax.scatter(row['lon'], row['lat'], s=200, c='lime', marker='*',\n               edgecolors='darkgreen', linewidths=1, zorder=5)\n    ax.annotate(row['faa'], (row['lon'], row['lat']), xytext=(5, 5),\n                textcoords='offset points', fontsize=8, fontweight='bold', color='darkgreen')\n\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title('Flight Routes from NYC Airports (2013)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#basemap-options",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#basemap-options",
    "title": "JSC 370: Data Science II",
    "section": "Basemap Options",
    "text": "Basemap Options\nThe contextily library provides several basemap tile providers:\n\nctx.providers.CartoDB.Positron      # light gray, minimal labels\nctx.providers.CartoDB.PositronNoLabels\nctx.providers.Stamen.TonerLite      # black and white\n\n# Terrain/satellite\nctx.providers.Stamen.Terrain        # terrain with labels\nctx.providers.Esri.WorldImagery     # satellite imagery\n\n# Standard map styles\nctx.providers.OpenStreetMap.Mapnik  # standard OSM\nctx.providers.CartoDB.Voyager       # colorful, detailed\n\nUse crs='EPSG:4326' when your data is in latitude/longitude coordinates."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-flip-on-barplot",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-flip-on-barplot",
    "title": "JSC 370: Data Science II",
    "section": "Coordinate flip on barplot",
    "text": "Coordinate flip on barplot\nLet’s look at the top destinations from the NYC airports\n\n# Aggregate by destination\ntop_dests = (flight_routes\n    .groupby('dest')['n_flights']\n    .sum()\n    .sort_values(ascending=False)\n    .head(15)\n    .reset_index())\n\n(ggplot(top_dests, aes(x='reorder(dest, n_flights)', y='n_flights')) +\n  geom_bar(stat='identity', fill='steelblue') +\n  coord_flip() +\n  labs(x='Destination', y='Number of Flights',\n       title='Top 15 Destinations from NYC') +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-map",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#flight-routes-map",
    "title": "JSC 370: Data Science II",
    "section": "Flight Routes Map",
    "text": "Flight Routes Map\nLike we saw last week, we can make a basic map we can use matplotlib and contixtily (basemap).\n\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Set map extent\nax.set_xlim(-130, -65)\nax.set_ylim(22, 52)\n\n# Add basemap \nctx.add_basemap(ax, crs='EPSG:4326', source=ctx.providers.CartoDB.Positron)\n\n# Plot flight routes as lines (alpha based on number of flights)\nmax_flights = flight_routes_geo['n_flights'].max()\nfor _, row in flight_routes_geo.iterrows():\n    alpha = min(0.1 + 0.5 * (row['n_flights'] / max_flights), 0.6)\n    ax.plot([row['origin_lon'], row['dest_lon']],\n            [row['origin_lat'], row['dest_lat']],\n            color='steelblue', alpha=alpha, linewidth=0.5)\n\n# Plot destination airports (coords of airport destinations)\nax.scatter(flight_routes_geo['dest_lon'], flight_routes_geo['dest_lat'],\n           s=flight_routes_geo['n_flights']/100, c='coral', alpha=0.7,\n           edgecolors='darkred', linewidths=0.5, label='Destinations', zorder=4)\n\n# Plot NYC origin airports (use coords of NYC airports origin)\nnyc_airports = airports[airports['faa'].isin(['JFK', 'LGA', 'EWR'])]\nfor _, row in nyc_airports.iterrows():\n    ax.scatter(row['lon'], row['lat'], s=200, c='lime', marker='*',\n               edgecolors='darkgreen', linewidths=1, zorder=5)\n    ax.annotate(row['faa'], (row['lon'], row['lat']), xytext=(5, 5),\n                textcoords='offset points', fontsize=8, fontweight='bold', color='darkgreen')\n\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#visualizations-with-plotnine",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#visualizations-with-plotnine",
    "title": "JSC 370: Data Science II",
    "section": "Visualizations with plotnine",
    "text": "Visualizations with plotnine\nplotnine is designed on the principle of adding layers."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#adding-labels-and-themes",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#adding-labels-and-themes",
    "title": "JSC 370: Data Science II",
    "section": "Adding labels and themes",
    "text": "Adding labels and themes\nYou can make nicer axes and add titles with the labs layer\nAlso showing a minimal theme that removes the background grey theme_bw() (has a plot border) or theme_minimal() (no plot border, lighter and sparser grid lines)\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(alpha=0.3) +\n  geom_smooth() +\n  labs(\n    x='Atmospheric Pressure, millibars',\n    y='Departure Delay, minutes',\n    title='Association between weather and flight departure delays'\n  ) +\n  theme_bw())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#shape-of-points",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#shape-of-points",
    "title": "JSC 370: Data Science II",
    "section": "Shape of points",
    "text": "Shape of points\nBy default plotnine uses up to 6 shapes with shape= in the aes() aesthetics. If there are more, some of your data is not plotted!! (At least it warns you.)\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay', shape='origin')) +\n  geom_point(alpha=0.6) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#controlling-transparency-using-alpha",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#controlling-transparency-using-alpha",
    "title": "JSC 370: Data Science II",
    "section": "Controlling transparency using “alpha”",
    "text": "Controlling transparency using “alpha”\nThe alpha control can go in the layer that you are plotting, for example in the geom_point(alpha = ) layer we can directly control the transparency of the points (0 is fully transparent, 1 is fully opaque).\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay')) +\n  geom_point(alpha=0.4) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#points-shape",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#points-shape",
    "title": "JSC 370: Data Science II",
    "section": "Points shape",
    "text": "Points shape\nBy default plotnine uses up to 6 shapes with shape= in the aes() aesthetics. If there are more, some of your data is not plotted!! (At least it warns you.)\n\n(ggplot(flights_weather_day, aes(x='arr_delay', y='dep_delay', shape='origin')) +\n  geom_point(alpha=0.6) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-systems-1",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-systems-1",
    "title": "JSC 370: Data Science II",
    "section": "Coordinate systems",
    "text": "Coordinate systems\nThe “easy” ones you’ll use a lot\ncoord_flip() swaps x and y after the plot is built. Great for boxplots/bar charts to make labels readable.\ncoord_cartesian(xlim=..., ylim=...) zooms without dropping data (unlike hard limits on scales).\ncoord_fixed(ratio=1) forces equal aspect ratio (useful for geometry or maps).\nWhere it gets tricky\n\nStats + coords interactions: some geoms/stats compute first, then coords transform. Most of the time that’s fine, but it can surprise you when you expect “compute in the new coordinate system.”\nLimits behavior:\n\nscale_*_continuous(limits=...) can drop data before statistics are computed.\ncoord_cartesian(...) usually keeps the data and just zooms the view.\nNon-Cartesian coords (if you use them): e.g., coord_polar() changes the whole geometry, so not every plot makes sense there."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#smooth-regression-and-gams",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#smooth-regression-and-gams",
    "title": "JSC 370: Data Science II",
    "section": "Smooth Regression and GAMs",
    "text": "Smooth Regression and GAMs"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#interpreting-gam-summary",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#interpreting-gam-summary",
    "title": "JSC 370: Data Science II",
    "section": "Interpreting GAM Summary",
    "text": "Interpreting GAM Summary\nIn pygam output:\n\ns(k) represents a smooth function of predictor column k in your X matrix\nf(k) represents a factor/categorical term (discrete levels)\n\nNote on notation: In pygam, s() denotes smooth terms. This matches our mathematical notation \\(s_j(x_j)\\) for GAM smooth functions.\nEDoF (effective degrees of freedom) tells you “what shape did the model learn?”\n\nedf \\(\\approx\\) 1: the term is essentially linear (heavily penalized)\nedf 2–4: mild curvature\nedf large: more wiggle/complexity (less penalized)\n\nLambda (\\(\\lambda\\)) is the smoothing penalty — larger values = smoother curves"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#interpreting-gam-coefficients",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#interpreting-gam-coefficients",
    "title": "JSC 370: Data Science II",
    "section": "Interpreting GAM Coefficients",
    "text": "Interpreting GAM Coefficients\nThe output of gam.coef_[:] gives us the rank 10 spline basis weights (for s(0)): 36.58, 38.26, 39.94, 41.24, 40.91, 38.84, 37.00, 37.08, 38.47, 40.04\nThese are the beta weights \\[s(x) = \\sum_{j=1}^{10} \\beta_j B_j(x)\\]\nAnd the intercept: 388.35533059\nSo the prediction is \\(\\hat{y} = \\beta_0 + s(x)\\) and the individual \\(\\beta_j\\) values are usually not interpretable by themselves; the interpretable object is the curve \\(s(x)\\)."
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-4",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#multiple-geoms-4",
    "title": "JSC 370: Data Science II",
    "section": "Multiple Geoms 4",
    "text": "Multiple Geoms 4\nWe can layer the points with geom_point and error bars on the geom_smooth.\nWe can add color and linetype by origin airport.\n\n(ggplot(flights_weather_day, aes(x='pressure', y='dep_delay', linetype='origin',color='origin')) +\n  geom_smooth(se=True) +\n  geom_point(alpha=0.3) +\n  theme_minimal())"
  },
  {
    "objectID": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-flip-the-statistical-transformation-plot",
    "href": "slides/04-viz-ml1/JSC370-slides-04.html#coordinate-flip-the-statistical-transformation-plot",
    "title": "JSC 370: Data Science II",
    "section": "Coordinate flip the statistical transformation plot",
    "text": "Coordinate flip the statistical transformation plot\n\n(ggplot(flights_weather_ss, aes(x='dest', y='dep_delay')) +\n  stat_summary(fun_y=np.median,\n               fun_ymin=lambda x: np.percentile(x.dropna(), 25),\n               fun_ymax=lambda x: np.percentile(x.dropna(), 75)) +\n  coord_flip() +\n  theme_minimal())"
  }
]