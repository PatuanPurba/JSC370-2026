[
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-details",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-details",
    "title": "JSC 370: Data Science II",
    "section": "Course Details",
    "text": "Course Details\nLectures: Mondays 1–3pm, MS 3278\nLabs: Wednesdays 1–3pm, HS 108\n\nMeredith Franklin: meredith.franklin@utoronto.ca\nTAs: Johnny Meng and Mandy Yao"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-background",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-background",
    "title": "JSC 370: Data Science II",
    "section": "My Background",
    "text": "My Background\n\nIn late 2021 moved from Los Angeles where I was an Assistant/Associate Professor of Biostatistics at University of Southern California\nFrom Canada: McGill (BSc), Ottawa/Carleton Institute of Math (MSc), Harvard (PhD), UChicago (postdoc)\nHere I’m an Associate Professor with tenure in the Department of Statistical Science (51%) and the School of the Environment (49%)\nData Science Concentration Lead for the MScAC\nExecutive committee for the U of T Data Science Institute"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-teaching",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-teaching",
    "title": "JSC 370: Data Science II",
    "section": "My Teaching",
    "text": "My Teaching\n\nFounded a Master’s of Health Data Science program at USC that launched in 2020\nCo-taught the introduction data science course\nTaught graduate-level spatial statistics, inference, linear models\nAt U of T I teach STA465/STA2016/ENV1112 (Spatial Data Analysis) and I have also taught STA255 (Statistical Theory) and ENV1197 (Research Methods)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#my-research",
    "href": "slides/01-introduction/JSC370-slides-01.html#my-research",
    "title": "JSC 370: Data Science II",
    "section": "My Research",
    "text": "My Research\n\nSpatial statistical methods for environmental data\nEnvironmental epidemiology\nData science techniques for remote sensing data/imagery\nFocus on pollution (air, noise) and climate (ghg, land cover change)\nMachine learning/Deep learning for spatiotemporal data"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-goals",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-goals",
    "title": "JSC 370: Data Science II",
    "section": "Course Goals",
    "text": "Course Goals\nThrough this course, you will hone techniques used in data science. You will learn:\n\nProgramming in Python, and tools Markdown/Quarto, Git\nExploratory data analysis — generating hypotheses and building intuition\nData visualization — interpretable summaries\nData collection — APIs, data scraping, wrangling, cleaning\nStatistical and machine learning algorithms\nBuilding a github.io website with interaction to communicate your work"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#course-platforms",
    "href": "slides/01-introduction/JSC370-slides-01.html#course-platforms",
    "title": "JSC 370: Data Science II",
    "section": "Course Platforms",
    "text": "Course Platforms\n\nCourse website: weekly breakdown, lecture slides, labs, datasets\nhttps://jsc370.github.io/JSC370-2026/\nQuercus: announcements, Piazza discussion, grades, course logistics"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-data-science",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-data-science",
    "title": "JSC 370: Data Science II",
    "section": "What is data science?",
    "text": "What is data science?\n\nData science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.\n\n\n\n\n\n\n\nSource: https://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-do-data-scientists-actually-do",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-do-data-scientists-actually-do",
    "title": "JSC 370: Data Science II",
    "section": "What do data scientists actually do?",
    "text": "What do data scientists actually do?\n\nFrame a question and define success (what would convince you?)\nAcquire data (files, databases, APIs, scraping)\nClean + validate (units, missingness, duplicates, joins)\nExplore (EDA) → iterate → refine hypotheses\nModel + evaluate (prediction, inference, uncertainty)\nCommunicate results (reports, dashboards, reproducible code)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-scientists-have-great-responsibility-to-communicate-effectively",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-scientists-have-great-responsibility-to-communicate-effectively",
    "title": "JSC 370: Data Science II",
    "section": "Data Scientists have great responsibility to communicate effectively",
    "text": "Data Scientists have great responsibility to communicate effectively\n\nSource: https://xkcd.com/605/"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#why-communication-matters-in-data-science",
    "href": "slides/01-introduction/JSC370-slides-01.html#why-communication-matters-in-data-science",
    "title": "JSC 370: Data Science II",
    "section": "Why communication matters in Data Science",
    "text": "Why communication matters in Data Science\n\nOpaque models without uncertainty estimation or discussion of limitations\nBiased or unrepresentative data may lead to biased predictions\nConfounding \\(\\ne\\) causation (EDA can sometimes mislead)\nOverfitting and weak validation\nCommentary on Data Science: Towards Data Science"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#top-job-titles",
    "href": "slides/01-introduction/JSC370-slides-01.html#top-job-titles",
    "title": "JSC 370: Data Science II",
    "section": "Top Job Titles",
    "text": "Top Job Titles"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-scientists-in-demand",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-scientists-in-demand",
    "title": "JSC 370: Data Science II",
    "section": "Data Scientists in Demand",
    "text": "Data Scientists in Demand\n\nDemand for data science skills is high across many sectors.\nSkills show up again and again:\n\nPython + libraries (pandas, numpy, sklearn)\nSQL + relational thinking (joins, grouping)\nVisualization + storytelling (matplotlib/plotline/plotly)\nReproducibility (git, environments, reports)\nCommunication (clear write-ups, assumptions, limitations)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#global-labor-market-outlook",
    "href": "slides/01-introduction/JSC370-slides-01.html#global-labor-market-outlook",
    "title": "JSC 370: Data Science II",
    "section": "Global labor market outlook",
    "text": "Global labor market outlook\nWorld Economic Forum (Future of Jobs Report 2025) projects that by 2030:\n\n170 million roles created (across the labor market)\n92 million roles displaced\nNet +78 million jobs (~7% net growth)\n\nThese totals reflect all occupations (not just AI). Data/AI roles show up when we look at the fastest-growing job families. Source: World Economic Forum"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#which-jobs-are-growing-fastest",
    "href": "slides/01-introduction/JSC370-slides-01.html#which-jobs-are-growing-fastest",
    "title": "JSC 370: Data Science II",
    "section": "Which jobs are growing fastest?",
    "text": "Which jobs are growing fastest?\nFastest-growing roles (percentage terms) include:\n\nBig Data Specialists\nAI and Machine Learning Specialists\nFinTech Engineers\nSoftware and Application Developers"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#the-data-behind-the-plot",
    "href": "slides/01-introduction/JSC370-slides-01.html#the-data-behind-the-plot",
    "title": "JSC 370: Data Science II",
    "section": "The data behind the plot",
    "text": "The data behind the plot\n\n\n\nQuantity\nValue (millions)\n\n\n\n\nJobs created\n170\n\n\nJobs displaced\n92\n\n\nNet change\n78"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#visualization-of-these-data",
    "href": "slides/01-introduction/JSC370-slides-01.html#visualization-of-these-data",
    "title": "JSC 370: Data Science II",
    "section": "Visualization of these data",
    "text": "Visualization of these data\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show code\"\n#| fig-width: 6\n#| fig-height: 3\nimport matplotlib.pyplot as plt\nlabels = [\"Jobs created\", \"Jobs displaced\", \"Net change\"]\nvals = [170, 92, 78]  # WEF article summary\nplt.figure()\nplt.bar(labels, vals)\nplt.ylabel(\"Millions of jobs\")\nplt.title(\"Projected global job changes by 2030 (WEF)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-this-course",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-this-course",
    "title": "JSC 370: Data Science II",
    "section": "What is this course?",
    "text": "What is this course?\nThis course is an introduction to the world of data science following on from where JSC270 left off.\nWe will focus on transferable skills and modern workflows:\n\nPython + VS Code for computing\nQuarto for reproducible reports/slides\nGit/GitHub for version control and collaboration"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-you-should-expect",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-you-should-expect",
    "title": "JSC 370: Data Science II",
    "section": "What you should expect",
    "text": "What you should expect\n\nWeekly labs and bi-weekly homework using Python + Quarto\nSubmissions via GitHub Classroom (version control matters)\nFocus on reproducibility: clean repos, clear reports, runnable code\nCollaboration encouraged for discussion, but write-ups/code must be your own"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#data-science-resources-python",
    "href": "slides/01-introduction/JSC370-slides-01.html#data-science-resources-python",
    "title": "JSC 370: Data Science II",
    "section": "Data Science Resources: Python",
    "text": "Data Science Resources: Python\n\nPython for Data Analysis (3e) — Wes McKinney\n\nFocus: pandas + NumPy + Jupyter for data wrangling, cleaning, and analysis\n\nGood for: practical workflows and “how to do it in pandas”\n\nPython Data Science Handbook — Jake VanderPlas\n\nFocus: core stack NumPy, pandas, Matplotlib, plus intro ML tooling\n\nBonus: full text is online and backed by notebooks GitHub notebooks repo**"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#python-and-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#python-and-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "Python and VS Code",
    "text": "Python and VS Code\n\nPython: general-purpose language widely used for data science\nVS Code: a lightweight, extensible editor with strong Python + notebook support"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#what-is-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#what-is-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "What is VS Code?",
    "text": "What is VS Code?\n\nAn editor for code, notebooks, and markdown\nWorks well with:\n\nPython environments (conda/venv)\nJupyter notebooks\nConnecting to remote environments (e.g. Digital Alliance Canada, AWS)\nGit/GitHub\nQuarto render/preview"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#vs-code-things-youll-use-every-week",
    "href": "slides/01-introduction/JSC370-slides-01.html#vs-code-things-youll-use-every-week",
    "title": "JSC 370: Data Science II",
    "section": "VS Code: Things you’ll use every week",
    "text": "VS Code: Things you’ll use every week\n\nSelect the correct Python environment\nIntegrated terminal (run quarto render, git, ssh)\nSource control panel (stage/commit/push)\nQuarto preview for slides/reports"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#vs-code-layout",
    "href": "slides/01-introduction/JSC370-slides-01.html#vs-code-layout",
    "title": "JSC 370: Data Science II",
    "section": "VS Code: Layout",
    "text": "VS Code: Layout"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-and-python-and-vs-code",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-and-python-and-vs-code",
    "title": "JSC 370: Data Science II",
    "section": "Quarto and Python and VS Code",
    "text": "Quarto and Python and VS Code\n\nQuarto: markdown-based publishing for reports, sites, and slides"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#how-does-quarto-work",
    "href": "slides/01-introduction/JSC370-slides-01.html#how-does-quarto-work",
    "title": "JSC 370: Data Science II",
    "section": "How does Quarto work?",
    "text": "How does Quarto work?\n\n\n\n\n\n\n\n\nYou write a single source file (.qmd) that mixes text, code, and output options (YAML + chunk options).\nQuarto executes code (e.g., Python via Jupyter) and captures tables, figures, and printed output.\nIt renders the document through Pandoc into a chosen format (HTML report, reveal.js slides, PDF, Word, etc.).\nThe final output is a self-contained deliverable (e.g., report.html) that can be shared or published."
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto",
    "href": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto",
    "title": "JSC 370: Data Science II",
    "section": "Generating reports with Quarto",
    "text": "Generating reports with Quarto\n\nA Quarto document is a plain-text file with extension .qmd.\nAt the top is a YAML header that controls metadata and output (title, author, date, format, options).\nThe format field determines what Quarto produces:\n\nhtml → a web report (.html)\npdf → a PDF report (LaTeX installed)\ndocx → a Word document\nrevealjs → slides"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto-1",
    "href": "slides/01-introduction/JSC370-slides-01.html#generating-reports-with-quarto-1",
    "title": "JSC 370: Data Science II",
    "section": "Generating reports with Quarto",
    "text": "Generating reports with Quarto\nAn example yaml header for an html report\n---\ntitle: \"My Report\"\nformat: html\n---\n\nRendering turns MyReport.qmd into MyReport.html (and supporting files if needed).\nCore command: quarto render MyReport.qmd"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\nA code chunk might look like this:"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-1",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-1",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 9])\nplt.show()"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-2",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-2",
    "title": "JSC 370: Data Science II",
    "section": "Quarto code chunks",
    "text": "Quarto code chunks\nPreamble (options):\necho: show/hide code\neval: run or skip\nfig-width / fig-height: control plot size"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-and-options",
    "href": "slides/01-introduction/JSC370-slides-01.html#quarto-code-chunks-and-options",
    "title": "JSC 370: Data Science II",
    "section": "Quarto Code chunks and options",
    "text": "Quarto Code chunks and options\nCode:\nimport: libraries\n.plot, .show: make figures and display them"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#github",
    "href": "slides/01-introduction/JSC370-slides-01.html#github",
    "title": "JSC 370: Data Science II",
    "section": "GitHub",
    "text": "GitHub\n\nVersion control is essential in industry and academia\nBuilding a GitHub portfolio supports job hunting\nYou will build a github.io website as part of this course, with interactivity and app-type features"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#git-vs-github",
    "href": "slides/01-introduction/JSC370-slides-01.html#git-vs-github",
    "title": "JSC 370: Data Science II",
    "section": "Git vs GitHub",
    "text": "Git vs GitHub\n\nGit: version control tool on your computer\nGitHub: hosting + collaboration (remote repos, PRs, issues)\nWorkflow: edit → commit (git) → push (to GitHub)"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#this-week-checklist",
    "href": "slides/01-introduction/JSC370-slides-01.html#this-week-checklist",
    "title": "JSC 370: Data Science II",
    "section": "This week: checklist",
    "text": "This week: checklist\n\nInstall Python + VS Code + Quarto\nClone your GitHub Classroom repo\nRender a .qmd locally\nCommit + push your changes\nLab will be on your own this week, starting in person next week, January 14!"
  },
  {
    "objectID": "slides/01-introduction/JSC370-slides-01.html#next-week",
    "href": "slides/01-introduction/JSC370-slides-01.html#next-week",
    "title": "JSC 370: Data Science II",
    "section": "Next Week",
    "text": "Next Week\n\nLecture: Monday January 12, 1–3pm (Version control)\nLab: Wednesday January 14, 1–3pm (VS Code, Python, Quarto, GitHub)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability",
    "href": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability",
    "title": "JSC 370: Data Science II",
    "section": "Repeatability vs Reproducibility vs Replicability",
    "text": "Repeatability vs Reproducibility vs Replicability\n\nThese terms are often used interchangeably, but they are different.\nRepeatability: Generating the exact same results when using the same data by the same person.\nReproducibility: Generating the exact same results when using the same data by a different person or group. If we can’t reproduce a study, how can we replicate it?\nReplicability: Repeating a study by independently performing another study on new data."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#repeatability-vs-reproducibility-vs-replicability-1",
    "title": "JSC 370: Data Science II",
    "section": "Repeatability vs Reproducibility vs Replicability",
    "text": "Repeatability vs Reproducibility vs Replicability"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility\nA different analyst/researcher re-performs the analysis with the\n\nsame code and\nsame data and\nobtains the same result\n\n\n⚠️ If your results are not repeatable then they will not be reproducible!"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-2",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-3",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility",
    "text": "Reproducibility\nBarriers to doing reproducible work:\n\nPoor documentation\nManual steps\nNon-transferable tools\nIncorrect training\nTime"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Workflow",
    "text": "Reproducible Workflow"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nIn academia, incentives often prioritize publication. But many results are difficult to reproduce, so there’s a push to publish code, data, and the tools needed to re-run analyses."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nIn computational sciences and data analysis, what is reproducibility?\nDefinition: The data and code used to make a finding are available and presented so that an independent researcher can (relatively) straightforwardly recreate the result."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-2",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\nThis still seldom happens. Two examples from Tim Vines DataSeer.ai:\n\nData availability declines rapidly with article age (reported ~17% lower odds per year in one analysis).\nReanalyses using the program STRUCTURE found a substantial fraction of published results could not be reproduced (reported ~30% in one study)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-3",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nScientific articles often include detailed methods, but they are typically insufficient to reproduce a computational analysis.\nRoger Peng and Stephanie Hicks wrote: “Reproducibility is typically thwarted by a lack of availability of the original data and computer code.”\nScientists owe it to themselves and their community to keep an explicit record of all steps in a computational analysis."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-dos",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-dos",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Do’s",
    "text": "Reproducible Research Do’s\n\nStart with a good question: make it focused and something you care about.\nTeach your computer to do the work from beginning to end (automation &gt; manual steps).\nUse version control.\nTrack your software environment (toolchain + package versions).\nSet a random seed for any random generation/sampling (e.g., train/test splits).\nThink about the entire pipeline (raw data -&gt; cleaning -&gt; analysis -&gt; output)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Don’ts",
    "text": "Reproducible Research Don’ts\nDo not do things by hand. This includes:\n\nEditing spreadsheets to “clean” them (e.g., removing outliers, ad hoc QA/QC)\nManually editing tables or figures\nDownloading data by clicking around in a web browser\nSplitting data and moving it around manually\n\nIf something truly must be done by hand, document it explicitly."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducible-research-donts-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducible Research Don’ts",
    "text": "Reproducible Research Don’ts\n\nAvoid point-and-click or highly interactive tools when possible.\n\nThey often leave no trace of the steps.\nIf you must use them, write down the exact sequence of actions.\n\nSave the data and code that generated the output, rather than the output alone."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility Challenges",
    "text": "Reproducibility Challenges\n\nData size\n\nBuild tools into your code to manage large datasets (chunking, efficient formats, parallelism).\nStore data in smaller chunks and write code that pulls and combines files automatically.\nWrite metadata and use tools that support data organization."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#reproducibility-challenges-1",
    "title": "JSC 370: Data Science II",
    "section": "Reproducibility Challenges",
    "text": "Reproducibility Challenges\n\nData complexity\n\nUse smaller “toy” subsets to regularly check reproducibility.\nBe explicit about training/validation/test sets.\nUse diagnostic visualizations.\n\nWorkflow complexity\n\nUse README files (and keep them updated)."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control",
    "title": "JSC 370: Data Science II",
    "section": "What is version control?",
    "text": "What is version control?"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-is-version-control-1",
    "title": "JSC 370: Data Science II",
    "section": "What is version control?",
    "text": "What is version control?\nVersion control is the management of changes to documents and code. Changes are identified by a revision (e.g., “revision 1”, then “revision 2”, …). Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and sometimes merged.\n— Wikipedia (Version control)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care",
    "title": "JSC 370: Data Science II",
    "section": "Why do we care?",
    "text": "Why do we care?\nHave you ever…\n\nMade a change to code, realized it was a mistake, and wanted to revert?\nLost work (or only had an old backup)?\nNeeded to maintain multiple versions (e.g., “final_final_v3”)?\nWanted to compare two versions of your code to see exactly what changed?\nNeeded to prove that a particular change broke (or fixed) something?\nWanted to review the history of a file to understand why it looks like it does?"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care-contd",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-care-contd",
    "title": "JSC 370: Data Science II",
    "section": "Why do we care? (cont’d)",
    "text": "Why do we care? (cont’d)\nIn these cases (and many others), a version control system should make your life easier.\n\nWanted to submit a change to someone else’s code (without emailing files around)?\nWanted to share code and collaborate without overwriting each other?\nWanted to see who did what, when, and where (accountability and provenance)?\nWanted to experiment with a feature without disrupting working code?\n\n— Version Control (Atlassian)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#a-common-workflow-pattern",
    "href": "slides/02-version-control/JSC370-slides-02.html#a-common-workflow-pattern",
    "title": "JSC 370: Data Science II",
    "section": "A common workflow pattern",
    "text": "A common workflow pattern"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#git",
    "href": "slides/02-version-control/JSC370-slides-02.html#git",
    "title": "JSC 370: Data Science II",
    "section": "Git",
    "text": "Git\nGit was created by Linus Torvalds and originally described as “the stupid content tracker.”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-git",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-git",
    "title": "JSC 370: Data Science II",
    "section": "Why Git?",
    "text": "Why Git?\n\nDistributed architecture: you have the full history locally (you can work offline).\nEfficient branching and merging: easy to switch between branches, supporting experimentation and collaboration.\nIn this course (and likely beyond), we’ll use Git + GitHub.\nGit is the dominant version control system in practice (Stack Overflow reports very high adoption).\n\nSee: Stack Overflow discussion of survey results\n\nA great reference: Pro Git (free online)\nMore on the name: Git naming (Wikipedia)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#how-can-i-use-git",
    "href": "slides/02-version-control/JSC370-slides-02.html#how-can-i-use-git",
    "title": "JSC 370: Data Science II",
    "section": "How can I use Git?",
    "text": "How can I use Git?\nA few common ways to include Git in your workflow:\n\nCommand line (most universal)\nVS Code Source Control panel (very common for Python work)\nGitHub Desktop (beginner-friendly)\nGitKraken (feature-rich GUI)\nGitHub web interface (quick edits + reviews)\n\nMore alternatives: Git GUI clients"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-git-tracks",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-git-tracks",
    "title": "JSC 370: Data Science II",
    "section": "What Git tracks",
    "text": "What Git tracks"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#key-terms-youll-hear-a-lot",
    "href": "slides/02-version-control/JSC370-slides-02.html#key-terms-youll-hear-a-lot",
    "title": "JSC 370: Data Science II",
    "section": "Key terms you’ll hear a lot",
    "text": "Key terms you’ll hear a lot\n\nRepository (repo): the project + its version history\nCommit: a saved snapshot with a message (and an author + timestamp)\nBranch: an independent line of development (safe experimentation)\nMerge / Pull request: integrating changes back together\nRemote: a copy of the repo hosted elsewhere (e.g., GitHub)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#git-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#git-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Git workflow",
    "text": "Git workflow"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#setting-up-the-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#setting-up-the-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Setting up the workflow",
    "text": "Setting up the workflow\n\nGo to GitHub and sign in.\nCreate a repository (name it, choose public/private as appropriate, add a README).\nClone it (copy it onto your local machine).\nMake sure Git knows who you are (git config) and that authentication works (HTTPS token or SSH key).\n\nNote: We assume Git is installed: https://git-scm.com"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#cloning-visual",
    "href": "slides/02-version-control/JSC370-slides-02.html#cloning-visual",
    "title": "JSC 370: Data Science II",
    "section": "Cloning (visual)",
    "text": "Cloning (visual)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo",
    "href": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo",
    "title": "JSC 370: Data Science II",
    "section": "Workflow for an existing repo",
    "text": "Workflow for an existing repo\n\nStart by syncing (if you collaborate):\n\ngit pull\n\nMake changes in your editor.\nInspect what changed:\n\ngit status\n\ngit diff\n\nStage changes (choose what will go into the next commit):\n\ngit add &lt;file&gt; (or git add .)\n\nCommit with a clear message:\n\ngit commit -m \"Explain *why* you changed it\"\n\nPush your commits to GitHub: git push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo-cont",
    "href": "slides/02-version-control/JSC370-slides-02.html#workflow-for-an-existing-repo-cont",
    "title": "JSC 370: Data Science II",
    "section": "Workflow for an existing repo (con’t)",
    "text": "Workflow for an existing repo (con’t)\nUndo helpers (common):\n\nUnstage a file: git restore --staged &lt;file&gt;\nDiscard local edits to a file: git restore &lt;file&gt;\n(Older syntax you may see online: git checkout -- &lt;file&gt;)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-0-introduce-yourself",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-0-introduce-yourself",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 0: Introduce yourself",
    "text": "Hands-on 0: Introduce yourself\nIn terminal set up your Git identity (this writes to your global Git config):\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@email.com\"\nCheck what Git thinks your settings are:\ngit config --list\n(Press q to exit the pager.)\nFor more see the config customizations on the git site"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo\nGoal: create a GitHub repo and make your first commit.\n\nCreate a new repository on GitHub (e.g., JSC370). Include a README.md.\nOn your computer, choose where you want the project folder to live. Change to that directory.\nClone the repository (copy the HTTPS/SSH URL from GitHub). Then in terminal git clone https://github.com//.git\nEdit README.md (VS Code is fine).\nStage + commit + push:\n\ngit add README.md\ngit commit -m “Edit README”\ngit push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-1",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-1",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo\nSome useful checks are to see what’s pending:\n- git status\n- See commit history:\ngit log --oneline --decorate --graph --all"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-2",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-2",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-3",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-remote-repo-3",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Remote repo",
    "text": "Hands-on 1: Remote repo"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-local-first",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-local-first",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Local first",
    "text": "Hands-on 1: Local first\nThis creates a local repo (no GitHub yet), just to practice the cycle:\nmkdir -p ~/JSC370-demo\ncd ~/JSC370-demo\n\ngit init\necho \"Hello Git\" &gt; README.md\n\ngit status\ngit add README.md\ngit commit -m \"Add README\"\n\ngit log --oneline"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-connect-your-local-repo-to-github",
    "href": "slides/02-version-control/JSC370-slides-02.html#hands-on-1-connect-your-local-repo-to-github",
    "title": "JSC 370: Data Science II",
    "section": "Hands-on 1: Connect your local repo to GitHub",
    "text": "Hands-on 1: Connect your local repo to GitHub\nOn GitHub, create a new empty repo (do not add a README if you already have one locally). Then in your local project folder, add the GitHub repo as a remote named origin:\ngit remote add origin https://github.com/&lt;you&gt;/&lt;repo&gt;.git\nConfirm the remote was added:\ngit remote -v\nPush your local commits to GitHub\ngit push -u origin main\n(if your default branch is master instead of main git push -u origin master or if main doesn’t exist yet locally you can create or rename it git branch -M main then run the push command above.)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#removing-a-mistakenly-stagedtracked-file",
    "href": "slides/02-version-control/JSC370-slides-02.html#removing-a-mistakenly-stagedtracked-file",
    "title": "JSC 370: Data Science II",
    "section": "Removing a mistakenly staged/tracked file",
    "text": "Removing a mistakenly staged/tracked file\nIf you accidentally added a file you don’t want to track (example: class-notes.docx):\ngit rm --cached class-notes.docx\nThis removes it from Git tracking but not from your computer.\nThen prevent it from being tracked again using .gitignore"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#example-.gitignore",
    "href": "slides/02-version-control/JSC370-slides-02.html#example-.gitignore",
    "title": "JSC 370: Data Science II",
    "section": "Example .gitignore",
    "text": "Example .gitignore\nExample adapted from Pro Git\n# ignore all .a files\n*.a\n\n# but do track lib.a, even though you're ignoring .a files above\n!lib.a\n\n# only ignore the TODO file in the current directory, not subdir/TODO\n/TODO\n\n# ignore all files in any directory named build\nbuild/\n\n# ignore doc/notes.txt, but not doc/server/arch.txt\ndoc/*.txt\n\n# ignore all .pdf files in the doc/ directory and any of its subdirectories\ndoc/**/*.pdf"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branches-forks-pull-requests-merge-conflicts",
    "href": "slides/02-version-control/JSC370-slides-02.html#branches-forks-pull-requests-merge-conflicts",
    "title": "JSC 370: Data Science II",
    "section": "Branches, Forks, Pull Requests, Merge Conflicts",
    "text": "Branches, Forks, Pull Requests, Merge Conflicts\nA typical flow is: branch (or fork + branch) → pull request → merge → resolve conflicts (if needed)\nThese concepts make collaboration (mostly) painless:\n\nBranches: work in parallel without breaking main\nForks: work on a copy of a repo when you don’t have write access\nPull Requests: propose + review changes before merging\nMerge conflicts: what happens when Git can’t auto-combine edits"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-vs-fork",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-vs-fork",
    "title": "JSC 370: Data Science II",
    "section": "Branch vs Fork",
    "text": "Branch vs Fork\nBranch = a new line of work inside the same repository\nFork = your own copy of the entire repository under your account\nRule of thumb:\n\nWorking in a shared class/team repo → branch\nContributing to a repo you can’t write to → fork"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch",
    "title": "JSC 370: Data Science II",
    "section": "Branch",
    "text": "Branch\n\nRepo: course-repo\nYou create: student/meredith-lab2\nYou push to the same repo\nPR: student/meredith-lab2 → main\n\nBest for: teams/classes with shared access"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#fork",
    "href": "slides/02-version-control/JSC370-slides-02.html#fork",
    "title": "JSC 370: Data Science II",
    "section": "Fork",
    "text": "Fork\n\nUpstream repo: org/course-repo\nYour fork: yourname/course-repo\nYou work in your fork (often on a branch)\nPR: yourname:branch → org:main\n\nBest for: open-source external projects with no write access"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branches-what-problem-do-they-solve",
    "href": "slides/02-version-control/JSC370-slides-02.html#branches-what-problem-do-they-solve",
    "title": "JSC 370: Data Science II",
    "section": "Branches: what problem do they solve?",
    "text": "Branches: what problem do they solve?\nWithout branches:\n\nEveryone edits main\nWork collides\nIt’s hard to experiment safely\n\nWith branches:\n\nmain stays stable\nEach feature/bugfix happens on its own branch\nChanges are merged back only when ready\n\nBranches are easy in Git: creating/switching is fast."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-naming-conventions",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-naming-conventions",
    "title": "JSC 370: Data Science II",
    "section": "Branch naming conventions",
    "text": "Branch naming conventions\nIndustry-style examples: feature, bugfix, hotfix, release, documentation\n\nfeature/lab1\nbugfix/path-images\ndocs/update-syllabus\n\nTip: use short, descriptive names. Avoid temp and final2. Here are some additional conventions"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-naming-convention-for-the-course",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-naming-convention-for-the-course",
    "title": "JSC 370: Data Science II",
    "section": "Branch naming convention for the course",
    "text": "Branch naming convention for the course\nCourse pattern student/&lt;name&gt;-lab1\nExample: student/meredith-lab2\nWhy this works:\n\ncommunicates who owns the branch\ncommunicates what it’s for (e.g. lab number)\navoids confusion with repo folders (the / is just naming)\n\n\n\n\n\n\n\nNote\n\n\nBranch names are labels, not file paths. Tools may group student/* branches together, but Git treats the whole string as the branch name."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#common-branch-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#common-branch-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Common branch workflow",
    "text": "Common branch workflow\n\nStart from an up-to-date main\nCreate a new branch\nMake changes and commit on the branch\nPush the branch to GitHub"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#branch-workflow",
    "href": "slides/02-version-control/JSC370-slides-02.html#branch-workflow",
    "title": "JSC 370: Data Science II",
    "section": "Branch workflow",
    "text": "Branch workflow\ngit switch main\ngit pull\n\ngit switch -c student/meredith-lab2\n# edit files...\n\ngit add .\ngit commit -m \"Add lab 2 report\"\ngit push -u origin student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-do-switch-and--c-mean",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-do-switch-and--c-mean",
    "title": "JSC 370: Data Science II",
    "section": "What do switch and -c mean?",
    "text": "What do switch and -c mean?\ngit switch is the Git command to move between branches\n(older ways often use git checkout for this).\n\ngit switch main\n“Move my working directory to the main branch.”\ngit switch -c student/meredith-lab2\n-c means create a new branch and switch to it immediately.\n\nSo this is equivalent to two steps:\ngit branch student/meredith-lab2\ngit switch student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-do-we-start-from-main-and-pull",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-do-we-start-from-main-and-pull",
    "title": "JSC 370: Data Science II",
    "section": "Why do we start from main and pull?",
    "text": "Why do we start from main and pull?\ngit switch main\ngit pull\n\nEnsures your branch starts from the latest main\nReduces merge conflicts later\nMakes your Pull Request easier to review"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#how-these-concepts-connect",
    "href": "slides/02-version-control/JSC370-slides-02.html#how-these-concepts-connect",
    "title": "JSC 370: Data Science II",
    "section": "How these concepts connect",
    "text": "How these concepts connect\n\nBranch: where you do your work safely\nCommit: save a snapshot with a message\nPush: publish your branch to GitHub\nPull Request: ask to merge your branch into main\nMerge: integrate the branch work into main\nConflict: Git needs you to decide how to combine edits"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#pull-requests-what-are-they",
    "href": "slides/02-version-control/JSC370-slides-02.html#pull-requests-what-are-they",
    "title": "JSC 370: Data Science II",
    "section": "Pull Requests: what are they?",
    "text": "Pull Requests: what are they?\nA Pull Request (PR) is:\n\na proposal to merge one branch into another (often → main)\na review space (comments, approvals, requested changes)\na record of what changed and why (discussion + diff + commits)\n\nPRs are the standard way to collaborate on GitHub."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#why-use-pull-requests",
    "href": "slides/02-version-control/JSC370-slides-02.html#why-use-pull-requests",
    "title": "JSC 370: Data Science II",
    "section": "Why use Pull Requests?",
    "text": "Why use Pull Requests?\nPRs help you:\n\ncatch bugs early (someone else reads your diff)\nenforce project standards (formatting, tests, style)\ndocument decisions (“why did we do this?”)\nreduce “surprise merges” into main"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-makes-a-good-pull-request",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-makes-a-good-pull-request",
    "title": "JSC 370: Data Science II",
    "section": "What makes a good Pull Request?",
    "text": "What makes a good Pull Request?\n\nSmall enough to review (avoid mega-PRs)\nClear title + description\nExplains intent: what changed and why\nScreenshots/output examples when relevant\nLinks to an issue (if you use issues)\nIncludes only relevant files (no accidental large data, secrets, etc.)"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#example-pull-request-terminal-commands",
    "href": "slides/02-version-control/JSC370-slides-02.html#example-pull-request-terminal-commands",
    "title": "JSC 370: Data Science II",
    "section": "Example Pull Request terminal commands",
    "text": "Example Pull Request terminal commands\n# Start from main and get the latest changes\ngit switch main\ngit pull\n\n# Create a new branch for your lab work\ngit switch -c student/meredith-lab2\n\n#  Do your work (edit files in VS Code or any editor)\n# (example files you might create/edit)\n# - train.py\n# - requirements.txt\n# - README.md\n\n# Check what changed\ngit status\ngit diff\n\n# Stage and commit\ngit add train.py requirements.txt README.md\ngit commit -m \"Lab 2: add reproducible model training script\"\n\n# Push the branch to GitHub\ngit push -u origin student/meredith-lab2\n\n# Open a Pull Request on GitHub:\n#    student/meredith-lab2  --&gt;  main\n# After review changes are requested:\n# Make edits, then repeat add/commit/push\ngit add .\ngit commit -m \"Address PR feedback\"\ngit push"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#merging-what-does-it-mean",
    "href": "slides/02-version-control/JSC370-slides-02.html#merging-what-does-it-mean",
    "title": "JSC 370: Data Science II",
    "section": "Merging: what does it mean?",
    "text": "Merging: what does it mean?\nMerging integrates two lines of work by combining their histories:\n\nFast-forward merge: main simply moves forward (no divergence)\n3-way merge: Git creates a new merge commit that joins two lines of work\n\nEither way, the goal is the same: integrate branch work into main."
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#merge-conflicts-what-are-they",
    "href": "slides/02-version-control/JSC370-slides-02.html#merge-conflicts-what-are-they",
    "title": "JSC 370: Data Science II",
    "section": "Merge conflicts: what are they?",
    "text": "Merge conflicts: what are they?\nA merge conflict happens when:\n\ntwo branches edited the same lines in the same file, and\nGit can’t determine how to combine them safely\n\nImportant:\n\nConflicts are normal in collaboration\nThey’re not “errors” so much as “decisions Git asks humans to make”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#when-do-conflicts-happen-most",
    "href": "slides/02-version-control/JSC370-slides-02.html#when-do-conflicts-happen-most",
    "title": "JSC 370: Data Science II",
    "section": "When do conflicts happen most?",
    "text": "When do conflicts happen most?\n\nLong-lived branches (you drift far from main)\nMany people editing the same file\nMoving/renaming files while someone else edits them"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#preventing-conflicts-best-practices",
    "href": "slides/02-version-control/JSC370-slides-02.html#preventing-conflicts-best-practices",
    "title": "JSC 370: Data Science II",
    "section": "Preventing conflicts (best practices)",
    "text": "Preventing conflicts (best practices)\n\nPull often (or merge main into your branch regularly)\nKeep PRs small and merge them sooner\nAvoid huge “format everything” commits mixed with logic changes\nCommunicate: “I’m editing slides/week2.qmd today”"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#what-a-conflict-looks-like",
    "href": "slides/02-version-control/JSC370-slides-02.html#what-a-conflict-looks-like",
    "title": "JSC 370: Data Science II",
    "section": "What a conflict looks like",
    "text": "What a conflict looks like\nGit inserts markers like this into a file:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the version from main\n=======\nThis is the version from your branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; student/meredith-lab2"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#resources",
    "href": "slides/02-version-control/JSC370-slides-02.html#resources",
    "title": "JSC 370: Data Science II",
    "section": "Resources",
    "text": "Resources\n\nGit everyday commands: man giteveryday in terminal\nGitHub’s cheat sheets: https://github.github.com/training-kit/￼\nPro Git (free online): https://git-scm.com/book￼\nGit exercises: https://gitexercises.fracz.com/￼\nGitHub Guides (YouTube): https://www.youtube.com/user/GitHubGuides￼"
  },
  {
    "objectID": "slides/02-version-control/JSC370-slides-02.html#other-tools-to-explore",
    "href": "slides/02-version-control/JSC370-slides-02.html#other-tools-to-explore",
    "title": "JSC 370: Data Science II",
    "section": "Other tools to explore",
    "text": "Other tools to explore\n\nJira￼: issue/task tracking + workflow management\nGit + Jira integrations exist via many clients"
  },
  {
    "objectID": "labs/lab2/lab02-github.html",
    "href": "labs/lab2/lab02-github.html",
    "title": "Lab 02 - GitHub",
    "section": "",
    "text": "In this lab, you are expected to learn/put in practice the following skills:\n\nCreating your own repo for JSC370\nBranching on GitHub\nGit workflow clone/commit/push\nUsing pull requests (PR)"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-1-clone-the-repo-and-create-a-branch",
    "href": "labs/lab2/lab02-github.html#step-1-clone-the-repo-and-create-a-branch",
    "title": "Lab 02 - GitHub",
    "section": "Step 1: Clone the repo and create a branch",
    "text": "Step 1: Clone the repo and create a branch\nBranching is a core feature of Git that allows you to create an independent line of development. When you create a branch, you’re making a copy of the code at that point in time where you can make changes without affecting the main branch. This is how teams collaborate: each person works on their own branch, then proposes their changes via a pull request (i.e. “I have this great update to the project! Would you like to add it by pulling my branch into main?”).\nFirst, clone the repository to your local machine using the git clone command:\ncd &lt;your-directory&gt;\ngit clone &lt;repository-url&gt;\nOnce you have the repository on your local machine, create a new branch for your changes. Use your name or a descriptive name for the branch2:\ncd &lt;repository-name&gt;\ngit switch -c &lt;your-name&gt;\nThe -c flag creates a new branch and switches to it. You can verify you’re on your new branch by running:\ngit branch\nThis will show all branches with an asterisk (*) next to your current branch.\nWhen you’re ready to push your local branch to the remote repository for the first time, use:\ngit push --set-upstream origin your-name\nThis sets up tracking between your local branch and the remote, so future pushes can simply use git push. Now you’re ready to make changes without affecting the main branch!"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-2-modifying-the-corresponding-line",
    "href": "labs/lab2/lab02-github.html#step-2-modifying-the-corresponding-line",
    "title": "Lab 02 - GitHub",
    "section": "Step 2: Modifying the corresponding line",
    "text": "Step 2: Modifying the corresponding line\nIf you got the correct copy, you should find a very simple repository with only two files: .gitignore and README.md. The first file is just a reference file for git to know what things it should be “looking at” (checkout the lecture slides), so we will ignore it at this time (pun intended). The second file is the one that we will be playing with. The README file, which happens to be a Markdown file, contains, or at least will contain, your and your team members biographies. Here is what you need to do:\n\nFind the line with your name.\nIn that single line (i.e. not spanning multiple lines), write something about yourself, e.g. “I am from XYZ, I love doing ABC, …”.\n(optional) if you feel like it, add at the end of the line a picture of yourself (or avatar) using either html or markdown. This will require you to include the figure in the repo (if you are not linking a web fig).\nCommit the changes and push the changes to your branch using git commit and git push, e.g.\n\ngit add .\ngit commit -m \"[A short but meaningful message]\"\ngit push\nYou are now one step closer to make your first “pull request”. We will see how that happens in the next part."
  },
  {
    "objectID": "labs/lab2/lab02-github.html#step-3-do-the-pull-request",
    "href": "labs/lab2/lab02-github.html#step-3-do-the-pull-request",
    "title": "Lab 02 - GitHub",
    "section": "Step 3: Do the pull request",
    "text": "Step 3: Do the pull request\nThis is the final step. Overall, pull requests (PR) are as complex as the proposed changes are. The PR that you are about to make should go smoothly, yet, any time that you make a new PR, the changes should be able to be merged in the original repository without conflicts. Conflicts may only appear if the proposed changes are out-dated with respect to the main repository, meaning that the main repository was modified after you created your branch and your proposed changes cannot be merged without generating conflicts3. For now, let’s just look at the simple case.\nTo create the PR, you just need to go to the repository on GitHub and click on the “Compare & pull request” button for your branch:\n\n\n\nSource: Screenshot\n\n\nThis will create a PR in the reference repository. GitHub will automatically analyze the PR and check whether merging the PR to the main branch will result in a conflict or not. If all is OK, then the owner/admin of the repository can merge the PR. Otherwise, if there’s a conflict, you can go back to your local repo, make the needed changes, commit the changes, and push the changes to your branch. In this stage, the PR will automatically update to reflect the new changes you made.\nFor more information checkout Creating a pull request on GitHub.\n\nSubmit a pull request to https://github.com/JSC370/JSC370-lab2-2026-whoami\nAdd details of the pull request below (the commit id of your change (you can find this using git log)\n\ncommit:\nAuthor:\nDate:"
  },
  {
    "objectID": "labs/lab2/lab02-github.html#footnotes",
    "href": "labs/lab2/lab02-github.html#footnotes",
    "title": "Lab 02 - GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTeam-members could be working on the same file but editing different lines of code. If this is the case, after pull/push, git will integrate the changes without conflicts.↩︎\nFor more on branch naming conventions, see Naming conventions for Git Branches.↩︎\nMore info about how to deal with conflicts in this very neat post on stackoverflow.com How to resolve merge conflicts in Git. GitHub also has a way to solve conflicts in PRs, but this is only available to the admins of target repo. More info here.↩︎"
  },
  {
    "objectID": "labs/JSC370-lab-01.html",
    "href": "labs/JSC370-lab-01.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#overview",
    "href": "labs/JSC370-lab-01.html#overview",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "",
    "text": "In this lab you will:\n\nInstall/confirm VS Code, Python, and Quarto\nConfirm Quarto can run Python chunks (via Jupyter)\nRender a Quarto document locally\nUpload your .qmd to MarkUs"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#what-you-will-submit-to-markus",
    "href": "labs/JSC370-lab-01.html#what-you-will-submit-to-markus",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "What you will submit to MarkUs",
    "text": "What you will submit to MarkUs\n\nThis completed .qmd file. Download the lab .qmd on GitHub\nThe rendered HTML output (JSC370-lab-01.html)"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#before-you-start",
    "href": "labs/JSC370-lab-01.html#before-you-start",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "0. Before you start",
    "text": "0. Before you start\n\nIf you do not already have it, install VS Code. Then open this file in .\nIn VS Code open the terminal (Terminal -&gt; New Terminal). Recall you can toggle between terminals in VS Code. You should see a list of terminals on the bottom right panel."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#quarto",
    "href": "labs/JSC370-lab-01.html#quarto",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "1. Quarto",
    "text": "1. Quarto\nIn the terminal, run:\nquarto --version\nIf it is not installed, go to the Quarto download page and install for your OS: https://quarto.org/docs/download/.\nAfter installing, quit and reopen VS Code (so PATH updates), then run quarto –version again.\nPASTE YOUR VERSION HERE\nNow install the Quarto extension in VS Code:\n\nIn VS Code, click the Extensions icon (left sidebar), or press:\n\nmacOS: Cmd+Shift+X\nWindows/Linux: Ctrl+Shift+X\n\nSearch: Quarto\nInstall: Quarto (publisher: quarto-dev)\nYou may need to reload VS Code.\n\nPreview this lab in VS Code\nOpen this .qmd file, then preview it with Command Palette (Cmd/Ctrl+Shift+P) and select Quarto: Preview.\nTo stop preview: press Ctrl+C in the terminal running the preview."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#confirm-python-is-installed",
    "href": "labs/JSC370-lab-01.html#confirm-python-is-installed",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "2. Confirm Python is installed",
    "text": "2. Confirm Python is installed\nYou can try this in terminal:\n#| echo: true\npython3 --version\n(If python3 doesn’t work, try python –version in terminal.) Paste the output, but the chunk above should do it for you if installed correctly."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#install-core-packages-in-terminal",
    "href": "labs/JSC370-lab-01.html#install-core-packages-in-terminal",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "3. Install core packages in terminal",
    "text": "3. Install core packages in terminal\nRun the following in terminal. It is best to install packages in the terminal (not from inside Quarto).\npython3 -m pip install -U pip\npython3 -m pip install numpy pandas matplotlib jupyter ipykernel plotnine plotly\nAlso install the Jupyter extension in VS Code, similarly to how we installed the Quarto extension above (search for Jupyter).\nConfirm Quarto can see Jupyter\nquarto check jupyter\nPASTE OUTPUT HERE"
  },
  {
    "objectID": "labs/JSC370-lab-01.html#verify-your-python-environment",
    "href": "labs/JSC370-lab-01.html#verify-your-python-environment",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "4. Verify your Python environment",
    "text": "4. Verify your Python environment\nRun the chunk below. If this fails, double-check that VS Code is using the Python environment where you installed packages."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#your-turn-make-a-plot",
    "href": "labs/JSC370-lab-01.html#your-turn-make-a-plot",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "5. Your turn: make a plot",
    "text": "5. Your turn: make a plot\nUse the plotnine library (ggplot-like) to make a simple scatterplot. Update the code below:\n\nTitle must include your name (e.g., \"Penguin bill measurements — Your Name\").\nChange the point transparency by setting alpha to a new value."
  },
  {
    "objectID": "labs/JSC370-lab-01.html#render-.qmd-to-.html",
    "href": "labs/JSC370-lab-01.html#render-.qmd-to-.html",
    "title": "Lab 1 — Setup: VS Code with Python and Quarto",
    "section": "6. Render ‘.qmd’ to ‘.html’",
    "text": "6. Render ‘.qmd’ to ‘.html’\nIn the terminal, make sure you are in the folder containing this file, then run:\nquarto render JSC370-lab-01.qmd\nThis should create an HTML file in the same folder, e.g. JSC370-lab-01.html.\nConfirm it exists:\nls -l JSC370-lab-01.html\nPrint out the output and make sure it is in this lab. Also open the HTML in your browser to confirm it looks correct."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "JSC370: Data Science II",
    "section": "Resources",
    "text": "Resources\n\nHelpers and Templates\n\nQuarto (download/install)\nQuarto in VS Code\nQuarto VS Code extension\nJupyter extension for VS Code\nCookiecutter Data Science\nJupytext (pair notebooks with .py/.md)\nQuarto: Markdown Basics\nQuarto: Gallery (examples)\nMarkdown Guide\nMarkdown tutorial\n\n\n\nGuides\n\nPEP 8 – Style Guide for Python Code\nGoogle Python Style Guide\npandas: 10 minutes to pandas\nNumPy user guide\nMatplotlib tutorials\nseaborn documentation\nVega-Altair documentation\nscikit-learn: Getting Started\npre-commit\npytest: Getting Started\n\n\n\nTools\n\nVisual Studio Code\nPython\nCore libraries:\n\npandas\nNumPy\nscikit-learn\n\nData access & wrangling:\n\nRequests\nBeautiful Soup\nDuckDB Python API\nSQLAlchemy\n\nPublishing / reproducibility:\n\nQuarto\nPandoc\nGit\nGitHub\nGitHub Classroom\nGitHub Pages"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "JSC370: Data Science II",
    "section": "Data",
    "text": "Data\nMany of these websites provide APIs and/or bulk downloads.\n\nCanadian Data\n\nGovernment of Canada Open Data (Open Government Portal)\nStatistics Canada (Census + surveys)\nStatistics Canada Web Data Service (API)\nCanada GIS Data\nUniversity of Toronto Library (Geospatial data guides)\nCity of Toronto Open Data\nToronto Police Service Open Data\nOntario Data Catalogue\nPublic Health Ontario Open Data\nBritish Columbia Data Catalogue\n\n\n\nEnvironmental and Climate Data\n\nUS EPA Air Quality Data\nUS EPA AQS Data API (Air Quality System)\nNOAA National Centers for Environmental Information (NCEI)\nNOAA NCEI Access Data Service (API)\nNorth American Regional Climate Change Assessment Program (NARCCAP)\nNatural Resources Canada (Geospatial data/tools)\nCoastal wave / buoy data (CDIP, UCSD)\nGreat Lakes Bathymetry (NOAA)\nUS Energy Information Administration (EIA)\n\n\n\nInternational and Global Data\n\nUN Geospatial Hub\nUNdata API manual\nWorld Bank Open Data\nWorld Bank Developer / API info\nFAO FAOSTAT (global food/agriculture stats)\nFAOSTAT API entry point\nNASA Earthdata (data catalog)\n\n\n\nUS Data\n\nData.gov (catalog)\nData.gov Geospatial datasets\nUS Census (NHGIS)\nNYU Spatial Data Repository\nGoogle Earth Engine datasets\nGoogle Dataset Search\nFiveThirtyEight data (GitHub)\nLos Angeles Open Data\n\n\n\nHealth and Biological Data\n\nSEER (NIH/NCI cancer surveillance)\nWorld Health Organization (GHO OData API)\nCDC Open Data\nCalifornia Health & Human Services Open Data\nCanada COVID tracking (community project)\nUniProt (proteins)\nGene Ontology\n\n\n\nNews, Media, and Text Data\n\nNew York Times APIs (specs repo)\nThe Guardian Open Platform\nGDELT Project (global news/events)\nCommon Crawl (web archive)\nWikipedia / Wikimedia APIs (MediaWiki REST API)\nThe Movie Database\n\n\n\nSocial Networks and Platforms\n\nX (Twitter) Developer Platform\nGitHub REST API\nInstagram Platform\nLinkedIn API documentation (Microsoft Learn)\nSpotify Web API\nZillow Group Data & APIs\n\n\n\nAcademic Publications and Research Data\n\nFigshare\nZenodo\nHarvard Dataverse\nElsevier Developer APIs\nOpenAlex (scholarly metadata + API)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#goals",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#goals",
    "title": "JSC 370: Data Science II",
    "section": "Goals",
    "text": "Goals\n\nLoad in large datasets stored locally and remotely (GitHub), check memory issues\nUnderstand what EDA is (and is not)\nBuild a repeatable EDA workflow\nMake plots that reveal structure, not just decoration\nAvoid common visualization pitfalls"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#data-science-pipeline",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#data-science-pipeline",
    "title": "JSC 370: Data Science II",
    "section": "Data Science Pipeline",
    "text": "Data Science Pipeline\n\nWe will work on the blue words today"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#what-is-exploratory-data-analysis",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#what-is-exploratory-data-analysis",
    "title": "JSC 370: Data Science II",
    "section": "What is Exploratory Data Analysis?",
    "text": "What is Exploratory Data Analysis?\n\nEDA = iterative exploration and building modeling intuition\nFocus on data distributions, missingness, relationships, anomalies\nCreate: summary stats, exploratory plots\nOutput: questions, hypotheses, and next steps"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-data-analysis",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-data-analysis",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the process of summarizing data\nIt should be the first step in your analysis pipeline, and it serves to:\n\n✅ check data (import issues, outliers, missing values, data errors)\n🧹 clean data (fix implausible values, remove missing if necessary, rename, etc.)\n\\(\\Sigma\\) summary statistics of key variables (univariate and bivariate)\n📊 basic plots and graphs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#eda-checklist",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#eda-checklist",
    "title": "JSC 370: Data Science II",
    "section": "EDA Checklist",
    "text": "EDA Checklist\nEDA Checklist:\n\n\nFormulate a question\n\n\nRead in the data\n\n\nCheck the dimensions and headers and footers of the data\n\n\nCheck the variable types in the data\n\n\nTake a closer look at some/all of the variables\n\n\nValidate with an external source\n\n\nConduct some summary statistics to answer the initial question\n\n\nMake exploratory graphs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#case-study",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#case-study",
    "title": "JSC 370: Data Science II",
    "section": "Case Study",
    "text": "Case Study\nWe are going to use a dataset created from the National Center for Environmental Information NOAA/NCEI. The data are hourly measurements from weather stations across the continental U.S."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#formulate-a-question",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#formulate-a-question",
    "title": "JSC 370: Data Science II",
    "section": "Formulate a Question",
    "text": "Formulate a Question\nIt is a good idea to first have a question such as:\n\nwhat weather stations reported the hottest and coldest daily temperatures?\nwhat day of the month was on average the hottest?\nis there covariation between temperature and humidity in my dataset?"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data",
    "title": "JSC 370: Data Science II",
    "section": "Read in the data",
    "text": "Read in the data\nThere are several ways to read in data (some depend on the type of data you have):\n\npandas.read_csv() for delimited files\npandas.read_parquet() for parquet\npandas.read_feather() for feather\npandas.read_excel() for .xls and .xlsx\npyarrow.dataset / dask for larger-than-memory workflows"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#read-in-the-data-1",
    "title": "JSC 370: Data Science II",
    "section": "Read in the data",
    "text": "Read in the data\nWe’ll use Python packages pandas, numpy, matplotlib, seaborn, plotnine\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotnine import ggplot, aes, geom_point, labs"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-local-file",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-local-file",
    "title": "JSC 370: Data Science II",
    "section": "Example: read in local file",
    "text": "Example: read in local file\nIf your data are a gzipped delimited file (e.g., met_all.gz), pandas can read it directly.\n\n\nmet = pd.read_csv(\"met_all.gz\", compression=\"gzip\")\nmet.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n0\n690150\n93121\n2019\n8\n1\n0\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n37.2\n5\n10.6\n5\n1009.9\n5\n19.881266\n\n\n1\n690150\n93121\n2019\n8\n1\n1\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n35.6\n5\n10.6\n5\n1010.3\n5\n21.760980\n\n\n2\n690150\n93121\n2019\n8\n1\n2\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n34.4\n5\n7.2\n5\n1010.6\n5\n18.482122\n\n\n3\n690150\n93121\n2019\n8\n1\n3\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n33.3\n5\n5.0\n5\n1011.6\n5\n16.888622\n\n\n\n\n4 rows × 30 columns"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-github-file",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#example-read-in-github-file",
    "title": "JSC 370: Data Science II",
    "section": "Example: read in github file",
    "text": "Example: read in github file\nIf your data are a gzipped delimited file but stored on github, use\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nmet = pd.read_csv(url, compression=\"gzip\")    \nmet.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n0\n690150\n93121\n2019\n8\n1\n0\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n37.2\n5\n10.6\n5\n1009.9\n5\n19.881266\n\n\n1\n690150\n93121\n2019\n8\n1\n1\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n35.6\n5\n10.6\n5\n1010.3\n5\n21.760980\n\n\n2\n690150\n93121\n2019\n8\n1\n2\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n34.4\n5\n7.2\n5\n1010.6\n5\n18.482122\n\n\n3\n690150\n93121\n2019\n8\n1\n3\n56\n34.3\n-116.166\n696\n...\n5\nN\n5\n33.3\n5\n5.0\n5\n1011.6\n5\n16.888622\n\n\n\n\n4 rows × 30 columns"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-types-of-variables-in-our-dataset",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-types-of-variables-in-our-dataset",
    "title": "JSC 370: Data Science II",
    "section": "Check the types of variables in our dataset",
    "text": "Check the types of variables in our dataset\nWe can get the types of data (integer, float, character) from met.info. This also tells us the memory usage.\n\nmet.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2377343 entries, 0 to 2377342\nData columns (total 30 columns):\n #   Column             Dtype  \n---  ------             -----  \n 0   USAFID             int64  \n 1   WBAN               int64  \n 2   year               int64  \n 3   month              int64  \n 4   day                int64  \n 5   hour               int64  \n 6   min                int64  \n 7   lat                float64\n 8   lon                float64\n 9   elev               int64  \n 10  wind.dir           float64\n 11  wind.dir.qc        object \n 12  wind.type.code     object \n 13  wind.sp            float64\n 14  wind.sp.qc         object \n 15  ceiling.ht         float64\n 16  ceiling.ht.qc      int64  \n 17  ceiling.ht.method  object \n 18  sky.cond           object \n 19  vis.dist           float64\n 20  vis.dist.qc        object \n 21  vis.var            object \n 22  vis.var.qc         object \n 23  temp               float64\n 24  temp.qc            object \n 25  dew.point          float64\n 26  dew.point.qc       object \n 27  atm.press          float64\n 28  atm.press.qc       int64  \n 29  rh                 float64\ndtypes: float64(10), int64(10), object(10)\nmemory usage: 544.1+ MB"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#what-counts-as-very-large-data-on-a-laptop",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#what-counts-as-very-large-data-on-a-laptop",
    "title": "JSC 370: Data Science II",
    "section": "What counts as “very large” data on a laptop?",
    "text": "What counts as “very large” data on a laptop?\nOn laptops, “very large” usually means it stresses RAM and slows basic operations.\nA dataset can be “very large” even with only a few million rows if it has:\n\nmany columns\nlots of strings (object dtype)\nexpensive operations (joins, groupbys, sorting)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#practical-rule-of-thumb",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#practical-rule-of-thumb",
    "title": "JSC 370: Data Science II",
    "section": "Practical rule-of-thumb",
    "text": "Practical rule-of-thumb\nThink in terms of memory, not just rows × columns.\n\nComfortable: under ~1 GB in RAM\nLarge: ~1–5 GB (needs care: dtypes, filtering early)\nVery large: &gt; ~5 GB or doesn’t fit in RAM → you need a different approach/tool"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas",
    "title": "JSC 370: Data Science II",
    "section": "Measure size in pandas",
    "text": "Measure size in pandas\n\nCalculate the total memory used by the DataFrame (convert bytes -&gt; GB)\n\n\nmet.memory_usage(deep=True).sum() / 1024**3\n\nnp.float64(1.401238577440381)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#measure-size-in-pandas-1",
    "title": "JSC 370: Data Science II",
    "section": "Measure size in pandas",
    "text": "Measure size in pandas\n\nFind the biggest memory columns (this time in MB)\n\n\n(met.memory_usage(deep=True).sort_values(ascending=False) / 1024**2).head(10)\n\ndew.point.qc         113.360548\ntemp.qc              113.360548\nvis.var              113.360548\nwind.type.code       113.360548\nsky.cond             113.360548\nceiling.ht.method    113.360548\nvis.dist.qc          103.298048\nvis.var.qc           102.860548\nwind.sp.qc            99.797945\nwind.dir.qc           85.994595\ndtype: float64"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#why-strings-make-data-big",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#why-strings-make-data-big",
    "title": "JSC 370: Data Science II",
    "section": "Why strings make data “big”",
    "text": "Why strings make data “big”\n\nIn pandas, object columns (strings) are often the memory hog.\ndtypes tells is what kind of variables we have in a dataset.\n\n\nmet.dtypes\n\nUSAFID                 int64\nWBAN                   int64\nyear                   int64\nmonth                  int64\nday                    int64\nhour                   int64\nmin                    int64\nlat                  float64\nlon                  float64\nelev                   int64\nwind.dir             float64\nwind.dir.qc           object\nwind.type.code        object\nwind.sp              float64\nwind.sp.qc            object\nceiling.ht           float64\nceiling.ht.qc          int64\nceiling.ht.method     object\nsky.cond              object\nvis.dist             float64\nvis.dist.qc           object\nvis.var               object\nvis.var.qc            object\ntemp                 float64\ntemp.qc               object\ndew.point            float64\ndew.point.qc          object\natm.press            float64\natm.press.qc           int64\nrh                   float64\ndtype: object"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously",
    "title": "JSC 370: Data Science II",
    "section": "Big data can be read in more cautiously",
    "text": "Big data can be read in more cautiously\n\nIf the file is really big (ours is borderline), we could read only the columns we need. This is where having a question in mind is useful. For ours, we probably just want station location (lat, lon), year, month, day, hour, temperature and humidity.\n\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nusecols = [\"USAFID\",\"lat\",\"lon\", \"year\", \"month\",\"day\",\"hour\",\"temp\", \"rh\"]\nmet_small = pd.read_csv(\n    url,\n    compression=\"gzip\",\n    usecols=usecols\n)\nmet_small.head(4)\n\n\n\n\n\n\n\n\nUSAFID\nyear\nmonth\nday\nhour\nlat\nlon\ntemp\nrh\n\n\n\n\n0\n690150\n2019\n8\n1\n0\n34.3\n-116.166\n37.2\n19.881266\n\n\n1\n690150\n2019\n8\n1\n1\n34.3\n-116.166\n35.6\n21.760980\n\n\n2\n690150\n2019\n8\n1\n2\n34.3\n-116.166\n34.4\n18.482122\n\n\n3\n690150\n2019\n8\n1\n3\n34.3\n-116.166\n33.3\n16.888622"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#big-data-can-be-read-in-more-cautiously-1",
    "title": "JSC 370: Data Science II",
    "section": "Big data can be read in more cautiously",
    "text": "Big data can be read in more cautiously\nChunking in pandas means reading in a file in pieces so you don’t load the whole dataset into RAM at once.\n\nurl = \"https://raw.githubusercontent.com/JSC370/JSC370-2026/main/data/met_all.gz\"\nchunks = pd.read_csv(url, compression=\"gzip\", chunksize=200_000)\nn = 0\ntemp_sum = 0.0\nfor ch in chunks:\n    temp_sum += ch[\"temp\"].sum(skipna=True)\n    n += ch[\"temp\"].notna().sum()\ntemp_sum / n\n\nnp.float64(23.590542469664527)\n\n\nThis computes the overall mean temperature without ever storing the full dataset:\n\ntemp_sum accumulates the sum of temps across chunks.\nn accumulates how many non-missing temps you saw.\ntemp_sum / n is the mean.\n\nWe’re basically doing: \\(\\bar{x} = \\frac{\\sum x_i}{\\#\\{x_i \\text{ not missing}\\}}\\)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data",
    "title": "JSC 370: Data Science II",
    "section": "Check the data",
    "text": "Check the data\nOnce we have loaded the data we should check the dimensions of the dataset. In pandas, the we use met.shape.\n\nprint(\"shape:\", met.shape)\nprint(\"n_rows:\", met.shape[0])\nprint(\"n_cols:\", met.shape[1])\n\nshape: (2377343, 30)\nn_rows: 2377343\nn_cols: 30"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-summary",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-summary",
    "title": "JSC 370: Data Science II",
    "section": "Check the data: Summary",
    "text": "Check the data: Summary\n\nWe see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables.\nWe know that we are supposed to have hourly measurements of weather data for the month of August 2019 for the entire US. We should check that we have all of these components. Let us check:\n\n\nthe year, month, hours\nthe range of locations (latitude and longitude)\ntemperature\nrelative humidity"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely",
    "title": "JSC 370: Data Science II",
    "section": "Check the data more closely",
    "text": "Check the data more closely\nWe can get a quick summary from met.describe.\n\nmet.describe()\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\nwind.dir\nwind.sp\nceiling.ht\nceiling.ht.qc\nvis.dist\ntemp\ndew.point\natm.press\natm.press.qc\nrh\n\n\n\n\ncount\n2.377343e+06\n2.377343e+06\n2377343.0\n2377343.0\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n2.377343e+06\n1.592053e+06\n2.297650e+06\n2.256068e+06\n2.377343e+06\n2.296387e+06\n2.317254e+06\n2.311055e+06\n711069.000000\n2.377343e+06\n2.310917e+06\n\n\nmean\n7.230995e+05\n2.953885e+04\n2019.0\n8.0\n1.599589e+01\n1.134106e+01\n3.956097e+01\n3.794132e+01\n-9.214509e+01\n4.158152e+02\n1.850082e+02\n2.459166e+00\n1.616648e+04\n5.027185e+00\n1.492117e+04\n2.359054e+01\n1.702084e+01\n1014.164390\n7.728208e+00\n7.164138e+01\n\n\nstd\n2.156678e+03\n3.336921e+04\n0.0\n0.0\n8.917096e+00\n6.860069e+00\n1.724761e+01\n5.171552e+00\n1.195324e+01\n5.737842e+02\n9.230697e+01\n2.147395e+00\n9.282465e+03\n1.249445e+00\n3.879577e+03\n6.059275e+00\n6.206226e+00\n4.059265\n2.018170e+00\n2.275361e+01\n\n\nmin\n6.901500e+05\n1.160000e+02\n2019.0\n8.0\n1.000000e+00\n0.000000e+00\n0.000000e+00\n2.455000e+01\n-1.242900e+02\n-1.300000e+01\n3.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n0.000000e+00\n-4.000000e+01\n-3.720000e+01\n960.500000\n1.000000e+00\n8.334298e-01\n\n\n25%\n7.209280e+05\n3.706000e+03\n2019.0\n8.0\n8.000000e+00\n5.000000e+00\n2.300000e+01\n3.396700e+01\n-9.801800e+01\n1.010000e+02\n1.200000e+02\n0.000000e+00\n3.048000e+03\n5.000000e+00\n1.609300e+04\n1.960000e+01\n1.380000e+01\n1011.800000\n5.000000e+00\n5.579015e+01\n\n\n50%\n7.227280e+05\n1.386000e+04\n2019.0\n8.0\n1.600000e+01\n1.100000e+01\n5.000000e+01\n3.835000e+01\n-9.170800e+01\n2.520000e+02\n1.800000e+02\n2.100000e+00\n2.200000e+04\n5.000000e+00\n1.609300e+04\n2.350000e+01\n1.810000e+01\n1014.100000\n9.000000e+00\n7.655374e+01\n\n\n75%\n7.250900e+05\n5.476700e+04\n2019.0\n8.0\n2.400000e+01\n1.700000e+01\n5.500000e+01\n4.194000e+01\n-8.298600e+01\n4.000000e+02\n2.600000e+02\n3.600000e+00\n2.200000e+04\n5.000000e+00\n1.609300e+04\n2.780000e+01\n2.170000e+01\n1016.400000\n9.000000e+00\n9.062916e+01\n\n\nmax\n7.268130e+05\n9.499800e+04\n2019.0\n8.0\n3.100000e+01\n2.300000e+01\n5.900000e+01\n4.894100e+01\n-6.831300e+01\n9.999000e+03\n3.600000e+02\n3.600000e+01\n2.200000e+04\n9.000000e+00\n1.600000e+05\n5.600000e+01\n3.600000e+01\n1059.900000\n9.000000e+00\n1.000000e+02\n\n\n\n\n\n\n\n\nNote that this doesn’t tell us if there are any missing data!"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-the-data-more-closely-1",
    "title": "JSC 370: Data Science II",
    "section": "Check the data more closely",
    "text": "Check the data more closely\nIt is a little harder to quickly summarize missing data in pandas, but here’s a way to count missing observations and calculate the percent missing.\n\nmissing = met.isna().sum().to_frame(\"n_missing\")\nmissing[\"pct_missing\"] = (missing[\"n_missing\"] / len(met)).round(4)\nmissing.sort_values(\"pct_missing\", ascending=False).head(20)\n\n\n\n\n\n\n\n\nn_missing\npct_missing\n\n\n\n\natm.press\n1666274\n0.7009\n\n\nwind.dir\n785290\n0.3303\n\n\nceiling.ht\n121275\n0.0510\n\n\nvis.dist\n80956\n0.0341\n\n\nwind.sp\n79693\n0.0335\n\n\ndew.point\n66288\n0.0279\n\n\nrh\n66426\n0.0279\n\n\ntemp\n60089\n0.0253\n\n\nlat\n0\n0.0000\n\n\nday\n0\n0.0000\n\n\natm.press.qc\n0\n0.0000\n\n\nyear\n0\n0.0000\n\n\ndew.point.qc\n0\n0.0000\n\n\nmonth\n0\n0.0000\n\n\ntemp.qc\n0\n0.0000\n\n\nvis.var.qc\n0\n0.0000\n\n\nvis.var\n0\n0.0000\n\n\nvis.dist.qc\n0\n0.0000\n\n\nsky.cond\n0\n0.0000\n\n\nlon\n0\n0.0000"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely",
    "title": "JSC 370: Data Science II",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIf we return to our initial question—what weather stations reported the hottest and coldest temperatures, we should take a closer look at our key variable, temperature (temp).\n\nmet[\"temp\"].describe()\n\ncount    2.317254e+06\nmean     2.359054e+01\nstd      6.059275e+00\nmin     -4.000000e+01\n25%      1.960000e+01\n50%      2.350000e+01\n75%      2.780000e+01\nmax      5.600000e+01\nName: temp, dtype: float64\n\n\n\nIt looks like the data are in Celsius."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#check-variables-more-closely-1",
    "title": "JSC 370: Data Science II",
    "section": "Check variables more closely",
    "text": "Check variables more closely\n\nIt’s odd to have -40C in August.\nSubset rows where temp == -40.0 and select a few columns with loc, which is an indexer for selecting rows and columns.\n\n\nmet_ss = met.loc[met[\"temp\"] == -40.0, [\"USAFID\",\"hour\", \"lat\", \"lon\",  \"elev\", \"temp\", \"wind.sp\"]]\nmet_ss.shape\nmet_ss.head()\n\n\n\n\n\n\n\n\nUSAFID\nhour\nlat\nlon\nelev\ntemp\nwind.sp\n\n\n\n\n496047\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496048\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496049\n720717\n0\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496050\n720717\n1\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n496051\n720717\n1\n29.117\n-89.55\n36\n-40.0\nNaN\n\n\n\n\n\n\n\n\nIt appears that all of the data with -40C are from the same site and have missing. Points to a data issue."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#validate-against-an-external-source",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#validate-against-an-external-source",
    "title": "JSC 370: Data Science II",
    "section": "Validate against an external source",
    "text": "Validate against an external source\nWe should check outside sources to make sure that our data make sense. The observation with -40°C is suspicious, so we should look up the location of the weather station.\nGo to Google maps and enter the coordinates for the site with -40C (29.12, -89.55)\n\n\n\n\n\n\n\n\nIt doesn’t make much sense to have a -40C reading in the Gulf of Mexico off the coast of Louisiana."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#filter-implausible-values",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#filter-implausible-values",
    "title": "JSC 370: Data Science II",
    "section": "Filter implausible values",
    "text": "Filter implausible values\nFilter with loc and sort the data with sort_values in one line:\n\nmet = met.loc[met[\"temp\"] &gt; -40].sort_values(\"temp\")\nmet.head(20)\n\n\n\n\n\n\n\n\nUSAFID\nWBAN\nyear\nmonth\nday\nhour\nmin\nlat\nlon\nelev\n...\nvis.dist.qc\nvis.var\nvis.var.qc\ntemp\ntemp.qc\ndew.point\ndew.point.qc\natm.press\natm.press.qc\nrh\n\n\n\n\n1203054\n722817\n3068\n2019\n8\n1\n1\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203127\n722817\n3068\n2019\n8\n3\n11\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203128\n722817\n3068\n2019\n8\n3\n12\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203221\n722817\n3068\n2019\n8\n6\n21\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203224\n722817\n3068\n2019\n8\n6\n22\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203052\n722817\n3068\n2019\n8\n1\n0\n56\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.2\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1203225\n722817\n3068\n2019\n8\n6\n23\n3\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203220\n722817\n3068\n2019\n8\n6\n21\n39\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203222\n722817\n3068\n2019\n8\n6\n22\n32\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203223\n722817\n3068\n2019\n8\n6\n22\n41\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n6\nNaN\n9\nNaN\n9\nNaN\n\n\n1203053\n722817\n3068\n2019\n8\n1\n1\n6\n38.767\n-104.300\n1838\n...\n9\nN\n5\n-17.0\n5\nNaN\n9\nNaN\n9\nNaN\n\n\n1105458\n722518\n12974\n2019\n8\n12\n3\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1011.6\n1\nNaN\n\n\n1105456\n722518\n12974\n2019\n8\n12\n1\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1010.3\n1\nNaN\n\n\n1105454\n722518\n12974\n2019\n8\n11\n23\n56\n27.901\n-98.052\n78\n...\n9\n9\n9\n-17.0\n1\nNaN\n9\n1010.3\n1\nNaN\n\n\n2370757\n726764\n94163\n2019\n8\n27\n11\n50\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\nC\n-5.0\nC\nNaN\n9\n86.265368\n\n\n2370758\n726764\n94163\n2019\n8\n27\n12\n10\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\n5\n-4.0\n5\nNaN\n9\n92.910834\n\n\n2370759\n726764\n94163\n2019\n8\n27\n12\n30\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\n5\n-4.0\n5\nNaN\n9\n92.910834\n\n\n2370760\n726764\n94163\n2019\n8\n27\n12\n50\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-3.0\nC\n-4.0\nC\nNaN\n9\n92.910834\n\n\n252488\n720411\n137\n2019\n8\n18\n12\n35\n36.422\n-105.290\n2554\n...\n5\nN\n5\n-2.4\n5\n-3.7\n5\nNaN\n9\n90.914749\n\n\n2370687\n726764\n94163\n2019\n8\n26\n12\n30\n44.683\n-111.116\n2025\n...\n5\nN\n5\n-2.0\n5\n-3.0\n5\nNaN\n9\n92.966900\n\n\n\n\n20 rows × 30 columns\n\n\n\n\nWe should do a quick check of (38.767, -104.300) and (27.901, -98.052) to see if the -17C rows are also implausible and should be removed."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#filter-and-select",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#filter-and-select",
    "title": "JSC 370: Data Science II",
    "section": "Filter and select",
    "text": "Filter and select\nBack to question: what weather stations reported the hottest and coldest daily temperatures?\nCompute the min and max hourly temperatures and the associated stations. Filter with loc and find min and max (it ignores missing) with idxmin() and inxmax()\n\nmin_row = met.loc[met[\"temp\"].idxmin(), [\"temp\", \"USAFID\", \"day\"]]\nmax_row = met.loc[met[\"temp\"].idxmax(), [\"temp\", \"USAFID\", \"day\"]]\nmin_row, max_row\n\n(temp       -17.2\n USAFID    722817\n day            1\n Name: 1203054, dtype: object,\n temp        56.0\n USAFID    720267\n day           26\n Name: 42402, dtype: object)\n\n\n\nOk but we need daily temperatures to answer our questions!"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data",
    "title": "JSC 370: Data Science II",
    "section": "Grouping and summarizing data",
    "text": "Grouping and summarizing data\nWe need to transform our data to answer our initial question at the daily scale. Let’s find daily mean temperature by station and day. Use groupby and agg to calculate the mean and create a new dataset met_daily.\n\nmet_daily = (\n    met.groupby([\"USAFID\", \"day\"], as_index=False)\n       .agg(\n           temp=(\"temp\", \"mean\"),\n           lat=(\"lat\", \"mean\"),\n           lon=(\"lon\", \"mean\"),\n           rh=(\"rh\", \"mean\"),\n       )\n       .sort_values(\"temp\")\n)\n\nmet_daily.head(), met_daily.tail() \n\n(       USAFID  day       temp     lat    lon         rh\n 20774  722817    3 -17.200000  38.767 -104.3        NaN\n 20773  722817    1 -17.133333  38.767 -104.3        NaN\n 20775  722817    6 -17.066667  38.767 -104.3        NaN\n 43605  726130   11   4.278261  44.270  -71.3  99.540440\n 43625  726130   31   4.304348  44.270  -71.3  99.710254,\n        USAFID  day       temp        lat         lon         rh\n 27043  723805    5  40.975000  34.768000 -114.618000  11.369535\n 2345   720339   14  41.000000  32.146000 -111.171000   9.137857\n 27042  723805    4  41.183333  34.768000 -114.618000  14.211822\n 20623  722787    5  41.357143  33.527000 -112.295000  19.829822\n 30     690150   31  41.716667  34.299667 -116.165667   8.676717)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#grouping-and-summarizing-data-1",
    "title": "JSC 370: Data Science II",
    "section": "Grouping and summarizing data",
    "text": "Grouping and summarizing data\nWhat day of the month was the hottest?\n\nmin_daily = met_daily.loc[met_daily[\"temp\"].idxmin(), [\"temp\", \"USAFID\", \"day\"]]\nmax_daily = met_daily.loc[met_daily[\"temp\"].idxmax(), [\"temp\", \"USAFID\", \"day\"]]\nmin_daily, max_daily\n\n(temp         -17.2\n USAFID    722817.0\n day            3.0\n Name: 20774, dtype: float64,\n temp          41.716667\n USAFID    690150.000000\n day           31.000000\n Name: 30, dtype: float64)\n\n\n\nThe hottest day was August 31st, and the average daily temperature was 41.7C"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#looking-at-two-variables-at-a-time",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#looking-at-two-variables-at-a-time",
    "title": "JSC 370: Data Science II",
    "section": "Looking at two variables at a time",
    "text": "Looking at two variables at a time\nTo answer the last question about covariation between temperature and humidity, we can first calculate correlation\n\npearson = met[\"temp\"].corr(met[\"rh\"], method=\"pearson\")\nspearman = met[\"temp\"].corr(met[\"rh\"], method=\"spearman\")\n\npearson, spearman, spearman - pearson\n\n(np.float64(-0.5464705142984516),\n np.float64(-0.5461260667477443),\n np.float64(0.00034444755070728306))\n\n\nIs it the same for the daily data?\n\npearson = met_daily[\"temp\"].corr(met_daily[\"rh\"], method=\"pearson\")\nspearman = met_daily[\"temp\"].corr(met_daily[\"rh\"], method=\"spearman\")\n\npearson, spearman, spearman - pearson\n\n(np.float64(-0.25729536769933825),\n np.float64(-0.20801597675598085),\n np.float64(0.0492793909433574))\n\n\nThe correlation is quite different between hourly and daily data. Why?"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nWith exploratory graphs we aim to:\n•   debug any issues remaining in the data\n•   understand properties of the data\n•   look for patterns in the data\n•   inform modeling strategies\nExploratory graphs do not need to be perfect, we’ll look at presentation-ready plots with python’s ggplot equivalent."
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-graphs-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nWhat type of data are these types of exploratory graphs good for?\n•   histograms \n•   boxplots\n•   scatterplots \n•   barplots \n•   lineplots\n•   violin plots \n•   maps"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory histogram",
    "text": "Exploratory histogram\nFocusing on temperature, let’s look at the distribution of hourly temperature (after removing -40°C) using a histogram.\n\nplt.hist(met[\"temp\"].dropna(), bins=50)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Count\")\nplt.title(\"Hourly Temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-histogram-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory histogram",
    "text": "Exploratory histogram\nLet’s look at the daily data\n\nplt.hist(met_daily[\"temp\"].dropna(), bins=50)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Count\")\nplt.title(\"Daily Mean Temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory boxplot",
    "text": "Exploratory boxplot\nA boxplot gives us an idea of the quantiles of the distribution and any outliers.\n\nplt.boxplot(met[\"temp\"].dropna(), vert=True)\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Hourly temperature\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-boxplot-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory boxplot",
    "text": "Exploratory boxplot\nOften boxplots are better for grouped continuous data, such as temperature by hour (over all days in the dataset).\n\nhours = range(24)\ndata_by_hour = [met.loc[met[\"hour\"] == h, \"temp\"].to_numpy() for h in hours]\n\nplt.boxplot(data_by_hour, labels=list(hours), showfliers=False)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Temperature by Hour\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-violin-plot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-violin-plot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory violin plot",
    "text": "Exploratory violin plot\nOften violin plots are helpful for grouped continuous data (they show the shape of the distribution), such as temperature by hour (over all days in the dataset).\n\nplt.violinplot(data_by_hour, showmeans=False, showmedians=True, showextrema=False)\nplt.xticks(ticks=np.arange(1, 25), labels=list(hours))\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Temperature (°C)\")\nplt.title(\"Temperature by Hour\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory scatterplot",
    "text": "Exploratory scatterplot\nLet’s check covariation with a scatterplot of humidity vs temperature.\n\nplt.scatter(met[\"temp\"], met[\"rh\"])\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Relative humidity (%)\")\nplt.title(\"Temperature vs humidity (sample)\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-scatterplot-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory scatterplot",
    "text": "Exploratory scatterplot\nSince there are so many points let’s take a random sample of 10,000 rows using sample and add alpha transparency to see the scatter better. random_state is like setting a seed for reproducibility.\n\nmet_s = met.sample(min(10_000, len(met)), random_state=1)\n\nplt.scatter(met_s[\"temp\"], met_s[\"rh\"], alpha=0.2)\nplt.xlabel(\"Temperature (°C)\")\nplt.ylabel(\"Relative humidity (%)\")\nplt.title(\"Temperature vs humidity (sample)\")\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-1",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map\nMay need to python3 -m pip install geopandas contextily to make these maps\n\nimport geopandas as gpd\nimport contextily as cx\n\n# extract latitudes and longitudes\ngdf = gpd.GeoDataFrame(\n    met_daily.dropna(subset=[\"lat\", \"lon\"]).copy(),\n    geometry=gpd.points_from_xy(met_daily[\"lon\"], met_daily[\"lat\"]),\n    crs=\"EPSG:4326\"   \n)\n\ngdf_web = gdf.to_crs(epsg=3857)\n\nax = gdf_web.plot(markersize=10, alpha=0.6)\ncx.add_basemap(ax, source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\n\nSlightly more advanced to make an interactive map with folium.\nYou can use leaflet as another (my preferred) option, but it seems to have some problems in quarto.\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-1",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-1",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\nThis is how folium works:\n\nselect unique lat and lon\nset up the map with folium.Map\nadd the location points on the map with folium.CircleMarker, add opacity and radius for point size here"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-2",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#interactive-map-2",
    "title": "JSC 370: Data Science II",
    "section": "Interactive map",
    "text": "Interactive map\n\nimport folium\n\npts = met_daily[[\"lat\", \"lon\"]].dropna().drop_duplicates()\n\nm = folium.Map(\n    location=[pts[\"lat\"].mean(), pts[\"lon\"].mean()],\n    zoom_start=4,\n    tiles=\"CartoDB positron\"\n)\n\nfor _, r in pts.iterrows():\n    folium.CircleMarker(\n        location=[r[\"lat\"], r[\"lon\"]],\n        radius=2,\n        fill=True,\n        fill_opacity=0.8,\n        opacity=0.8,\n    ).add_to(m)\n\nm"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-2",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#exploratory-map-2",
    "title": "JSC 370: Data Science II",
    "section": "Exploratory map",
    "text": "Exploratory map\nLet’s now look at where the hottest and coldest places were based on the daily temperatures. We add a layer for cold and a layer for hot.\n\ncold = met_daily.loc[met_daily[\"temp\"].idxmin()]\nhot  = met_daily.loc[met_daily[\"temp\"].idxmax()]\ncold, hot\n\ncenter = [met_daily[\"lat\"].mean(), met_daily[\"lon\"].mean()]\nm = folium.Map(location=center, zoom_start=4, tiles=\"CartoDB positron\")\n\n# Coldest (blue)\nfolium.CircleMarker(\n    location=[cold[\"lat\"], cold[\"lon\"]],\n    radius=10,\n    popup=f\"Coldest daily mean: {cold['temp']:.2f}°C&lt;br&gt;USAFID={cold['USAFID']}&lt;br&gt;day={cold['day']}\",\n    color=\"blue\",\n    fill=True,\n    fill_color=\"blue\",\n    fill_opacity=0.9,\n).add_to(m)\n\n# Hottest (red)\nfolium.CircleMarker(\n    location=[hot[\"lat\"], hot[\"lon\"]],\n    radius=10,\n    popup=f\"Hottest daily mean: {hot['temp']:.2f}°C&lt;br&gt;USAFID={hot['USAFID']}&lt;br&gt;day={hot['day']}\",\n    color=\"red\",\n    fill=True,\n    fill_color=\"red\",\n    fill_opacity=0.9,\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#plotnine-ggplot-style-plots-in-python",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#plotnine-ggplot-style-plots-in-python",
    "title": "JSC 370: Data Science II",
    "section": "plotnine (ggplot-style plots in Python)",
    "text": "plotnine (ggplot-style plots in Python)\nIf you like ggplot2, plotnine uses the same “add layers with +” workflow: - map variables with aes() - add geoms (points, smoothers) - add labels/themes\n\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom plotnine import (\n    ggplot, aes, geom_point, geom_smooth, labs, theme_bw, annotate, coord_cartesian\n)\n\nmet_s = met[[\"temp\", \"rh\"]].dropna().sample(min(10_000, len(met)), random_state=1)\n\n# Fit line: rh = m*temp + b\nx = met_s[\"temp\"].to_numpy()\ny = met_s[\"rh\"].to_numpy()\nm, b = np.polyfit(x, y, 1)\n\ny_hat = m * x + b\nr2 = r2_score(y, y_hat)\n\nlabel = f\"rh = {m:.2f}·temp + {b:.2f}\\nR² = {r2:.3f}\"\n\n# location for the label\nx_pos = np.quantile(x, 0.9)\ny_pos = np.quantile(y, 0.999)\n\n(\n  ggplot(met_s, aes(x=\"temp\", y=\"rh\"))\n  + geom_point(alpha=0.2, size=1.0)\n  + geom_smooth(method=\"lm\", se=False)\n  + annotate(\"text\", x=x_pos, y=y_pos, label=label, ha=\"left\")\n  + coord_cartesian(ylim=(0, 100))\n  + labs(\n      title=\"Temperature vs humidity (sample) with linear fit\",\n      x=\"Temperature (°C)\",\n      y=\"Relative humidity (%)\"\n    )\n  + theme_bw()\n)"
  },
  {
    "objectID": "slides/03-eda-viz/JSC370-slides-03.html#plotnine",
    "href": "slides/03-eda-viz/JSC370-slides-03.html#plotnine",
    "title": "JSC 370: Data Science II",
    "section": "plotnine",
    "text": "plotnine\n\n(\n  ggplot(met_s, aes(x=\"temp\", y=\"rh\"))\n  + geom_point(alpha=0.2, size=1.0)\n  + geom_smooth(method=\"loess\", se=False)\n  + coord_cartesian(ylim=(0, 100))\n  + labs(\n      title=\"Temperature vs humidity (sample) with linear fit\",\n      x=\"Temperature (°C)\",\n      y=\"Relative humidity (%)\"\n    )\n  + theme_bw()\n)"
  },
  {
    "objectID": "labs/lab3/lab03-github.html",
    "href": "labs/lab3/lab03-github.html",
    "title": "Lab 03 - EDA and Viz 1",
    "section": "",
    "text": "Read in from github and get familiar with a meteorology dataset\nPlan out how to tackle the objective question\nStep through the EDA “checklist” presented in the class slides\nMake exploratory graphs and maps"
  },
  {
    "objectID": "labs/lab3/lab03-github.html#answer-research-questions",
    "href": "labs/lab3/lab03-github.html#answer-research-questions",
    "title": "Lab 03 - EDA and Viz 1",
    "section": "6. Answer research questions",
    "text": "6. Answer research questions\nRemember to keep the initial questions in mind. We want to pick out the weather station with minimum elevation and examine its temperature.\nSome ideas for steps: 1. subset the data for the weather station with minimum elevation 2. look at histograms of temperature 3. make a time series plot (need to create date variable) 4. make a map to see where it is\n\n6a. Subset minimum elevation site\n\n# Subset data for the weather station with minimum elevation\n\n\n\n6b. Histogram of temperature at minimum elevation site\n\nplt.hist(...)\n\n\n\n6c. Create date variable and time series plot\n\nLook at the time series of temperature at this location. For this we will need to create a date-time variable for the x-axis.\nSummarize any trends that you see in these time series plots.\n\n\n# Create a date variable for min_elev_station\nmin_elev_station = min_elev_station.copy()\nmin_elev_station['date'] = pd.to_datetime(\n    min_elev_station[...]\n)\n\n# Sort by date\nmin_elev_station = min_elev_station.sort_values('date')\n\n# Create line plot of temperature by date\n\nplt.plot(...)\nplt.xlabel()\nplt.ylabel()\nplt.show()\n\nSummary: - Summarize time series plot.\n\n\n6d. Where is the lowest weather station?\n\nstation_location = min_elev_station[...].drop_duplicates()\n\nlat = station_location['lat'].iloc[0]\nlon = station_location['lon'].iloc[0]\nelev = station_location['elev'].iloc[0]\n\n# make elev into GeoDataFrame for mapping\ngdf = gpd.GeoDataFrame(\n    {'elevation': [elev]},\n    geometry=gpd.points_from_xy([lon], [lat]),\n    crs='EPSG:4326'\n)\n\n# Convert to Web Mercator for basemap\ngdf = gdf.to_crs('EPSG:3857')\n\n# map\nfig, ax = plt.subplots(figsize=(10, 8))\ngdf.plot(ax=ax, color='red', markersize=300, marker='*', edgecolor='black', linewidth=2, zorder=5)\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nax.set_title(f'Lowest Elevation Station (Elevation: {elev} m)')\nplt.show()\n\nSummary: where is the lowest elevation station?"
  }
]